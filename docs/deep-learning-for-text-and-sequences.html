<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Deep Learning for Text and Sequences | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="8 Deep Learning for Text and Sequences | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Deep Learning for Text and Sequences | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Deep Learning for Text and Sequences | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter-5-deep-learning-for-computer-vision.html"/>
<link rel="next" href="advanced-deep-learning-best-practices.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#models-beyond-linearity"><i class="fa fa-check"></i><b>2.1</b> Models Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#degrees-of-freedom"><i class="fa fa-check"></i><b>2.1.3.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="2.1.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#basis-splines-vs.-natural-splines"><i class="fa fa-check"></i><b>2.1.3.5</b> Basis splines vs.Â natural splines</a></li>
<li class="chapter" data-level="2.1.3.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.6</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#feature-selection"><i class="fa fa-check"></i><b>2.1.6.1</b> Feature Selection</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for regression problems</a>
<ul>
<li class="chapter" data-level="2.1.6.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#interpretation-of-output"><i class="fa fa-check"></i><b>2.1.6.2.1</b> Interpretation of output</a></li>
</ul></li>
<li class="chapter" data-level="2.1.6.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.3</b> GAM for classification problems</a></li>
</ul></li>
<li class="chapter" data-level="2.1.7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#model-assessment"><i class="fa fa-check"></i><b>2.1.7</b> Model assessment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lecture-notes"><i class="fa fa-check"></i><b>2.2</b> Lecture notes</a></li>
<li class="chapter" data-level="2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lab-section"><i class="fa fa-check"></i><b>2.3</b> Lab section</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression-and-step-functions"><i class="fa fa-check"></i><b>2.3.1</b> Polynomial Regression and Step Functions</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#continous-model"><i class="fa fa-check"></i><b>2.3.1.1</b> Continous model</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logarithmic-model"><i class="fa fa-check"></i><b>2.3.1.2</b> Logarithmic model</a></li>
<li class="chapter" data-level="2.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-function"><i class="fa fa-check"></i><b>2.3.1.3</b> Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#splines"><i class="fa fa-check"></i><b>2.3.2</b> Splines</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#basis-function-splines"><i class="fa fa-check"></i><b>2.3.2.1</b> Basis Function Splines</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#natural-splines"><i class="fa fa-check"></i><b>2.3.2.2</b> Natural Splines</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smooth-splines"><i class="fa fa-check"></i><b>2.3.2.3</b> Smooth Splines</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression-1"><i class="fa fa-check"></i><b>2.3.2.4</b> Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gams"><i class="fa fa-check"></i><b>2.3.3</b> GAMs</a>
<ul>
<li class="chapter" data-level="2.3.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-only-natural-splines"><i class="fa fa-check"></i><b>2.3.3.1</b> With only natural splines</a></li>
<li class="chapter" data-level="2.3.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-different-splines"><i class="fa fa-check"></i><b>2.3.3.2</b> With different splines</a></li>
<li class="chapter" data-level="2.3.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#but-what-variables-to-include"><i class="fa fa-check"></i><b>2.3.3.3</b> But what variables to include?</a></li>
<li class="chapter" data-level="2.3.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-with-local-regression"><i class="fa fa-check"></i><b>2.3.3.4</b> GAM with local regression</a></li>
<li class="chapter" data-level="2.3.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logistic-regression"><i class="fa fa-check"></i><b>2.3.3.5</b> Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a>
<ul>
<li class="chapter" data-level="2.4.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-polynomial-regression"><i class="fa fa-check"></i><b>2.4.1.1</b> 6.a Polynomial Regression</a></li>
<li class="chapter" data-level="2.4.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-step-function"><i class="fa fa-check"></i><b>2.4.1.2</b> 6.b Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a>
<ul>
<li class="chapter" data-level="2.4.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-using-poly-function-to-fit-cubic-polynomial-regression"><i class="fa fa-check"></i><b>2.4.4.1</b> (a) using poly function to fit cubic polynomial regression</a></li>
<li class="chapter" data-level="2.4.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-plotting-polynomial-fits-for-a-range-of-polynomials"><i class="fa fa-check"></i><b>2.4.4.2</b> (b) Plotting polynomial fits for a range of polynomials</a></li>
<li class="chapter" data-level="2.4.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-using-cv-to-select-best-degree-of-d"><i class="fa fa-check"></i><b>2.4.4.3</b> (c) Using CV to select best degree of d</a></li>
<li class="chapter" data-level="2.4.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-use-bs-to-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.4</b> (d) Use <code>bs()</code> to fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#e-now-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.5</b> (e) Now fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#f-perform-cross-validation-to-select-degrees"><i class="fa fa-check"></i><b>2.4.4.6</b> (f) Perform cross-validation, to select degrees</a></li>
</ul></li>
<li class="chapter" data-level="2.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a>
<ul>
<li class="chapter" data-level="2.4.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-partitioning-the-data"><i class="fa fa-check"></i><b>2.4.5.1</b> (a) Partitioning the data</a></li>
<li class="chapter" data-level="2.4.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-fitting-a-gam"><i class="fa fa-check"></i><b>2.4.5.2</b> (b) Fitting a GAM</a></li>
<li class="chapter" data-level="2.4.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-evaluating-on-the-test-set"><i class="fa fa-check"></i><b>2.4.5.3</b> (c) Evaluating on the test set</a></li>
<li class="chapter" data-level="2.4.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-which-variables-appear-to-have-a-non-linear-relationship"><i class="fa fa-check"></i><b>2.4.5.4</b> (d) Which variables appear to have a non linear relationship?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#FacebookCasestudy"><i class="fa fa-check"></i><b>2.5</b> Casestudy - Predicting the Return on Advertising Spent</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#background"><i class="fa fa-check"></i><b>2.5.1</b> 1. Background</a></li>
<li class="chapter" data-level="2.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#case-study-business-understanding-phase"><i class="fa fa-check"></i><b>2.5.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#the-data-data-understanding-phase"><i class="fa fa-check"></i><b>2.5.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#specific-requirements"><i class="fa fa-check"></i><b>2.5.4</b> 4. Specific requirements:</a>
<ul>
<li class="chapter" data-level="2.5.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-1---import-and-overview"><i class="fa fa-check"></i><b>2.5.4.1</b> 4.1 Task 1 - Import and overview</a></li>
<li class="chapter" data-level="2.5.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-2---data-inspection"><i class="fa fa-check"></i><b>2.5.4.2</b> 4.2 Task 2 - Data inspection</a></li>
<li class="chapter" data-level="2.5.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-3---building-different-models"><i class="fa fa-check"></i><b>2.5.4.3</b> 4.3 Task 3 - Building different models</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-generalized-additive-model-gam-to-predict-roas"><i class="fa fa-check"></i><b>2.5.4.3.1</b> A Generalized Additive Model (GAM) to predict ROAS</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c1-feature-selection-using-regsubsets"><i class="fa fa-check"></i><b>2.5.4.3.1.1</b> c1) Feature selection using regsubsets()</a></li>
<li class="chapter" data-level="2.5.4.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c2-feature-selection-using-step.gam"><i class="fa fa-check"></i><b>2.5.4.3.1.2</b> c2) Feature selection using step.GAM</a></li>
<li class="chapter" data-level="2.5.4.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c3-fetaure-selection-using-random-forest"><i class="fa fa-check"></i><b>2.5.4.3.1.3</b> c3) Fetaure selection using random forest</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>3.1</b> The Basics of Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-trees"><i class="fa fa-check"></i><b>3.1.1</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#how-to-make-the-decision-trees"><i class="fa fa-check"></i><b>3.1.1.1</b> How to make the decision trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-goal-of-regression"><i class="fa fa-check"></i><b>3.1.1.1.1</b> The goal of regression</a></li>
<li class="chapter" data-level="3.1.1.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning-algoritm"><i class="fa fa-check"></i><b>3.1.1.1.2</b> Tree Pruning &amp; Algoritm</a></li>
<li class="chapter" data-level="3.1.1.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#setting-contrains-of-the-tree-sise"><i class="fa fa-check"></i><b>3.1.1.1.3</b> Setting contrains of the tree sise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#classification-trees"><i class="fa fa-check"></i><b>3.1.2</b> Classification Trees</a></li>
<li class="chapter" data-level="3.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-vs.-lienar-models"><i class="fa fa-check"></i><b>3.1.3</b> Tree vs.Â Lienar Models</a></li>
<li class="chapter" data-level="3.1.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advantages-and-disadvantages-of-trees"><i class="fa fa-check"></i><b>3.1.4</b> Advantages and Disadvantages of Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-random-forests-boosting"><i class="fa fa-check"></i><b>3.2</b> Bagging, Random Forests, Boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-bootstrap-aggregation"><i class="fa fa-check"></i><b>3.2.1</b> Bagging (Bootstrap Aggregation)</a></li>
<li class="chapter" data-level="3.2.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="3.2.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-i.e.-gradient-boosting"><i class="fa fa-check"></i><b>3.2.3</b> Boosting (i.e.Â Gradient Boosting)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost"><i class="fa fa-check"></i><b>3.3</b> XGBoost</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#application-in-r"><i class="fa fa-check"></i><b>3.4</b> Application in R</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.4.1</b> Decision trees</a></li>
<li class="chapter" data-level="3.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.4.2</b> Bagging</a></li>
<li class="chapter" data-level="3.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-1"><i class="fa fa-check"></i><b>3.4.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4.4</b> Boosting</a></li>
<li class="chapter" data-level="3.4.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost-1"><i class="fa fa-check"></i><b>3.4.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#lab-section-1"><i class="fa fa-check"></i><b>3.5</b> Lab section</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#LabClassification"><i class="fa fa-check"></i><b>3.5.1</b> Fitting Classification Trees</a></li>
<li class="chapter" data-level="3.5.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#fitting-regression-trees"><i class="fa fa-check"></i><b>3.5.2</b> Fitting Regression Trees</a></li>
<li class="chapter" data-level="3.5.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-and-random-forests"><i class="fa fa-check"></i><b>3.5.3</b> Bagging and Random Forests</a>
<ul>
<li class="chapter" data-level="3.5.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-1"><i class="fa fa-check"></i><b>3.5.3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.5.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forest"><i class="fa fa-check"></i><b>3.5.3.2</b> Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="3.5.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-1"><i class="fa fa-check"></i><b>3.5.4</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercies"><i class="fa fa-check"></i><b>3.6</b> Exercies</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-7---decision-tree-assessment"><i class="fa fa-check"></i><b>3.6.1</b> Exercise 7 - Decision Tree Assessment</a></li>
<li class="chapter" data-level="3.6.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#but-what-does-it-really-show-ntrees-are-fixed-at-500"><i class="fa fa-check"></i><b>3.6.2</b> But what does it really show?, ntrees are fixed at 500</a></li>
<li class="chapter" data-level="3.6.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-8---dtrfboosting"><i class="fa fa-check"></i><b>3.6.3</b> Exercise 8 - DT/RF/Boosting</a></li>
<li class="chapter" data-level="3.6.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-9---decision-tree-pruning"><i class="fa fa-check"></i><b>3.6.4</b> Exercise 9 - Decision Tree / Pruning</a></li>
<li class="chapter" data-level="3.6.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-10---boostinggamlinearregbagging---comparison"><i class="fa fa-check"></i><b>3.6.5</b> Exercise 10 - Boosting/GAM/LinearReg/Bagging - Comparison</a></li>
<li class="chapter" data-level="3.6.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-11---boosting"><i class="fa fa-check"></i><b>3.6.6</b> Exercise 11 - Boosting</a></li>
<li class="chapter" data-level="3.6.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-12"><i class="fa fa-check"></i><b>3.6.7</b> Exercise 12</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#casestudy---predicting-algae-blooms"><i class="fa fa-check"></i><b>3.7</b> Casestudy - Predicting Algae Blooms</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#background-1"><i class="fa fa-check"></i><b>3.7.1</b> 1. Background</a></li>
<li class="chapter" data-level="3.7.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#case-study-business-understanding-phase-1"><i class="fa fa-check"></i><b>3.7.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="3.7.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-data-data-understanding-phase-1"><i class="fa fa-check"></i><b>3.7.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="3.7.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#requirements"><i class="fa fa-check"></i><b>3.7.4</b> 4. Requirements:</a>
<ul>
<li class="chapter" data-level="3.7.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-1-understand-and-import-data-properly"><i class="fa fa-check"></i><b>3.7.4.1</b> 4.1 Task 1: Understand and import data properly</a></li>
<li class="chapter" data-level="3.7.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-2-inspect-your-data-and-do-the-required-variable-adaptations-and-transformations"><i class="fa fa-check"></i><b>3.7.4.2</b> 4.2 Task 2: Inspect your data and do the required variable adaptations and transformations</a></li>
<li class="chapter" data-level="3.7.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-3-build-one-or-several-predictive-models-and-evaluate-their-performance."><i class="fa fa-check"></i><b>3.7.4.3</b> 4.3 Task 3: Build one or several predictive models and evaluate their performance.</a>
<ul>
<li class="chapter" data-level="3.7.4.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#growing-a-regression-tree"><i class="fa fa-check"></i><b>3.7.4.3.1</b> 4.3.1. Growing a regression tree</a></li>
<li class="chapter" data-level="3.7.4.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#pruning-the-tree"><i class="fa fa-check"></i><b>3.7.4.3.2</b> 4.3.2 Pruning the tree</a></li>
<li class="chapter" data-level="3.7.4.3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#model-evaluation-and-selection"><i class="fa fa-check"></i><b>3.7.4.3.3</b> 4.3.3.Model evaluation and selection</a></li>
<li class="chapter" data-level="3.7.4.3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#ensemble-methods-random-forests"><i class="fa fa-check"></i><b>3.7.4.3.4</b> 4.3.4. Ensemble methods: Random Forests</a></li>
<li class="chapter" data-level="3.7.4.3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#predicting-in-the-test-sample"><i class="fa fa-check"></i><b>3.7.4.3.5</b> 4.3.5. Predicting in the test sample</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="4.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-vector-machines"><i class="fa fa-check"></i><b>4.1</b> Maximal Vector Machines</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#what-is-a-hyperplane"><i class="fa fa-check"></i><b>4.1.1</b> What is a hyperplane?</a></li>
<li class="chapter" data-level="4.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#classification-using-a-separating-hyperplane"><i class="fa fa-check"></i><b>4.1.2</b> Classification Using a Separating Hyperplane</a></li>
<li class="chapter" data-level="4.1.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-maximal-margin-classifier"><i class="fa fa-check"></i><b>4.1.3</b> The Maximal Margin Classifier</a></li>
<li class="chapter" data-level="4.1.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#ConstructMMC"><i class="fa fa-check"></i><b>4.1.4</b> Construction of the Maximal Margin Classifer</a></li>
<li class="chapter" data-level="4.1.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>4.1.5</b> The Non-separable Case</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifiers"><i class="fa fa-check"></i><b>4.2</b> Support Vector Classifiers</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#overview-of-the-support-vector-classifier"><i class="fa fa-check"></i><b>4.2.1</b> Overview of the Support Vector Classifier</a></li>
<li class="chapter" data-level="4.2.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#details-of-the-support-vector-classifer"><i class="fa fa-check"></i><b>4.2.2</b> Details of the Support Vector Classifer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>4.3</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#classification-with-non-linear-decision-boundary"><i class="fa fa-check"></i><b>4.3.1</b> Classification with non linear decision boundary</a></li>
<li class="chapter" data-level="4.3.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-support-vector-machine"><i class="fa fa-check"></i><b>4.3.2</b> The Support Vector Machine</a></li>
<li class="chapter" data-level="4.3.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#MoreThanTwoClasses"><i class="fa fa-check"></i><b>4.3.3</b> SVMs with More than Two Classes</a></li>
<li class="chapter" data-level="4.3.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#relationship-to-logistic-regression"><i class="fa fa-check"></i><b>4.3.4</b> Relationship to logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#process-of-kernels-methods"><i class="fa fa-check"></i><b>4.4</b> Process of kernels methods</a></li>
<li class="chapter" data-level="4.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#lab"><i class="fa fa-check"></i><b>4.5</b> Lab</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>4.5.1</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="4.5.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machine"><i class="fa fa-check"></i><b>4.5.2</b> Support Vector Machine</a></li>
<li class="chapter" data-level="4.5.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#roc-curves"><i class="fa fa-check"></i><b>4.5.3</b> ROC Curves</a></li>
<li class="chapter" data-level="4.5.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-with-multiple-classes"><i class="fa fa-check"></i><b>4.5.4</b> SVM with Multiple Classes</a></li>
<li class="chapter" data-level="4.5.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-to-gene-expression-data"><i class="fa fa-check"></i><b>4.5.5</b> Application to Gene Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#exercises-1"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Deep Learning Fundamentals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#basic-deep-learning"><i class="fa fa-check"></i><b>5.1</b> Basic Deep Learning</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#terms"><i class="fa fa-check"></i><b>5.1.1</b> Terms</a></li>
<li class="chapter" data-level="5.1.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#optimizers-loss-metrics-and-activation-rules"><i class="fa fa-check"></i><b>5.1.2</b> Optimizers, Loss, Metrics and Activation rules</a>
<ul>
<li class="chapter" data-level="5.1.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#gradient-descents"><i class="fa fa-check"></i><b>5.1.2.1</b> Gradient Descents</a></li>
</ul></li>
<li class="chapter" data-level="5.1.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#example-with-image-recognizion"><i class="fa fa-check"></i><b>5.1.3</b> Example with image recognizion</a></li>
<li class="chapter" data-level="5.1.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#model-building"><i class="fa fa-check"></i><b>5.1.4</b> Model building</a></li>
<li class="chapter" data-level="5.1.5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#validating-the-model"><i class="fa fa-check"></i><b>5.1.5</b> Validating the model</a></li>
<li class="chapter" data-level="5.1.6" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#overfitting-underfitting"><i class="fa fa-check"></i><b>5.1.6</b> Overfitting / Underfitting</a>
<ul>
<li class="chapter" data-level="5.1.6.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#hyperparameters"><i class="fa fa-check"></i><b>5.1.6.1</b> Hyperparameters:</a></li>
<li class="chapter" data-level="5.1.6.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#regularization"><i class="fa fa-check"></i><b>5.1.6.2</b> Regularization:</a></li>
<li class="chapter" data-level="5.1.6.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#dropout"><i class="fa fa-check"></i><b>5.1.6.3</b> Dropout</a></li>
<li class="chapter" data-level="5.1.6.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#then-how-do-we-control-for-a-good-fit"><i class="fa fa-check"></i><b>5.1.6.4</b> Then how do we control for a good fit?</a></li>
</ul></li>
<li class="chapter" data-level="5.1.7" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#dealing-with-missing-data"><i class="fa fa-check"></i><b>5.1.7</b> Dealing with missing data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#the-workflow-for-building-the-neural-network"><i class="fa fa-check"></i><b>5.2</b> The workflow for building the neural network</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#being-aware-of-the-process"><i class="fa fa-check"></i><b>5.2.1</b> Being aware of the process</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#when-neural-networks-will-fail"><i class="fa fa-check"></i><b>5.3</b> When Neural Networks will fail</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyper parameter tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Chapter 3 - Getting Started With Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#positive-negative-imdb-reviews"><i class="fa fa-check"></i><b>6.1</b> Positive / Negative IMDB reviews</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#extracting-the-data"><i class="fa fa-check"></i><b>6.1.1</b> Extracting the data</a></li>
<li class="chapter" data-level="6.1.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#preparing-the-data"><i class="fa fa-check"></i><b>6.1.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.1.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#building-the-model"><i class="fa fa-check"></i><b>6.1.3</b> Building the model</a></li>
<li class="chapter" data-level="6.1.4" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#assessing-model-performance"><i class="fa fa-check"></i><b>6.1.4</b> Assessing model performance</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#multiclass-classification---classifying-newswires"><i class="fa fa-check"></i><b>6.2</b> Multiclass classification - Classifying newswires</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#loading-the-data"><i class="fa fa-check"></i><b>6.2.1</b> Loading the data</a></li>
<li class="chapter" data-level="6.2.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#preparing-the-data-1"><i class="fa fa-check"></i><b>6.2.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.2.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#building-the-model-1"><i class="fa fa-check"></i><b>6.2.3</b> Building the model</a></li>
<li class="chapter" data-level="6.2.4" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#validating-the-model-model-assessment"><i class="fa fa-check"></i><b>6.2.4</b> Validating the model + model assessment</a></li>
<li class="chapter" data-level="6.2.5" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#training-model-with-optimal-epochs"><i class="fa fa-check"></i><b>6.2.5</b> Training model with optimal epochs</a></li>
<li class="chapter" data-level="6.2.6" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#predictions-on-new-data"><i class="fa fa-check"></i><b>6.2.6</b> Predictions on new data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#continous-prediction-a-regression-example---predicting-houseprices"><i class="fa fa-check"></i><b>6.3</b> Continous prediction / a regression example - Predicting houseprices</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#loading-the-data-1"><i class="fa fa-check"></i><b>6.3.1</b> Loading the data</a></li>
<li class="chapter" data-level="6.3.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#preparing-the-data-2"><i class="fa fa-check"></i><b>6.3.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.3.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#building-the-model-2"><i class="fa fa-check"></i><b>6.3.3</b> Building the model</a>
<ul>
<li class="chapter" data-level="6.3.3.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#validating-the-approach-using-k-fold-validation"><i class="fa fa-check"></i><b>6.3.3.1</b> Validating the approach using K-fold validation</a></li>
<li class="chapter" data-level="6.3.3.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#validation-with-more-iterations"><i class="fa fa-check"></i><b>6.3.3.2</b> Validation with more iterations</a></li>
<li class="chapter" data-level="6.3.3.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#tuning-amount-fo-hidden-layers"><i class="fa fa-check"></i><b>6.3.3.3</b> Tuning amount fo hidden layers</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html"><i class="fa fa-check"></i><b>7</b> Chapter 5 - Deep learning for computer vision</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#definition-of-convoluted-network"><i class="fa fa-check"></i><b>7.1</b> Definition of convoluted network</a></li>
<li class="chapter" data-level="7.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#spetial-heirarchical"><i class="fa fa-check"></i><b>7.2</b> Spetial heirarchical</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#tuning-parameters"><i class="fa fa-check"></i><b>7.2.1</b> Tuning parameters</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#data-modeling-techniques"><i class="fa fa-check"></i><b>7.2.2</b> Data modeling techniques</a>
<ul>
<li class="chapter" data-level="7.2.2.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#padding"><i class="fa fa-check"></i><b>7.2.2.1</b> Padding</a></li>
<li class="chapter" data-level="7.2.2.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#strides"><i class="fa fa-check"></i><b>7.2.2.2</b> Strides</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#the-max-pooling-operation"><i class="fa fa-check"></i><b>7.3</b> The max-pooling operation</a></li>
<li class="chapter" data-level="7.4" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#examples"><i class="fa fa-check"></i><b>7.4</b> Examples</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#training-a-cats-and-dogs-classifier-from-scratch."><i class="fa fa-check"></i><b>7.4.1</b> Training a cats and dogs classifier from scratch.</a>
<ul>
<li class="chapter" data-level="7.4.1.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#loading-data"><i class="fa fa-check"></i><b>7.4.1.1</b> Loading data</a></li>
<li class="chapter" data-level="7.4.1.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#building-the-model-3"><i class="fa fa-check"></i><b>7.4.1.2</b> Building the model</a></li>
<li class="chapter" data-level="7.4.1.3" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#data-preprocessing"><i class="fa fa-check"></i><b>7.4.1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.4.1.4" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#fitting-the-model"><i class="fa fa-check"></i><b>7.4.1.4</b> Fitting the model</a></li>
<li class="chapter" data-level="7.4.1.5" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#dealing-with-overfitting"><i class="fa fa-check"></i><b>7.4.1.5</b> Dealing with overfitting</a>
<ul>
<li class="chapter" data-level="7.4.1.5.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#randomized-transformations-data-augmentation"><i class="fa fa-check"></i><b>7.4.1.5.1</b> Randomized transformations / Data augmentation</a></li>
<li class="chapter" data-level="7.4.1.5.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#adding-dropout"><i class="fa fa-check"></i><b>7.4.1.5.2</b> Adding dropout</a></li>
<li class="chapter" data-level="7.4.1.5.3" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#max-pooling"><i class="fa fa-check"></i><b>7.4.1.5.3</b> Max pooling</a></li>
</ul></li>
<li class="chapter" data-level="7.4.1.6" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#training-with-dropout-and-random-image-transformations"><i class="fa fa-check"></i><b>7.4.1.6</b> Training with dropout and random image transformations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#using-a-pretrained-convnet"><i class="fa fa-check"></i><b>7.5</b> Using a pretrained convnet</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#feature-extraction"><i class="fa fa-check"></i><b>7.5.1</b> Feature extraction</a>
<ul>
<li class="chapter" data-level="7.5.1.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#feature-extraction-without-data-augmentation"><i class="fa fa-check"></i><b>7.5.1.1</b> Feature extraction without data augmentation</a></li>
<li class="chapter" data-level="7.5.1.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#feature-extraction-with-data-augmentation"><i class="fa fa-check"></i><b>7.5.1.2</b> Feature extraction with data augmentation</a></li>
</ul></li>
<li class="chapter" data-level="7.5.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#fine-tuning"><i class="fa fa-check"></i><b>7.5.2</b> Fine-tuning</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#visualizing-what-convnets-learn"><i class="fa fa-check"></i><b>7.6</b> Visualizing what convnets learn</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html"><i class="fa fa-check"></i><b>8</b> Deep Learning for Text and Sequences</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#working-with-text-data"><i class="fa fa-check"></i><b>8.1</b> Working with Text Data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#one-hot-encoding-of-words-and-characters"><i class="fa fa-check"></i><b>8.1.1</b> One-hot encoding of words and characters</a></li>
<li class="chapter" data-level="8.1.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-word-embeddings"><i class="fa fa-check"></i><b>8.1.2</b> Using word embeddings</a>
<ul>
<li class="chapter" data-level="8.1.2.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#the-first-approach"><i class="fa fa-check"></i><b>8.1.2.1</b> The first approach</a></li>
<li class="chapter" data-level="8.1.2.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#the-second-approach---using-pretrained-word-embeddings"><i class="fa fa-check"></i><b>8.1.2.2</b> The second approach - using pretrained word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="8.1.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#putting-it-all-together-from-raw-text-to-word-embeddings"><i class="fa fa-check"></i><b>8.1.3</b> Putting it all together: from raw text to word embeddings</a>
<ul>
<li class="chapter" data-level="8.1.3.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#preprocessing-the-embeddings"><i class="fa fa-check"></i><b>8.1.3.1</b> Preprocessing the embeddings</a></li>
<li class="chapter" data-level="8.1.3.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#defining-a-model"><i class="fa fa-check"></i><b>8.1.3.2</b> Defining a model</a></li>
<li class="chapter" data-level="8.1.3.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#loading-glove-embeddings-in-the-model"><i class="fa fa-check"></i><b>8.1.3.3</b> Loading GloVe embeddings in the model</a></li>
<li class="chapter" data-level="8.1.3.4" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#training-and-evaluating-the-model"><i class="fa fa-check"></i><b>8.1.3.4</b> Training and evaluating the model</a></li>
<li class="chapter" data-level="8.1.3.5" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#training-and-evaluating-without-glove"><i class="fa fa-check"></i><b>8.1.3.5</b> Training and evaluating without GloVe</a></li>
<li class="chapter" data-level="8.1.3.6" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-test-data"><i class="fa fa-check"></i><b>8.1.3.6</b> Using test data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#understanding-recurrent-neural-networks-rnn"><i class="fa fa-check"></i><b>8.2</b> Understanding Recurrent Neural Networks (RNN)</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-recurrent-layer-in-keras"><i class="fa fa-check"></i><b>8.2.1</b> A recurrent layer in Keras</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#understanding-the-lstm-and-gru-layers"><i class="fa fa-check"></i><b>8.2.2</b> Understanding the LSTM and GRU layers</a>
<ul>
<li class="chapter" data-level="8.2.2.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#units-inside-gru-and-lstm"><i class="fa fa-check"></i><b>8.2.2.1</b> Units inside GRU and LSTM</a></li>
</ul></li>
<li class="chapter" data-level="8.2.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-concrete-lstm-example-in-keras"><i class="fa fa-check"></i><b>8.2.3</b> A concrete LSTM example in Keras</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#advanced-use-of-recurrent-neural-networks"><i class="fa fa-check"></i><b>8.3</b> Advanced use of Recurrent neural networks</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-temperature-forecasting-problem"><i class="fa fa-check"></i><b>8.3.1</b> A temperature-forecasting problem</a></li>
<li class="chapter" data-level="8.3.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#preparing-the-data-3"><i class="fa fa-check"></i><b>8.3.2</b> Preparing the data</a></li>
<li class="chapter" data-level="8.3.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-common-sense-non-machine-learning-baseline"><i class="fa fa-check"></i><b>8.3.3</b> A common-sense, non-machine-learning baseline</a></li>
<li class="chapter" data-level="8.3.4" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-basic-machine-learning-approach"><i class="fa fa-check"></i><b>8.3.4</b> A basic machine-learning approach</a></li>
<li class="chapter" data-level="8.3.5" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-first-recurrent-baseline"><i class="fa fa-check"></i><b>8.3.5</b> A first recurrent baseline</a></li>
<li class="chapter" data-level="8.3.6" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-recurrent-dropout-to-fight-overfitting"><i class="fa fa-check"></i><b>8.3.6</b> using recurrent dropout to fight overfitting</a></li>
<li class="chapter" data-level="8.3.7" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#stacking-recurrent-layers"><i class="fa fa-check"></i><b>8.3.7</b> Stacking recurrent layers</a></li>
<li class="chapter" data-level="8.3.8" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-bidirectional-rnns"><i class="fa fa-check"></i><b>8.3.8</b> Using bidirectional RNNs</a></li>
<li class="chapter" data-level="8.3.9" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#going-even-further"><i class="fa fa-check"></i><b>8.3.9</b> Going even further</a></li>
<li class="chapter" data-level="8.3.10" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#wrap-up"><i class="fa fa-check"></i><b>8.3.10</b> Wrap up</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#sequence-processing-with-convnets"><i class="fa fa-check"></i><b>8.4</b> Sequence processing with convnets</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#implementing-a-1d-convnet"><i class="fa fa-check"></i><b>8.4.1</b> Implementing a 1D convnet</a></li>
<li class="chapter" data-level="8.4.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#combining-cnns-and-rnns-to-process-long-sequences"><i class="fa fa-check"></i><b>8.4.2</b> Combining CNNs and RNNs to process long sequences</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html"><i class="fa fa-check"></i><b>9</b> Advanced Deep-Learning Best Practices</a>
<ul>
<li class="chapter" data-level="9.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#going-beyond-the-sequential-model-the-keras-functino-api"><i class="fa fa-check"></i><b>9.1</b> Going beyond the sequential model: the Keras functino API</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#introduction-to-the-functional-api"><i class="fa fa-check"></i><b>9.1.1</b> Introduction to the functional API</a></li>
<li class="chapter" data-level="9.1.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#multi-input-models"><i class="fa fa-check"></i><b>9.1.2</b> Multi-input models</a></li>
<li class="chapter" data-level="9.1.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#multi-output"><i class="fa fa-check"></i><b>9.1.3</b> Multi-output</a></li>
<li class="chapter" data-level="9.1.4" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#directed-acyclic-graphs-of-layers-dag"><i class="fa fa-check"></i><b>9.1.4</b> Directed acyclic graphs of layers (DAG)</a>
<ul>
<li class="chapter" data-level="9.1.4.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#inception-modules"><i class="fa fa-check"></i><b>9.1.4.1</b> Inception modules</a></li>
<li class="chapter" data-level="9.1.4.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#residual-connection"><i class="fa fa-check"></i><b>9.1.4.2</b> Residual Connection</a></li>
</ul></li>
<li class="chapter" data-level="9.1.5" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#layer-weight-sharing"><i class="fa fa-check"></i><b>9.1.5</b> Layer weight sharing</a></li>
<li class="chapter" data-level="9.1.6" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#models-as-layers"><i class="fa fa-check"></i><b>9.1.6</b> Models as layers</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#inspecting-and-monitoring-deep-learning-models-using-keras-callba--acks-and-tensorboard"><i class="fa fa-check"></i><b>9.2</b> Inspecting and monitoring deep-learning models using Keras callba- acks and TensorBoard</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#using-callbacks-to-act-on-a-model-during-training"><i class="fa fa-check"></i><b>9.2.1</b> Using callbacks to act on a model during training</a>
<ul>
<li class="chapter" data-level="9.2.1.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#the-model-checkpoint-and-early-stopping-callbacks"><i class="fa fa-check"></i><b>9.2.1.1</b> The model-checkpoint and early-stopping callbacks</a></li>
<li class="chapter" data-level="9.2.1.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#the-reduce-learning-rate-on-plateau-callback"><i class="fa fa-check"></i><b>9.2.1.2</b> The reduce-learning-rate-on-plateau callback</a></li>
<li class="chapter" data-level="9.2.1.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#writing-your-own-callback-functions"><i class="fa fa-check"></i><b>9.2.1.3</b> Writing your own callback functions</a></li>
</ul></li>
<li class="chapter" data-level="9.2.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#introduction-to-tensorboard-the-tensorflow-visualization-framework"><i class="fa fa-check"></i><b>9.2.2</b> Introduction to tensorBoard: the TensorFlow visualization framework</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#getting-the-most-of-your-models"><i class="fa fa-check"></i><b>9.3</b> Getting the most of your models</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#advanced-architecture-patterns"><i class="fa fa-check"></i><b>9.3.1</b> Advanced architecture patterns</a>
<ul>
<li class="chapter" data-level="9.3.1.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#batch-normalization"><i class="fa fa-check"></i><b>9.3.1.1</b> Batch normalization</a></li>
<li class="chapter" data-level="9.3.1.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#depthwise-separable-convolution"><i class="fa fa-check"></i><b>9.3.1.2</b> Depthwise separable convolution</a></li>
</ul></li>
<li class="chapter" data-level="9.3.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#hyperparameter-optimization"><i class="fa fa-check"></i><b>9.3.2</b> Hyperparameter optimization</a></li>
<li class="chapter" data-level="9.3.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#model-ensembling"><i class="fa fa-check"></i><b>9.3.3</b> Model ensembling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="structuring-data-transformation-and-model-assessments.html"><a href="structuring-data-transformation-and-model-assessments.html"><i class="fa fa-check"></i><b>10</b> Structuring data transformation and model assessments</a></li>
<li class="chapter" data-level="11" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html"><i class="fa fa-check"></i><b>11</b> Github and CSS styling</a>
<ul>
<li class="chapter" data-level="11.1" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html#managing-github"><i class="fa fa-check"></i><b>11.1</b> Managing GitHub</a></li>
<li class="chapter" data-level="11.2" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html#css-styling"><i class="fa fa-check"></i><b>11.2</b> CSS Styling</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning-for-text-and-sequences" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Deep Learning for Text and Sequences</h1>
<p>The following chapters follow the structure of the book.</p>
<p><strong>Introduction</strong></p>
<p>This method is able to cover:</p>
<ul>
<li><p>Text, understood as a sequence of words</p></li>
<li><p>Timeseries</p></li>
<li><p>Sequence data in general</p></li>
</ul>
<p>To deal with this, we are going to work with recurrent neural networks and 1D convoluted networks.</p>
<div id="working-with-text-data" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Working with Text Data</h2>
<p>Notice that NN only works with numbers, hence we cannot use text as input. Hence we must create tensors that are text vectorized. To do this, we have several methods:</p>
<ol style="list-style-type: decimal">
<li>Segment text into words and transforming each word into a vector.</li>
<li>Segment text into characters and transform each character into a vector.</li>
<li>Extract n-grams of words or characters, and transform each n-gram ino a vector. N-Grams are overlapping groups of multiple consecutive words or characters.</li>
</ol>
<p><strong>N-Grams</strong></p>
<p>We see that the sentence, âThe cat sat on the mat.â may be composed in 2-grams.</p>
<p>{âThe,â âThe cat,â âcat,â âcat sat,â âsat,ââsat on,â âon,â âon the,â âthe,â âthe mat,â âmatâ}, or 3-grams</p>
<p>{âThe,â âThe cat,â âcat,â âcat sat,â âThe cat sat,ââsat,â âsat on,â âon,â âcat sat on,â âon the,â âthe,ââsat on the,â âthe mat,â âmat,â âon the matâ}.</p>
<p><em>This is also cllaed bag-of-2-grams or equvilently bag-of-3-grams</em></p>
<div id="one-hot-encoding-of-words-and-characters" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> One-hot encoding of words and characters</h3>
<p>A visual example:</p>
<p><img src="Images/paste-DB1F1A56.png" width="440" /></p>
<p><em>We see that the matrix will be sparse as we have a lot of 0âs in there. Also it is hard coded, henced fixted dictionaries.</em></p>
<p>This is the most common and basic way of tokenizing.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="deep-learning-for-text-and-sequences.html#cb522-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Listing 6.1. Word-level one-hot encoding (toy example)</span></span>
<span id="cb522-2"><a href="deep-learning-for-text-and-sequences.html#cb522-2" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;The cat sat on the mat.&quot;</span>, <span class="st">&quot;The dog ate my homework.&quot;</span>)</span>
<span id="cb522-3"><a href="deep-learning-for-text-and-sequences.html#cb522-3" aria-hidden="true" tabindex="-1"></a>token_index <span class="ot">&lt;-</span> <span class="fu">list</span>() <span class="co">#Creating an index</span></span>
<span id="cb522-4"><a href="deep-learning-for-text-and-sequences.html#cb522-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb522-5"><a href="deep-learning-for-text-and-sequences.html#cb522-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (sample <span class="cf">in</span> samples){</span>
<span id="cb522-6"><a href="deep-learning-for-text-and-sequences.html#cb522-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (word <span class="cf">in</span> <span class="fu">strsplit</span>(sample, <span class="st">&quot; &quot;</span>)[[<span class="dv">1</span>]]){ <span class="co">#Tokenizing</span></span>
<span id="cb522-7"><a href="deep-learning-for-text-and-sequences.html#cb522-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span>word <span class="sc">%in%</span> <span class="fu">names</span>(token_index)) <span class="co">#If not in the index</span></span>
<span id="cb522-8"><a href="deep-learning-for-text-and-sequences.html#cb522-8" aria-hidden="true" tabindex="-1"></a>      token_index[[word]] <span class="ot">&lt;-</span> <span class="fu">length</span>(token_index) <span class="sc">+</span> <span class="dv">2</span> <span class="co">#Ins</span></span>
<span id="cb522-9"><a href="deep-learning-for-text-and-sequences.html#cb522-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb522-10"><a href="deep-learning-for-text-and-sequences.html#cb522-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb522-11"><a href="deep-learning-for-text-and-sequences.html#cb522-11" aria-hidden="true" tabindex="-1"></a>max_length <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb522-12"><a href="deep-learning-for-text-and-sequences.html#cb522-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb522-13"><a href="deep-learning-for-text-and-sequences.html#cb522-13" aria-hidden="true" tabindex="-1"></a><span class="co">#Saving the results in a 3D tensor</span></span>
<span id="cb522-14"><a href="deep-learning-for-text-and-sequences.html#cb522-14" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(samples),</span>
<span id="cb522-15"><a href="deep-learning-for-text-and-sequences.html#cb522-15" aria-hidden="true" tabindex="-1"></a>                            max_length,</span>
<span id="cb522-16"><a href="deep-learning-for-text-and-sequences.html#cb522-16" aria-hidden="true" tabindex="-1"></a>                            <span class="fu">max</span>(<span class="fu">as.integer</span>(token_index))))</span>
<span id="cb522-17"><a href="deep-learning-for-text-and-sequences.html#cb522-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb522-18"><a href="deep-learning-for-text-and-sequences.html#cb522-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(samples)) {</span>
<span id="cb522-19"><a href="deep-learning-for-text-and-sequences.html#cb522-19" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> samples[[i]]</span>
<span id="cb522-20"><a href="deep-learning-for-text-and-sequences.html#cb522-20" aria-hidden="true" tabindex="-1"></a>  words <span class="ot">&lt;-</span> <span class="fu">head</span>(<span class="fu">strsplit</span>(sample, <span class="st">&quot; &quot;</span>)[[<span class="dv">1</span>]], <span class="at">n =</span> max_length)</span>
<span id="cb522-21"><a href="deep-learning-for-text-and-sequences.html#cb522-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(words)) {</span>
<span id="cb522-22"><a href="deep-learning-for-text-and-sequences.html#cb522-22" aria-hidden="true" tabindex="-1"></a>    index <span class="ot">&lt;-</span> token_index[[words[[j]]]]</span>
<span id="cb522-23"><a href="deep-learning-for-text-and-sequences.html#cb522-23" aria-hidden="true" tabindex="-1"></a>    results[[i, j, index]] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb522-24"><a href="deep-learning-for-text-and-sequences.html#cb522-24" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb522-25"><a href="deep-learning-for-text-and-sequences.html#cb522-25" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we can also see an example on character level.</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="deep-learning-for-text-and-sequences.html#cb523-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Listing 6.2. Character-level one-hot encoding (toy example)</span></span>
<span id="cb523-2"><a href="deep-learning-for-text-and-sequences.html#cb523-2" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;The cat sat on the mat.&quot;</span>, <span class="st">&quot;The dog ate my homework.&quot;</span>)</span>
<span id="cb523-3"><a href="deep-learning-for-text-and-sequences.html#cb523-3" aria-hidden="true" tabindex="-1"></a>ascii_tokens <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;&quot;</span>, <span class="fu">sapply</span>(<span class="fu">as.raw</span>(<span class="fu">c</span>(<span class="dv">32</span><span class="sc">:</span><span class="dv">126</span>)), rawToChar))</span>
<span id="cb523-4"><a href="deep-learning-for-text-and-sequences.html#cb523-4" aria-hidden="true" tabindex="-1"></a>token_index <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(ascii_tokens))) <span class="co">#Loading ascii tokens, predefined list</span></span>
<span id="cb523-5"><a href="deep-learning-for-text-and-sequences.html#cb523-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(token_index) <span class="ot">&lt;-</span> ascii_tokens</span>
<span id="cb523-6"><a href="deep-learning-for-text-and-sequences.html#cb523-6" aria-hidden="true" tabindex="-1"></a>max_length <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb523-7"><a href="deep-learning-for-text-and-sequences.html#cb523-7" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(samples), max_length, <span class="fu">length</span>(token_index)))</span>
<span id="cb523-8"><a href="deep-learning-for-text-and-sequences.html#cb523-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(samples)) {</span>
<span id="cb523-9"><a href="deep-learning-for-text-and-sequences.html#cb523-9" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> samples[[i]]</span>
<span id="cb523-10"><a href="deep-learning-for-text-and-sequences.html#cb523-10" aria-hidden="true" tabindex="-1"></a>  characters <span class="ot">&lt;-</span> <span class="fu">strsplit</span>(sample, <span class="st">&quot;&quot;</span>)[[<span class="dv">1</span>]]</span>
<span id="cb523-11"><a href="deep-learning-for-text-and-sequences.html#cb523-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(characters)) {</span>
<span id="cb523-12"><a href="deep-learning-for-text-and-sequences.html#cb523-12" aria-hidden="true" tabindex="-1"></a>    character <span class="ot">&lt;-</span> characters[[j]]</span>
<span id="cb523-13"><a href="deep-learning-for-text-and-sequences.html#cb523-13" aria-hidden="true" tabindex="-1"></a>    results[i, j, token_index[[character]]] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co">#Inserting to dim i, j and</span></span>
<span id="cb523-14"><a href="deep-learning-for-text-and-sequences.html#cb523-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb523-15"><a href="deep-learning-for-text-and-sequences.html#cb523-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Naturally we see that the dimensions of the tensor is greatly increased.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="deep-learning-for-text-and-sequences.html#cb524-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.3. Using Keras for word-level one-hot encoding</span></span>
<span id="cb524-2"><a href="deep-learning-for-text-and-sequences.html#cb524-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb524-3"><a href="deep-learning-for-text-and-sequences.html#cb524-3" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;The cat sat on the mat.&quot;</span>, <span class="st">&quot;The dog ate my homework.&quot;</span>)</span>
<span id="cb524-4"><a href="deep-learning-for-text-and-sequences.html#cb524-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="ot">&lt;-</span> <span class="fu">text_tokenizer</span>(<span class="at">num_words =</span> <span class="dv">1000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb524-5"><a href="deep-learning-for-text-and-sequences.html#cb524-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit_text_tokenizer</span>(samples)</span>
<span id="cb524-6"><a href="deep-learning-for-text-and-sequences.html#cb524-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb524-7"><a href="deep-learning-for-text-and-sequences.html#cb524-7" aria-hidden="true" tabindex="-1"></a>sequences <span class="ot">&lt;-</span> <span class="fu">texts_to_sequences</span>(tokenizer, samples)</span>
<span id="cb524-8"><a href="deep-learning-for-text-and-sequences.html#cb524-8" aria-hidden="true" tabindex="-1"></a>one_hot_results <span class="ot">&lt;-</span> <span class="fu">texts_to_matrix</span>(tokenizer, samples, <span class="at">mode =</span> <span class="st">&quot;binary&quot;</span>)</span>
<span id="cb524-9"><a href="deep-learning-for-text-and-sequences.html#cb524-9" aria-hidden="true" tabindex="-1"></a>word_index <span class="ot">&lt;-</span> tokenizer<span class="sc">$</span>word_index</span>
<span id="cb524-10"><a href="deep-learning-for-text-and-sequences.html#cb524-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Found&quot;</span>, <span class="fu">length</span>(word_index), <span class="st">&quot;unique tokens.</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>We can also apply something called hashing trick, notice that the hashFunction cannot be isntalled. This is specifically useful if the vocabulary is very large.</p>
<p>In the following example we will be hashing words in 1000 characters long vectors. Notice that the more words you have, the longer must the vectors be.</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="deep-learning-for-text-and-sequences.html#cb525-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.4. Word-level one-hot encoding with hashing trick (toy example)</span></span>
<span id="cb525-2"><a href="deep-learning-for-text-and-sequences.html#cb525-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(hashFunction)</span></span>
<span id="cb525-3"><a href="deep-learning-for-text-and-sequences.html#cb525-3" aria-hidden="true" tabindex="-1"></a><span class="co"># samples &lt;- c(&quot;The cat sat on the mat.&quot;, &quot;The dog ate my homework.&quot;)</span></span>
<span id="cb525-4"><a href="deep-learning-for-text-and-sequences.html#cb525-4" aria-hidden="true" tabindex="-1"></a><span class="co"># dimensionality &lt;- 1000                                                  1</span></span>
<span id="cb525-5"><a href="deep-learning-for-text-and-sequences.html#cb525-5" aria-hidden="true" tabindex="-1"></a><span class="co"># max_length &lt;- 10</span></span>
<span id="cb525-6"><a href="deep-learning-for-text-and-sequences.html#cb525-6" aria-hidden="true" tabindex="-1"></a><span class="co"># results &lt;- array(0, dim = c(length(samples), max_length, dimensionality))</span></span>
<span id="cb525-7"><a href="deep-learning-for-text-and-sequences.html#cb525-7" aria-hidden="true" tabindex="-1"></a><span class="co"># for (i in 1:length(samples)) {</span></span>
<span id="cb525-8"><a href="deep-learning-for-text-and-sequences.html#cb525-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   sample &lt;- samples[[i]]</span></span>
<span id="cb525-9"><a href="deep-learning-for-text-and-sequences.html#cb525-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   words &lt;- head(strsplit(sample, &quot; &quot;)[[1]], n = max_length)</span></span>
<span id="cb525-10"><a href="deep-learning-for-text-and-sequences.html#cb525-10" aria-hidden="true" tabindex="-1"></a><span class="co">#   for (j in 1:length(words)) {</span></span>
<span id="cb525-11"><a href="deep-learning-for-text-and-sequences.html#cb525-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     index &lt;- abs(spooky.32(words[[i]])) %% dimensionality               2</span></span>
<span id="cb525-12"><a href="deep-learning-for-text-and-sequences.html#cb525-12" aria-hidden="true" tabindex="-1"></a><span class="co">#     results[[i, j, index]] &lt;- 1</span></span>
<span id="cb525-13"><a href="deep-learning-for-text-and-sequences.html#cb525-13" aria-hidden="true" tabindex="-1"></a><span class="co">#   }</span></span>
<span id="cb525-14"><a href="deep-learning-for-text-and-sequences.html#cb525-14" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span></code></pre></div>
</div>
<div id="using-word-embeddings" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Using word embeddings</h3>
<p>One sees that one hot encoding leads to very big and sparse matrices, we can overcome this with word embeddings, which is the essence of the following.</p>
<p>To do this, there are two approaches:</p>
<ol style="list-style-type: decimal">
<li>You start with some random word vectors and then learn the word vectors following the same principle as learning neural network weights.</li>
<li>Load precomputed word embeddings, just like loading a pretrained conv. So called pretrained word embeddings.</li>
</ol>
<p>The goal of the word embeddings is that one will map out similarity/connectedness of words. Meaning that we want to reflect the actual language. One must be aware that the advanced language nuances for what it is concerning, meaning that movie reviews and scientific papers may have different word embeddings.</p>
<p>It is hypothesized, that there is a true word embeddings map, although that is yet to me discovered. For example one can arrance wolf, tiger, dog and cat on two vectors, 1) wild to pet animal, and 2) canine to feline.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-399"></span>
<img src="Images/paste-795B4BE1.png" alt="Figure 6.3. A toy example of a wordembedding space" width="102" />
<p class="caption">
Figure 8.1: Figure 6.3. A toy example of a wordembedding space
</p>
</div>
<p>Naturally, this could be on many different scales. Thus, we see that we have vectorized the words and geometrically we can see if they are pointing in the same direction or if they are going away from each other, hence we can calculate the distance between the words, to measure how similar words are.</p>
<p>Compared to one-hot encoding, we see the following:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-400"></span>
<img src="Images/paste-AA3D9DB3.png" alt="One-hot encoding vs. word embedding" width="579" />
<p class="caption">
Figure 8.2: One-hot encoding vs.Â word embedding
</p>
</div>
<p><em>We see that the left = the one-hot encoded and the right = words embedding.</em></p>
<div id="the-first-approach" class="section level4" number="8.1.2.1">
<h4><span class="header-section-number">8.1.2.1</span> The first approach</h4>
<p>Here we can use the power of backpropaganation.</p>
<p>Here is an example with IMDB reviews, with sentiment prediction.</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="deep-learning-for-text-and-sequences.html#cb526-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.6. Loading the IMDB data for use with an embedding layer</span></span>
<span id="cb526-2"><a href="deep-learning-for-text-and-sequences.html#cb526-2" aria-hidden="true" tabindex="-1"></a>max_features <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#Number of words for consideration</span></span>
<span id="cb526-3"><a href="deep-learning-for-text-and-sequences.html#cb526-3" aria-hidden="true" tabindex="-1"></a>maxlen <span class="ot">&lt;-</span> <span class="dv">20</span> <span class="co">#Cutting the text after 20 words</span></span>
<span id="cb526-4"><a href="deep-learning-for-text-and-sequences.html#cb526-4" aria-hidden="true" tabindex="-1"></a>imdb <span class="ot">&lt;-</span> <span class="fu">dataset_imdb</span>(<span class="at">num_words =</span> max_features)</span>
<span id="cb526-5"><a href="deep-learning-for-text-and-sequences.html#cb526-5" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(x_train, y_train), <span class="fu">c</span>(x_test, y_test)) <span class="sc">%&lt;-%</span> imdb</span>
<span id="cb526-6"><a href="deep-learning-for-text-and-sequences.html#cb526-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb526-7"><a href="deep-learning-for-text-and-sequences.html#cb526-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Creating 2D integer tensors, shape = (samples,maxlen)</span></span>
<span id="cb526-8"><a href="deep-learning-for-text-and-sequences.html#cb526-8" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_train, <span class="at">maxlen =</span> maxlen) <span class="co">#Adding 0, if the review is &lt;20 words</span></span>
<span id="cb526-9"><a href="deep-learning-for-text-and-sequences.html#cb526-9" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_test, <span class="at">maxlen =</span> maxlen)</span></code></pre></div>
<p>We see that the max length and max features are parameters where we need to control how much information to include, while having a competitive model and equally a model that is runable.</p>
<p>Now we can create a model with the word embedding.</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="deep-learning-for-text-and-sequences.html#cb527-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.7. Using an embedding layer and classifier on the IMDB data</span></span>
<span id="cb527-2"><a href="deep-learning-for-text-and-sequences.html#cb527-2" aria-hidden="true" tabindex="-1"></a>dim_embeddings <span class="ot">&lt;-</span> <span class="dv">8</span> <span class="co">#The amount of dimensions the words is to be measured on.</span></span>
<span id="cb527-3"><a href="deep-learning-for-text-and-sequences.html#cb527-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb527-4"><a href="deep-learning-for-text-and-sequences.html#cb527-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb527-5"><a href="deep-learning-for-text-and-sequences.html#cb527-5" aria-hidden="true" tabindex="-1"></a>val_split <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb527-6"><a href="deep-learning-for-text-and-sequences.html#cb527-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb527-7"><a href="deep-learning-for-text-and-sequences.html#cb527-7" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb527-8"><a href="deep-learning-for-text-and-sequences.html#cb527-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_features</span>
<span id="cb527-9"><a href="deep-learning-for-text-and-sequences.html#cb527-9" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">output_dim =</span> dim_embeddings</span>
<span id="cb527-10"><a href="deep-learning-for-text-and-sequences.html#cb527-10" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">input_length =</span> maxlen) <span class="sc">%&gt;%</span></span>
<span id="cb527-11"><a href="deep-learning-for-text-and-sequences.html#cb527-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span></span>
<span id="cb527-12"><a href="deep-learning-for-text-and-sequences.html#cb527-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span></code></pre></div>
<p>Note to the mode, We see that the embedding layer can be compared with the feature maps from the convnn, where we have a 3D tensor, which is going to be flattened.</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="deep-learning-for-text-and-sequences.html#cb528-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb528-2"><a href="deep-learning-for-text-and-sequences.html#cb528-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb528-3"><a href="deep-learning-for-text-and-sequences.html#cb528-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb528-4"><a href="deep-learning-for-text-and-sequences.html#cb528-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>) <span class="co">#We have balanced data, so acc. should do</span></span>
<span id="cb528-5"><a href="deep-learning-for-text-and-sequences.html#cb528-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb528-6"><a href="deep-learning-for-text-and-sequences.html#cb528-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb528-7"><a href="deep-learning-for-text-and-sequences.html#cb528-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb528-8"><a href="deep-learning-for-text-and-sequences.html#cb528-8" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb528-9"><a href="deep-learning-for-text-and-sequences.html#cb528-9" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb528-10"><a href="deep-learning-for-text-and-sequences.html#cb528-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> epochs,</span>
<span id="cb528-11"><a href="deep-learning-for-text-and-sequences.html#cb528-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size,</span>
<span id="cb528-12"><a href="deep-learning-for-text-and-sequences.html#cb528-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> val_split</span>
<span id="cb528-13"><a href="deep-learning-for-text-and-sequences.html#cb528-13" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We see that there is an indication of overfitting. Although the validation accuracy tend towards 75%. And notice that we are only using 20 words from each review.</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="deep-learning-for-text-and-sequences.html#cb529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>What we are not doing and what we could do:</p>
<ul>
<li>Canât tell the difference between <em>this movie is shit</em> and <em>this movie is the shit</em></li>
<li>We could use RNN, to capture word relationships</li>
<li>We could take more than 20 words, or perhaps not the first 20 words, but rather 20 words in the middle or in the end, as for instance reviewee may tend to start with some kind of summary of the movie.</li>
</ul>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="deep-learning-for-text-and-sequences.html#cb530-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
</div>
<div id="the-second-approach---using-pretrained-word-embeddings" class="section level4" number="8.1.2.2">
<h4><span class="header-section-number">8.1.2.2</span> The second approach - using pretrained word embeddings</h4>
<p>Like with other pretrained models, if you donât have much data, then you are better of just loading in a model that is trained on sufficient data and then use their weighs etc. The same applies here, where we can load in the word embeddings.</p>
<p><em>Recall that features are constantly learned by the model, hence not enough data, means that the features that you end up with, will depend on the data, and if that is sparse, so will the features be</em></p>
<p>Examples of the word embeddings is:</p>
<ul>
<li><p>Word2vec</p></li>
<li><p>GloVe - Global Vectors for Word Representation</p></li>
</ul>
<p>We see that a word embedding vectorize the words on n dimensions, the following example is King, Man and Woman on 50 dimension, where we see that particularly two dimensions light up respectively blue and red.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-406"></span>
<img src="Images/paste-3D12324A.png" alt="Word embeddings example" width="260" />
<p class="caption">
Figure 8.3: Word embeddings example
</p>
</div>
<p>and another example with more words</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-407"></span>
<img src="Images/paste-46C140C0.png" alt="Word embeddings example 2" width="1103" />
<p class="caption">
Figure 8.4: Word embeddings example 2
</p>
</div>
<p>It is worth mentioning that algorithm has found scores based on no labels, one merely specify how many dimensions they want.</p>
<p>The GloVe intuition can be represented in the following, where we see that <code>king - man + woman = queen</code> , where we see that you should be able to go from one word to other words.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-408"></span>
<img src="Images/paste-5D6C3CBF.png" alt="GloVe intuition" width="818" />
<p class="caption">
Figure 8.5: GloVe intuition
</p>
</div>
</div>
</div>
<div id="putting-it-all-together-from-raw-text-to-word-embeddings" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Putting it all together: from raw text to word embeddings</h3>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="deep-learning-for-text-and-sequences.html#cb531-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.8. Processing the labels of the raw IMDB data</span></span>
<span id="cb531-2"><a href="deep-learning-for-text-and-sequences.html#cb531-2" aria-hidden="true" tabindex="-1"></a>imdb_dir <span class="ot">&lt;-</span> <span class="st">&quot;Data/3. Deep Learning/aclImdb&quot;</span></span>
<span id="cb531-3"><a href="deep-learning-for-text-and-sequences.html#cb531-3" aria-hidden="true" tabindex="-1"></a>train_dir <span class="ot">&lt;-</span> <span class="fu">file.path</span>(imdb_dir, <span class="st">&quot;train&quot;</span>)</span>
<span id="cb531-4"><a href="deep-learning-for-text-and-sequences.html#cb531-4" aria-hidden="true" tabindex="-1"></a>labels <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb531-5"><a href="deep-learning-for-text-and-sequences.html#cb531-5" aria-hidden="true" tabindex="-1"></a>texts <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb531-6"><a href="deep-learning-for-text-and-sequences.html#cb531-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (label_type <span class="cf">in</span> <span class="fu">c</span>(<span class="st">&quot;neg&quot;</span>, <span class="st">&quot;pos&quot;</span>)) {</span>
<span id="cb531-7"><a href="deep-learning-for-text-and-sequences.html#cb531-7" aria-hidden="true" tabindex="-1"></a>  label <span class="ot">&lt;-</span> <span class="cf">switch</span>(label_type, <span class="at">neg =</span> <span class="dv">0</span>, <span class="at">pos =</span> <span class="dv">1</span>)</span>
<span id="cb531-8"><a href="deep-learning-for-text-and-sequences.html#cb531-8" aria-hidden="true" tabindex="-1"></a>  dir_name <span class="ot">&lt;-</span> <span class="fu">file.path</span>(train_dir, label_type)</span>
<span id="cb531-9"><a href="deep-learning-for-text-and-sequences.html#cb531-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (fname <span class="cf">in</span> <span class="fu">list.files</span>(dir_name, <span class="at">pattern =</span> <span class="fu">glob2rx</span>(<span class="st">&quot;*.txt&quot;</span>),</span>
<span id="cb531-10"><a href="deep-learning-for-text-and-sequences.html#cb531-10" aria-hidden="true" tabindex="-1"></a>                           <span class="at">full.names =</span> <span class="cn">TRUE</span>)) {</span>
<span id="cb531-11"><a href="deep-learning-for-text-and-sequences.html#cb531-11" aria-hidden="true" tabindex="-1"></a>    texts <span class="ot">&lt;-</span> <span class="fu">c</span>(texts, <span class="fu">readChar</span>(fname, <span class="fu">file.info</span>(fname)<span class="sc">$</span>size))</span>
<span id="cb531-12"><a href="deep-learning-for-text-and-sequences.html#cb531-12" aria-hidden="true" tabindex="-1"></a>    labels <span class="ot">&lt;-</span> <span class="fu">c</span>(labels, label)</span>
<span id="cb531-13"><a href="deep-learning-for-text-and-sequences.html#cb531-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb531-14"><a href="deep-learning-for-text-and-sequences.html#cb531-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we can <strong>tokenize</strong> the data.</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="deep-learning-for-text-and-sequences.html#cb532-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.9. Tokenizing the text of the raw IMDB data</span></span>
<span id="cb532-2"><a href="deep-learning-for-text-and-sequences.html#cb532-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb532-3"><a href="deep-learning-for-text-and-sequences.html#cb532-3" aria-hidden="true" tabindex="-1"></a>maxlen <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co">#Cuttin reviews at 100 words</span></span>
<span id="cb532-4"><a href="deep-learning-for-text-and-sequences.html#cb532-4" aria-hidden="true" tabindex="-1"></a>training_samples <span class="ot">&lt;-</span> <span class="dv">200</span> <span class="co">#Amount of train samples</span></span>
<span id="cb532-5"><a href="deep-learning-for-text-and-sequences.html#cb532-5" aria-hidden="true" tabindex="-1"></a>validation_samples <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#Amount of validation samples</span></span>
<span id="cb532-6"><a href="deep-learning-for-text-and-sequences.html#cb532-6" aria-hidden="true" tabindex="-1"></a>max_words <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#We only want to consider the top 10.000 words.</span></span>
<span id="cb532-7"><a href="deep-learning-for-text-and-sequences.html#cb532-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-8"><a href="deep-learning-for-text-and-sequences.html#cb532-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="ot">&lt;-</span> <span class="fu">text_tokenizer</span>(<span class="at">num_words =</span> max_words) <span class="sc">%&gt;%</span></span>
<span id="cb532-9"><a href="deep-learning-for-text-and-sequences.html#cb532-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit_text_tokenizer</span>(texts) <span class="co">#F</span></span>
<span id="cb532-10"><a href="deep-learning-for-text-and-sequences.html#cb532-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-11"><a href="deep-learning-for-text-and-sequences.html#cb532-11" aria-hidden="true" tabindex="-1"></a>sequences <span class="ot">&lt;-</span> <span class="fu">texts_to_sequences</span>(tokenizer, texts)</span>
<span id="cb532-12"><a href="deep-learning-for-text-and-sequences.html#cb532-12" aria-hidden="true" tabindex="-1"></a>word_index <span class="ot">=</span> tokenizer<span class="sc">$</span>word_index</span>
<span id="cb532-13"><a href="deep-learning-for-text-and-sequences.html#cb532-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Found&quot;</span>, <span class="fu">length</span>(word_index), <span class="st">&quot;unique tokens.</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb532-14"><a href="deep-learning-for-text-and-sequences.html#cb532-14" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(sequences, <span class="at">maxlen =</span> maxlen)</span>
<span id="cb532-15"><a href="deep-learning-for-text-and-sequences.html#cb532-15" aria-hidden="true" tabindex="-1"></a>labels <span class="ot">&lt;-</span> <span class="fu">as.array</span>(labels)</span>
<span id="cb532-16"><a href="deep-learning-for-text-and-sequences.html#cb532-16" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Shape of data tensor:&quot;</span>, <span class="fu">dim</span>(data), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb532-17"><a href="deep-learning-for-text-and-sequences.html#cb532-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&#39;Shape of label tensor:&#39;</span>, <span class="fu">dim</span>(labels), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb532-18"><a href="deep-learning-for-text-and-sequences.html#cb532-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-19"><a href="deep-learning-for-text-and-sequences.html#cb532-19" aria-hidden="true" tabindex="-1"></a><span class="co">#Splitting train and validation data</span></span>
<span id="cb532-20"><a href="deep-learning-for-text-and-sequences.html#cb532-20" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data)) <span class="co">#We are shuffling the data</span></span>
<span id="cb532-21"><a href="deep-learning-for-text-and-sequences.html#cb532-21" aria-hidden="true" tabindex="-1"></a>training_indices <span class="ot">&lt;-</span> indices[<span class="dv">1</span><span class="sc">:</span>training_samples]</span>
<span id="cb532-22"><a href="deep-learning-for-text-and-sequences.html#cb532-22" aria-hidden="true" tabindex="-1"></a>validation_indices <span class="ot">&lt;-</span> indices[(training_samples <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span></span>
<span id="cb532-23"><a href="deep-learning-for-text-and-sequences.html#cb532-23" aria-hidden="true" tabindex="-1"></a>                              (training_samples <span class="sc">+</span> validation_samples)]</span>
<span id="cb532-24"><a href="deep-learning-for-text-and-sequences.html#cb532-24" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> data[training_indices,]</span>
<span id="cb532-25"><a href="deep-learning-for-text-and-sequences.html#cb532-25" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> labels[training_indices]</span>
<span id="cb532-26"><a href="deep-learning-for-text-and-sequences.html#cb532-26" aria-hidden="true" tabindex="-1"></a>x_val <span class="ot">&lt;-</span> data[validation_indices,]</span>
<span id="cb532-27"><a href="deep-learning-for-text-and-sequences.html#cb532-27" aria-hidden="true" tabindex="-1"></a>y_val <span class="ot">&lt;-</span> labels[validation_indices]</span></code></pre></div>
<div id="preprocessing-the-embeddings" class="section level4" number="8.1.3.1">
<h4><span class="header-section-number">8.1.3.1</span> Preprocessing the embeddings</h4>
<p>First we need to create an index that maps the words.</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="deep-learning-for-text-and-sequences.html#cb533-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.10. Parsing the GloVe word-embeddings file</span></span>
<span id="cb533-2"><a href="deep-learning-for-text-and-sequences.html#cb533-2" aria-hidden="true" tabindex="-1"></a>glove_dir <span class="ot">=</span> <span class="st">&quot;Data/3. Deep Learning/glove&quot;</span></span>
<span id="cb533-3"><a href="deep-learning-for-text-and-sequences.html#cb533-3" aria-hidden="true" tabindex="-1"></a>lines <span class="ot">&lt;-</span> <span class="fu">readLines</span>(<span class="fu">file.path</span>(glove_dir, <span class="st">&quot;glove.6B.100d.txt&quot;</span>))</span>
<span id="cb533-4"><a href="deep-learning-for-text-and-sequences.html#cb533-4" aria-hidden="true" tabindex="-1"></a>embeddings_index <span class="ot">&lt;-</span> <span class="fu">new.env</span>(<span class="at">hash =</span> <span class="cn">TRUE</span>, <span class="at">parent =</span> <span class="fu">emptyenv</span>())</span>
<span id="cb533-5"><a href="deep-learning-for-text-and-sequences.html#cb533-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(lines)) {</span>
<span id="cb533-6"><a href="deep-learning-for-text-and-sequences.html#cb533-6" aria-hidden="true" tabindex="-1"></a>  line <span class="ot">&lt;-</span> lines[[i]]</span>
<span id="cb533-7"><a href="deep-learning-for-text-and-sequences.html#cb533-7" aria-hidden="true" tabindex="-1"></a>  values <span class="ot">&lt;-</span> <span class="fu">strsplit</span>(line, <span class="st">&quot; &quot;</span>)[[<span class="dv">1</span>]]</span>
<span id="cb533-8"><a href="deep-learning-for-text-and-sequences.html#cb533-8" aria-hidden="true" tabindex="-1"></a>  word <span class="ot">&lt;-</span> values[[<span class="dv">1</span>]]</span>
<span id="cb533-9"><a href="deep-learning-for-text-and-sequences.html#cb533-9" aria-hidden="true" tabindex="-1"></a>  embeddings_index[[word]] <span class="ot">&lt;-</span> <span class="fu">as.double</span>(values[<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb533-10"><a href="deep-learning-for-text-and-sequences.html#cb533-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb533-11"><a href="deep-learning-for-text-and-sequences.html#cb533-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Found&quot;</span>, <span class="fu">length</span>(embeddings_index), <span class="st">&quot;word vectors.</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>Then we want to build</p>
<p>embedding vector = EV</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="deep-learning-for-text-and-sequences.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.11. Preparing the GloVe word-embeddings matrix</span></span>
<span id="cb534-2"><a href="deep-learning-for-text-and-sequences.html#cb534-2" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb534-3"><a href="deep-learning-for-text-and-sequences.html#cb534-3" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="fu">c</span>(max_words, embedding_dim))</span>
<span id="cb534-4"><a href="deep-learning-for-text-and-sequences.html#cb534-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb534-5"><a href="deep-learning-for-text-and-sequences.html#cb534-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (word <span class="cf">in</span> <span class="fu">names</span>(word_index)) { <span class="co">#Word_index is tokenized texts.</span></span>
<span id="cb534-6"><a href="deep-learning-for-text-and-sequences.html#cb534-6" aria-hidden="true" tabindex="-1"></a>  index <span class="ot">&lt;-</span> word_index[[word]]</span>
<span id="cb534-7"><a href="deep-learning-for-text-and-sequences.html#cb534-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (index <span class="sc">&lt;</span> max_words) {</span>
<span id="cb534-8"><a href="deep-learning-for-text-and-sequences.html#cb534-8" aria-hidden="true" tabindex="-1"></a>    embedding_vector <span class="ot">&lt;-</span> embeddings_index[[word]]</span>
<span id="cb534-9"><a href="deep-learning-for-text-and-sequences.html#cb534-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(embedding_vector))</span>
<span id="cb534-10"><a href="deep-learning-for-text-and-sequences.html#cb534-10" aria-hidden="true" tabindex="-1"></a>      embedding_matrix[index<span class="sc">+</span><span class="dv">1</span>,] <span class="ot">&lt;-</span> embedding_vector <span class="co">#NOTICE, words that are not in the EV will be 0</span></span>
<span id="cb534-11"><a href="deep-learning-for-text-and-sequences.html#cb534-11" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb534-12"><a href="deep-learning-for-text-and-sequences.html#cb534-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb534-13"><a href="deep-learning-for-text-and-sequences.html#cb534-13" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(embedding_matrix)</span></code></pre></div>
<p>We see that the matrix consists of 100 columns, hence corresponding to the max length of the reviews. And the rows correspond with the amount of words we are assessing. We see that we are only interested in the top 10.000 words.</p>
</div>
<div id="defining-a-model" class="section level4" number="8.1.3.2">
<h4><span class="header-section-number">8.1.3.2</span> Defining a model</h4>
<p>We create a network where we start with the word embedding, then we flatten, as an input for the densely connected layers, and in the end, we have the one unit layer, that assess the sentiment of the review.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="deep-learning-for-text-and-sequences.html#cb535-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.12. Model definition</span></span>
<span id="cb535-2"><a href="deep-learning-for-text-and-sequences.html#cb535-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb535-3"><a href="deep-learning-for-text-and-sequences.html#cb535-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="co">#10.000</span></span>
<span id="cb535-4"><a href="deep-learning-for-text-and-sequences.html#cb535-4" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">output_dim =</span> embedding_dim, <span class="co">#100</span></span>
<span id="cb535-5"><a href="deep-learning-for-text-and-sequences.html#cb535-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">input_length =</span> maxlen) <span class="sc">%&gt;%</span> <span class="co">#100</span></span>
<span id="cb535-6"><a href="deep-learning-for-text-and-sequences.html#cb535-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span></span>
<span id="cb535-7"><a href="deep-learning-for-text-and-sequences.html#cb535-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb535-8"><a href="deep-learning-for-text-and-sequences.html#cb535-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb535-9"><a href="deep-learning-for-text-and-sequences.html#cb535-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<p>We see that the embedded layer is corresponding with the amount of entries in the embedding matrix (hence 10.000 * 100).</p>
<p><em>Notice, that we donât want to train the embedded words, hence we freeze the layers</em></p>
</div>
<div id="loading-glove-embeddings-in-the-model" class="section level4" number="8.1.3.3">
<h4><span class="header-section-number">8.1.3.3</span> Loading GloVe embeddings in the model</h4>
<p>Now we are going to load the layer.</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="deep-learning-for-text-and-sequences.html#cb536-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.13. Loading pretrained word embeddings into the embedding layer</span></span>
<span id="cb536-2"><a href="deep-learning-for-text-and-sequences.html#cb536-2" aria-hidden="true" tabindex="-1"></a><span class="fu">get_layer</span>(<span class="at">object =</span> model</span>
<span id="cb536-3"><a href="deep-learning-for-text-and-sequences.html#cb536-3" aria-hidden="true" tabindex="-1"></a>          ,<span class="at">index =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> <span class="co">#What layer are we calling</span></span>
<span id="cb536-4"><a href="deep-learning-for-text-and-sequences.html#cb536-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_weights</span>(<span class="fu">list</span>(embedding_matrix)) <span class="sc">%&gt;%</span> <span class="co">#Setting the weights </span></span>
<span id="cb536-5"><a href="deep-learning-for-text-and-sequences.html#cb536-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">freeze_weights</span>() <span class="co">#We want to freeze these weights</span></span></code></pre></div>
</div>
<div id="training-and-evaluating-the-model" class="section level4" number="8.1.3.4">
<h4><span class="header-section-number">8.1.3.4</span> Training and evaluating the model</h4>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="deep-learning-for-text-and-sequences.html#cb537-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.14. Training and evaluation</span></span>
<span id="cb537-2"><a href="deep-learning-for-text-and-sequences.html#cb537-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb537-3"><a href="deep-learning-for-text-and-sequences.html#cb537-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb537-4"><a href="deep-learning-for-text-and-sequences.html#cb537-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb537-5"><a href="deep-learning-for-text-and-sequences.html#cb537-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>)</span>
<span id="cb537-6"><a href="deep-learning-for-text-and-sequences.html#cb537-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb537-7"><a href="deep-learning-for-text-and-sequences.html#cb537-7" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb537-8"><a href="deep-learning-for-text-and-sequences.html#cb537-8" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb537-9"><a href="deep-learning-for-text-and-sequences.html#cb537-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb537-10"><a href="deep-learning-for-text-and-sequences.html#cb537-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">32</span>,</span>
<span id="cb537-11"><a href="deep-learning-for-text-and-sequences.html#cb537-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> <span class="fu">list</span>(x_val, y_val)</span>
<span id="cb537-12"><a href="deep-learning-for-text-and-sequences.html#cb537-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb537-13"><a href="deep-learning-for-text-and-sequences.html#cb537-13" aria-hidden="true" tabindex="-1"></a><span class="fu">save_model_weights_hdf5</span>(model, <span class="st">&quot;Saved Objects/pre_trained_glove_model.h5&quot;</span>)</span></code></pre></div>
<p>Now we can plot the results.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="deep-learning-for-text-and-sequences.html#cb538-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>We see that the model quickly starts overfitting and thus the validation data has really poor performance.</p>
</div>
<div id="training-and-evaluating-without-glove" class="section level4" number="8.1.3.5">
<h4><span class="header-section-number">8.1.3.5</span> Training and evaluating without GloVe</h4>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="deep-learning-for-text-and-sequences.html#cb539-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.16. Training the same model without pretrained word embeddings</span></span>
<span id="cb539-2"><a href="deep-learning-for-text-and-sequences.html#cb539-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb539-3"><a href="deep-learning-for-text-and-sequences.html#cb539-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words, <span class="at">output_dim =</span> embedding_dim,</span>
<span id="cb539-4"><a href="deep-learning-for-text-and-sequences.html#cb539-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">input_length =</span> maxlen) <span class="sc">%&gt;%</span></span>
<span id="cb539-5"><a href="deep-learning-for-text-and-sequences.html#cb539-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span></span>
<span id="cb539-6"><a href="deep-learning-for-text-and-sequences.html#cb539-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb539-7"><a href="deep-learning-for-text-and-sequences.html#cb539-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb539-8"><a href="deep-learning-for-text-and-sequences.html#cb539-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb539-9"><a href="deep-learning-for-text-and-sequences.html#cb539-9" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb539-10"><a href="deep-learning-for-text-and-sequences.html#cb539-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb539-11"><a href="deep-learning-for-text-and-sequences.html#cb539-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb539-12"><a href="deep-learning-for-text-and-sequences.html#cb539-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>)</span>
<span id="cb539-13"><a href="deep-learning-for-text-and-sequences.html#cb539-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb539-14"><a href="deep-learning-for-text-and-sequences.html#cb539-14" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb539-15"><a href="deep-learning-for-text-and-sequences.html#cb539-15" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb539-16"><a href="deep-learning-for-text-and-sequences.html#cb539-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb539-17"><a href="deep-learning-for-text-and-sequences.html#cb539-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">32</span>,</span>
<span id="cb539-18"><a href="deep-learning-for-text-and-sequences.html#cb539-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> <span class="fu">list</span>(x_val, y_val)</span>
<span id="cb539-19"><a href="deep-learning-for-text-and-sequences.html#cb539-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="deep-learning-for-text-and-sequences.html#cb540-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>Now we see that the overfitting looks even worse also the accuracy on the validation data is worse.</p>
<p>That is also kinda expected, as we have very little train data, so we may by chance be fitting to a very little foundation.</p>
</div>
<div id="using-test-data" class="section level4" number="8.1.3.6">
<h4><span class="header-section-number">8.1.3.6</span> Using test data</h4>
<p>First we must tokenize the test data. This is what we also did with the train and validation data.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="deep-learning-for-text-and-sequences.html#cb541-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.17. Tokenizing the data of the test set</span></span>
<span id="cb541-2"><a href="deep-learning-for-text-and-sequences.html#cb541-2" aria-hidden="true" tabindex="-1"></a>test_dir <span class="ot">&lt;-</span> <span class="fu">file.path</span>(imdb_dir, <span class="st">&quot;test&quot;</span>)</span>
<span id="cb541-3"><a href="deep-learning-for-text-and-sequences.html#cb541-3" aria-hidden="true" tabindex="-1"></a>labels <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb541-4"><a href="deep-learning-for-text-and-sequences.html#cb541-4" aria-hidden="true" tabindex="-1"></a>texts <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb541-5"><a href="deep-learning-for-text-and-sequences.html#cb541-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (label_type <span class="cf">in</span> <span class="fu">c</span>(<span class="st">&quot;neg&quot;</span>, <span class="st">&quot;pos&quot;</span>)) {</span>
<span id="cb541-6"><a href="deep-learning-for-text-and-sequences.html#cb541-6" aria-hidden="true" tabindex="-1"></a>  label <span class="ot">&lt;-</span> <span class="cf">switch</span>(label_type, <span class="at">neg =</span> <span class="dv">0</span>, <span class="at">pos =</span> <span class="dv">1</span>)</span>
<span id="cb541-7"><a href="deep-learning-for-text-and-sequences.html#cb541-7" aria-hidden="true" tabindex="-1"></a>  dir_name <span class="ot">&lt;-</span> <span class="fu">file.path</span>(test_dir, label_type)</span>
<span id="cb541-8"><a href="deep-learning-for-text-and-sequences.html#cb541-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (fname <span class="cf">in</span> <span class="fu">list.files</span>(dir_name, <span class="at">pattern =</span> <span class="fu">glob2rx</span>(<span class="st">&quot;*.txt&quot;</span>),</span>
<span id="cb541-9"><a href="deep-learning-for-text-and-sequences.html#cb541-9" aria-hidden="true" tabindex="-1"></a>                           <span class="at">full.names =</span> <span class="cn">TRUE</span>)) {</span>
<span id="cb541-10"><a href="deep-learning-for-text-and-sequences.html#cb541-10" aria-hidden="true" tabindex="-1"></a>    texts <span class="ot">&lt;-</span> <span class="fu">c</span>(texts, <span class="fu">readChar</span>(fname, <span class="fu">file.info</span>(fname)<span class="sc">$</span>size))</span>
<span id="cb541-11"><a href="deep-learning-for-text-and-sequences.html#cb541-11" aria-hidden="true" tabindex="-1"></a>    labels <span class="ot">&lt;-</span> <span class="fu">c</span>(labels, label)</span>
<span id="cb541-12"><a href="deep-learning-for-text-and-sequences.html#cb541-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb541-13"><a href="deep-learning-for-text-and-sequences.html#cb541-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb541-14"><a href="deep-learning-for-text-and-sequences.html#cb541-14" aria-hidden="true" tabindex="-1"></a>sequences <span class="ot">&lt;-</span> <span class="fu">texts_to_sequences</span>(tokenizer, texts)</span>
<span id="cb541-15"><a href="deep-learning-for-text-and-sequences.html#cb541-15" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(sequences, <span class="at">maxlen =</span> maxlen)</span>
<span id="cb541-16"><a href="deep-learning-for-text-and-sequences.html#cb541-16" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">as.array</span>(labels)</span></code></pre></div>
<p>Testing the model</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="deep-learning-for-text-and-sequences.html#cb542-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.18. Evaluating the model on the test set</span></span>
<span id="cb542-2"><a href="deep-learning-for-text-and-sequences.html#cb542-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb542-3"><a href="deep-learning-for-text-and-sequences.html#cb542-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">load_model_weights_hdf5</span>(<span class="st">&quot;Saved Objects/pre_trained_glove_model.h5&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb542-4"><a href="deep-learning-for-text-and-sequences.html#cb542-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">evaluate</span>(x_test, y_test)</span></code></pre></div>
<p>We get an accuracy of 55%. Notice that is with only 200 reviews</p>
</div>
</div>
</div>
<div id="understanding-recurrent-neural-networks-rnn" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Understanding Recurrent Neural Networks (RNN)</h2>
<p>So far we have seen densely connected feed forward networks along with convoluted networks. These lack ability of storing memory. Meaning that each input that the layers are given, they will regard them individually. That is a problem when we have data that is naturally followed in sequences, e.g., text or time series data.</p>
<p>To deal with this, we introduce recurrent neural networks. These enable the model the <em>âmemorizeâ</em> what it has previously seen.</p>
<hr />
<p><em>For example with IMDB reviews, each review is an input and all the tokens from each review are conveyed in their right order. Hence tokens are sequentially analyzed, while taking in account past words. When the review is over, the loop will reset.</em></p>
<hr />
<p>We notice that it is not possible to calculate Gradients for the layers, as we are in a recurrent scenario.</p>
<p><strong>ReLU</strong>: We see that the activation function is slightly iterated, where we add the state, hence something for the past memory, hence doing an activation based on that. <strong><em>This is the essence of the RNN, as that is what will decide whether the neurons will light up or not.</em></strong> This can be shown in the following way:</p>
<p><img src="C:/Users/402137/AppData/Local/RStudio/tmp/paste-2C32204C.png" /></p>
<p>We also tend to use the <code>tanh</code> function: this is the hyperbolic tangent function. It puts on numbers in the scale -1 and 1, so similar to sigmoid, where the function can take on values between 0 and 1.</p>
<p>Why tanh and not ReLU:</p>
<ul>
<li>ReLU = exploding gradient issue. This can leave you with bad number approximation, heavy use of memory and similar. That is because the result can become very large very quick.</li>
<li>Tanh will help keep the outcome stable.</li>
</ul>
<p><strong>A toy example</strong></p>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="deep-learning-for-text-and-sequences.html#cb543-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.21. R implementation of a simple RNN</span></span>
<span id="cb543-2"><a href="deep-learning-for-text-and-sequences.html#cb543-2" aria-hidden="true" tabindex="-1"></a>timesteps <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co">#No. of timestamps</span></span>
<span id="cb543-3"><a href="deep-learning-for-text-and-sequences.html#cb543-3" aria-hidden="true" tabindex="-1"></a>input_features <span class="ot">&lt;-</span> <span class="dv">32</span> <span class="co">#Dimensionality of input feature space</span></span>
<span id="cb543-4"><a href="deep-learning-for-text-and-sequences.html#cb543-4" aria-hidden="true" tabindex="-1"></a>output_features <span class="ot">&lt;-</span> <span class="dv">64</span> <span class="co">#Dimensionality of input feature space</span></span>
<span id="cb543-5"><a href="deep-learning-for-text-and-sequences.html#cb543-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb543-6"><a href="deep-learning-for-text-and-sequences.html#cb543-6" aria-hidden="true" tabindex="-1"></a>random_array <span class="ot">&lt;-</span> <span class="cf">function</span>(dim) {</span>
<span id="cb543-7"><a href="deep-learning-for-text-and-sequences.html#cb543-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">array</span>(<span class="fu">runif</span>(<span class="fu">prod</span>(dim)), <span class="at">dim =</span> dim)</span>
<span id="cb543-8"><a href="deep-learning-for-text-and-sequences.html#cb543-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb543-9"><a href="deep-learning-for-text-and-sequences.html#cb543-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb543-10"><a href="deep-learning-for-text-and-sequences.html#cb543-10" aria-hidden="true" tabindex="-1"></a>inputs <span class="ot">&lt;-</span> <span class="fu">random_array</span>(<span class="at">dim =</span> <span class="fu">c</span>(timesteps, input_features)) <span class="co">#Random input noice</span></span>
<span id="cb543-11"><a href="deep-learning-for-text-and-sequences.html#cb543-11" aria-hidden="true" tabindex="-1"></a>state_t <span class="ot">&lt;-</span> <span class="fu">rep_len</span>(<span class="dv">0</span>, <span class="at">length =</span> <span class="fu">c</span>(output_features)) <span class="co">#Set all states to 0</span></span>
<span id="cb543-12"><a href="deep-learning-for-text-and-sequences.html#cb543-12" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">random_array</span>(<span class="at">dim =</span> <span class="fu">c</span>(output_features, input_features)) <span class="co">#Random weight vector</span></span>
<span id="cb543-13"><a href="deep-learning-for-text-and-sequences.html#cb543-13" aria-hidden="true" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="fu">random_array</span>(<span class="at">dim =</span> <span class="fu">c</span>(output_features, output_features)) <span class="co">#Random weight vector</span></span>
<span id="cb543-14"><a href="deep-learning-for-text-and-sequences.html#cb543-14" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">random_array</span>(<span class="at">dim =</span> <span class="fu">c</span>(output_features, <span class="dv">1</span>)) <span class="co">#Random weight vector</span></span>
<span id="cb543-15"><a href="deep-learning-for-text-and-sequences.html#cb543-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb543-16"><a href="deep-learning-for-text-and-sequences.html#cb543-16" aria-hidden="true" tabindex="-1"></a><span class="co">#One can show the dimensions of the wheights</span></span>
<span id="cb543-17"><a href="deep-learning-for-text-and-sequences.html#cb543-17" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(W) <span class="co">#[1] 64 32</span></span>
<span id="cb543-18"><a href="deep-learning-for-text-and-sequences.html#cb543-18" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(U) <span class="co">#[1] 64 64</span></span>
<span id="cb543-19"><a href="deep-learning-for-text-and-sequences.html#cb543-19" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(b) <span class="co">#[1] 64  1</span></span>
<span id="cb543-20"><a href="deep-learning-for-text-and-sequences.html#cb543-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb543-21"><a href="deep-learning-for-text-and-sequences.html#cb543-21" aria-hidden="true" tabindex="-1"></a>output_sequence <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(timesteps, output_features))</span>
<span id="cb543-22"><a href="deep-learning-for-text-and-sequences.html#cb543-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb543-23"><a href="deep-learning-for-text-and-sequences.html#cb543-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(inputs)) {</span>
<span id="cb543-24"><a href="deep-learning-for-text-and-sequences.html#cb543-24" aria-hidden="true" tabindex="-1"></a>  input_t <span class="ot">&lt;-</span> inputs[i,] <span class="co">#The first column in the input data</span></span>
<span id="cb543-25"><a href="deep-learning-for-text-and-sequences.html#cb543-25" aria-hidden="true" tabindex="-1"></a>  output_t <span class="ot">&lt;-</span> <span class="fu">tanh</span>(<span class="fu">as.numeric</span>((W <span class="sc">%*%</span> input_t) <span class="sc">+</span> (U <span class="sc">%*%</span> state_t) <span class="sc">+</span> b)) <span class="co">#Combination of current state and the input, thus we memorize</span></span>
<span id="cb543-26"><a href="deep-learning-for-text-and-sequences.html#cb543-26" aria-hidden="true" tabindex="-1"></a>  output_sequence[i,] <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(output_t) <span class="co">#Updating the results matrix</span></span>
<span id="cb543-27"><a href="deep-learning-for-text-and-sequences.html#cb543-27" aria-hidden="true" tabindex="-1"></a>  state_t <span class="ot">&lt;-</span> output_t <span class="co">#Updating the state of the network</span></span>
<span id="cb543-28"><a href="deep-learning-for-text-and-sequences.html#cb543-28" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We see that we first set the state is set to 0, while we are iterating through each timestamp. This is basically what the RNN does. Thus, one can say that RNN is just a for loop, which reuse computations from previous steps.</p>
<p>We see that the output_sequence (being the result matrix), is a 2D tensor.</p>
<p><em>Notice that the example above only simulates what would be one input.</em></p>
<div id="a-recurrent-layer-in-keras" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> A recurrent layer in Keras</h3>
<p>The most simple layer is <code>layer_simple_rnn(units = 32)</code>. This function actually take batch sizes into account, hence we add another dimension, so we are now working with a 3D tensor input.</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="deep-learning-for-text-and-sequences.html#cb544-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb544-2"><a href="deep-learning-for-text-and-sequences.html#cb544-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb544-3"><a href="deep-learning-for-text-and-sequences.html#cb544-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> <span class="dv">10000</span></span>
<span id="cb544-4"><a href="deep-learning-for-text-and-sequences.html#cb544-4" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb544-5"><a href="deep-learning-for-text-and-sequences.html#cb544-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>)</span>
<span id="cb544-6"><a href="deep-learning-for-text-and-sequences.html#cb544-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<p>We can also stack more layers on-top of each other.</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="deep-learning-for-text-and-sequences.html#cb545-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb545-2"><a href="deep-learning-for-text-and-sequences.html#cb545-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> <span class="dv">10000</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-3"><a href="deep-learning-for-text-and-sequences.html#cb545-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">return_sequences =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-4"><a href="deep-learning-for-text-and-sequences.html#cb545-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">return_sequences =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-5"><a href="deep-learning-for-text-and-sequences.html#cb545-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">return_sequences =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-6"><a href="deep-learning-for-text-and-sequences.html#cb545-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>) <span class="co">#We only want the last output</span></span>
<span id="cb545-7"><a href="deep-learning-for-text-and-sequences.html#cb545-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<p>We can then use this model on the IMDB movie-review-classification problem. Before we can train the model, we must prepare the data.</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="deep-learning-for-text-and-sequences.html#cb546-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.22. Preparing the IMDB data</span></span>
<span id="cb546-2"><a href="deep-learning-for-text-and-sequences.html#cb546-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb546-3"><a href="deep-learning-for-text-and-sequences.html#cb546-3" aria-hidden="true" tabindex="-1"></a>max_features <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#No. of words to consider as features</span></span>
<span id="cb546-4"><a href="deep-learning-for-text-and-sequences.html#cb546-4" aria-hidden="true" tabindex="-1"></a>maxlen <span class="ot">&lt;-</span> <span class="dv">500</span> <span class="co">#At what point are we cutting reviews</span></span>
<span id="cb546-5"><a href="deep-learning-for-text-and-sequences.html#cb546-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">32</span></span>
<span id="cb546-6"><a href="deep-learning-for-text-and-sequences.html#cb546-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Loading data...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb546-7"><a href="deep-learning-for-text-and-sequences.html#cb546-7" aria-hidden="true" tabindex="-1"></a>imdb <span class="ot">&lt;-</span> <span class="fu">dataset_imdb</span>(<span class="at">num_words =</span> max_features)</span>
<span id="cb546-8"><a href="deep-learning-for-text-and-sequences.html#cb546-8" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(input_train, y_train), <span class="fu">c</span>(input_test, y_test)) <span class="sc">%&lt;-%</span> imdb</span>
<span id="cb546-9"><a href="deep-learning-for-text-and-sequences.html#cb546-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">length</span>(input_train), <span class="st">&quot;train sequences</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb546-10"><a href="deep-learning-for-text-and-sequences.html#cb546-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">length</span>(input_test), <span class="st">&quot;test sequences&quot;</span>)</span>
<span id="cb546-11"><a href="deep-learning-for-text-and-sequences.html#cb546-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Pad sequences (samples x time)</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb546-12"><a href="deep-learning-for-text-and-sequences.html#cb546-12" aria-hidden="true" tabindex="-1"></a>input_train <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(input_train <span class="co">#Pads sequences to the same length</span></span>
<span id="cb546-13"><a href="deep-learning-for-text-and-sequences.html#cb546-13" aria-hidden="true" tabindex="-1"></a>                             ,<span class="at">maxlen =</span> maxlen</span>
<span id="cb546-14"><a href="deep-learning-for-text-and-sequences.html#cb546-14" aria-hidden="true" tabindex="-1"></a>                             ,<span class="at">value =</span> <span class="dv">0</span>) <span class="co">#Default, fills with 0&#39;s. </span></span>
<span id="cb546-15"><a href="deep-learning-for-text-and-sequences.html#cb546-15" aria-hidden="true" tabindex="-1"></a>input_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(input_test, <span class="at">maxlen =</span> maxlen) <span class="co">#Pads sequences to the same length</span></span>
<span id="cb546-16"><a href="deep-learning-for-text-and-sequences.html#cb546-16" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;input_train shape:&quot;</span>, <span class="fu">dim</span>(input_train), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb546-17"><a href="deep-learning-for-text-and-sequences.html#cb546-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;input_test shape:&quot;</span>, <span class="fu">dim</span>(input_test), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>Now we can train the model.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="deep-learning-for-text-and-sequences.html#cb547-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.23. Training the model with embedding and simple RNN layers</span></span>
<span id="cb547-2"><a href="deep-learning-for-text-and-sequences.html#cb547-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb547-3"><a href="deep-learning-for-text-and-sequences.html#cb547-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_features, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb547-4"><a href="deep-learning-for-text-and-sequences.html#cb547-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb547-5"><a href="deep-learning-for-text-and-sequences.html#cb547-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb547-6"><a href="deep-learning-for-text-and-sequences.html#cb547-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb547-7"><a href="deep-learning-for-text-and-sequences.html#cb547-7" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb547-8"><a href="deep-learning-for-text-and-sequences.html#cb547-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb547-9"><a href="deep-learning-for-text-and-sequences.html#cb547-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb547-10"><a href="deep-learning-for-text-and-sequences.html#cb547-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>)</span>
<span id="cb547-11"><a href="deep-learning-for-text-and-sequences.html#cb547-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb547-12"><a href="deep-learning-for-text-and-sequences.html#cb547-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb547-13"><a href="deep-learning-for-text-and-sequences.html#cb547-13" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb547-14"><a href="deep-learning-for-text-and-sequences.html#cb547-14" aria-hidden="true" tabindex="-1"></a>  input_train, y_train,</span>
<span id="cb547-15"><a href="deep-learning-for-text-and-sequences.html#cb547-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb547-16"><a href="deep-learning-for-text-and-sequences.html#cb547-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb547-17"><a href="deep-learning-for-text-and-sequences.html#cb547-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb547-18"><a href="deep-learning-for-text-and-sequences.html#cb547-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="deep-learning-for-text-and-sequences.html#cb548-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.24. Plotting results</span></span>
<span id="cb548-2"><a href="deep-learning-for-text-and-sequences.html#cb548-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>In chapter three, we also dealt with this data and got an accuracy of 88%. There we used all data.</p>
<p>In this example we are only evaluating the first 500 words, and get an accuracy of a bit above 75% (in the book). In my example we are able to get validation accuracy of 85%, hence close to the baseline from chapter 3. So it is not better than what we have seen earlier.</p>
<p>Also it looks like we are overfitting in this approach.</p>
<p>Also <code>layer_simple_rnn</code> is not the best approach for dealing with long sequences, hence the result is kinda expected.</p>
</div>
<div id="understanding-the-lstm-and-gru-layers" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Understanding the LSTM and GRU layers</h3>
<p>Now we are going to introduce two new concepts. The reason is that the <code>layer_simple_rnn</code> is too simple to deal with actual sequences, as knowledge from previous steps vanish with the loops that it is iterating through, hence earlier data will overwritten/ruled out too quick. This is called <strong><em>the vanishing gradient problem</em></strong>, also called <strong><em>the long term dependency problem</em></strong>.</p>
<p><strong>LSTM = Long Short-Term Memory</strong>, is an approach that saves information for later, hence preventing that older signals gets ruled out. In the book <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018</a>, pg. 186 - 188)</span> they describe it more into details, but the key take-away is that <strong><em>LSTM allows past information to be reinjected at a liter time, to fight the vanishing gradient problem</em></strong>.</p>
<div id="units-inside-gru-and-lstm" class="section level4" number="8.2.2.1">
<h4><span class="header-section-number">8.2.2.1</span> Units inside GRU and LSTM</h4>
<p>Recall from simple RNNs that the hidden state is the informaton that is handed over between to iterations, so kind alike memory.</p>
<p>We see that in GRU we have the following:</p>
<p><strong>GRU</strong></p>
<ul>
<li>Update gate: This is similar to the input and forget gate from LSTM. Basically it decides whether to keep or throw away information.</li>
<li>Reset gate: Decides how much past information to forget.</li>
</ul>
<p><strong>LSTM</strong></p>
<ul>
<li>Input gate: Decides whether the gate statuses should be updated. It evaluates the new information, both the new data and also the hidden state. Then it decides whether to update the state or not.</li>
<li>Forget gate: LSTM is able to decide to forget information. It applies a sigmoid function (somewhere between 0 and 1), to decide whether the information should be passed on or not. If the sigmoid activation = 0, then no information is passed on. His example is <code>information * activation = output</code>. if sigmoid activation between 0 and 1, then some informatino gets through and if = 1, then all information is let through. Notice that this stage is evaluating both the input and the hidden state input.</li>
<li>Output gate: Fist the two previous gates are passed, now we know what to keep and what to leave out. This gate merely updates the cell status. Notice, that we do not have this specific gate in the GRU.</li>
</ul>
</div>
</div>
<div id="a-concrete-lstm-example-in-keras" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> A concrete LSTM example in Keras</h3>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="deep-learning-for-text-and-sequences.html#cb549-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.27. Using the LSTM layer in Keras</span></span>
<span id="cb549-2"><a href="deep-learning-for-text-and-sequences.html#cb549-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb549-3"><a href="deep-learning-for-text-and-sequences.html#cb549-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_features, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb549-4"><a href="deep-learning-for-text-and-sequences.html#cb549-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb549-5"><a href="deep-learning-for-text-and-sequences.html#cb549-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb549-6"><a href="deep-learning-for-text-and-sequences.html#cb549-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb549-7"><a href="deep-learning-for-text-and-sequences.html#cb549-7" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb549-8"><a href="deep-learning-for-text-and-sequences.html#cb549-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb549-9"><a href="deep-learning-for-text-and-sequences.html#cb549-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb549-10"><a href="deep-learning-for-text-and-sequences.html#cb549-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>)</span>
<span id="cb549-11"><a href="deep-learning-for-text-and-sequences.html#cb549-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb549-12"><a href="deep-learning-for-text-and-sequences.html#cb549-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb549-13"><a href="deep-learning-for-text-and-sequences.html#cb549-13" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb549-14"><a href="deep-learning-for-text-and-sequences.html#cb549-14" aria-hidden="true" tabindex="-1"></a>  input_train, y_train,</span>
<span id="cb549-15"><a href="deep-learning-for-text-and-sequences.html#cb549-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb549-16"><a href="deep-learning-for-text-and-sequences.html#cb549-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb549-17"><a href="deep-learning-for-text-and-sequences.html#cb549-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb549-18"><a href="deep-learning-for-text-and-sequences.html#cb549-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="deep-learning-for-text-and-sequences.html#cb550-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>We see that the model is able to achieve up to 88% accuracy. Notice, that this is using a default model with no fine-tuning and also we are cutting the reviews at 500 words. Thus, it proves that when we attempt to get rid of the vanishing gradient problem (the previous information being forgotten), we are able to improve the model.</p>
<p><strong>What is LSTM good at?</strong></p>
<ol style="list-style-type: decimal">
<li>Sentiment-analysis problems</li>
</ol>
</div>
</div>
<div id="advanced-use-of-recurrent-neural-networks" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Advanced use of Recurrent neural networks</h2>
<p>Now we will dig a bit deeper and the literature presents three advanced techniques for improving the performance and generalization power of recurrent neural networks. That being:</p>
<ol style="list-style-type: decimal">
<li>Recurrent dropout, dropout like we have seen earlier to fight overfitting.</li>
<li>Stacking recurrent layers, to increase the representational power of the network.</li>
<li>Bidirectional recurrent layers, here we are able to present the same information to a recurrent network in different ways. This attempts to overcome gradient vanishing problems and hence improve the accuracy.</li>
</ol>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="deep-learning-for-text-and-sequences.html#cb551-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
<div id="a-temperature-forecasting-problem" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> A temperature-forecasting problem</h3>
<p>Now we are going to look at numeric data that comes in a natural time series. Hence we will look at weather data and predict temperature.</p>
<p>Notice that we have one observation for each 10 minutes.</p>
<p>First we must create a file directory and then download the data.</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="deep-learning-for-text-and-sequences.html#cb552-1" aria-hidden="true" tabindex="-1"></a>dir.download <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="st">&quot;Data/3. Deep Learning/jena_climate&quot;</span>)</span>
<span id="cb552-2"><a href="deep-learning-for-text-and-sequences.html#cb552-2" aria-hidden="true" tabindex="-1"></a><span class="co"># dir.create(dir.download, recursive = TRUE)</span></span>
<span id="cb552-3"><a href="deep-learning-for-text-and-sequences.html#cb552-3" aria-hidden="true" tabindex="-1"></a><span class="co"># download.file(url = &quot;https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip&quot;</span></span>
<span id="cb552-4"><a href="deep-learning-for-text-and-sequences.html#cb552-4" aria-hidden="true" tabindex="-1"></a><span class="co">#               ,destfile = file.path(dir.download,&quot;jena_climate_2009_2016.csv.zip&quot;)</span></span>
<span id="cb552-5"><a href="deep-learning-for-text-and-sequences.html#cb552-5" aria-hidden="true" tabindex="-1"></a><span class="co">#               )</span></span>
<span id="cb552-6"><a href="deep-learning-for-text-and-sequences.html#cb552-6" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb552-7"><a href="deep-learning-for-text-and-sequences.html#cb552-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fname &lt;- file.path(dir.download, &quot;jena_climate_2009_2016.csv.zip&quot;)</span></span>
<span id="cb552-8"><a href="deep-learning-for-text-and-sequences.html#cb552-8" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb552-9"><a href="deep-learning-for-text-and-sequences.html#cb552-9" aria-hidden="true" tabindex="-1"></a><span class="co"># unzip(zipfile = fname</span></span>
<span id="cb552-10"><a href="deep-learning-for-text-and-sequences.html#cb552-10" aria-hidden="true" tabindex="-1"></a><span class="co">#       ,exdir = dir.download</span></span>
<span id="cb552-11"><a href="deep-learning-for-text-and-sequences.html#cb552-11" aria-hidden="true" tabindex="-1"></a><span class="co">#       )</span></span></code></pre></div>
<p>Now we can take a look at the data</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="deep-learning-for-text-and-sequences.html#cb553-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tibble)</span>
<span id="cb553-2"><a href="deep-learning-for-text-and-sequences.html#cb553-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb553-3"><a href="deep-learning-for-text-and-sequences.html#cb553-3" aria-hidden="true" tabindex="-1"></a>data_dir <span class="ot">&lt;-</span> dir.download</span>
<span id="cb553-4"><a href="deep-learning-for-text-and-sequences.html#cb553-4" aria-hidden="true" tabindex="-1"></a>fname <span class="ot">&lt;-</span> <span class="fu">file.path</span>(data_dir, <span class="st">&quot;jena_climate_2009_2016.csv&quot;</span>)</span>
<span id="cb553-5"><a href="deep-learning-for-text-and-sequences.html#cb553-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(fname)</span>
<span id="cb553-6"><a href="deep-learning-for-text-and-sequences.html#cb553-6" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(data)</span></code></pre></div>
<p>Now we can also do some exploration of the data.</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="deep-learning-for-text-and-sequences.html#cb554-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.29. Plotting the temperature timeseries</span></span>
<span id="cb554-2"><a href="deep-learning-for-text-and-sequences.html#cb554-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(data)</span>
<span id="cb554-3"><a href="deep-learning-for-text-and-sequences.html#cb554-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.new</span>()</span>
<span id="cb554-4"><a href="deep-learning-for-text-and-sequences.html#cb554-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rect</span>(<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">1</span>],<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">2</span>],<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">4</span>],<span class="at">col =</span> <span class="st">&quot;lightgray&quot;</span>,<span class="at">border =</span> <span class="st">&quot;white&quot;</span>)</span>
<span id="cb554-5"><a href="deep-learning-for-text-and-sequences.html#cb554-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new =</span> <span class="cn">TRUE</span>)</span>
<span id="cb554-6"><a href="deep-learning-for-text-and-sequences.html#cb554-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data)</span>
<span id="cb554-7"><a href="deep-learning-for-text-and-sequences.html#cb554-7" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">y =</span> <span class="st">`</span><span class="at">T (degC)</span><span class="st">`</span></span>
<span id="cb554-8"><a href="deep-learning-for-text-and-sequences.html#cb554-8" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">type =</span> <span class="st">&quot;l&quot;</span></span>
<span id="cb554-9"><a href="deep-learning-for-text-and-sequences.html#cb554-9" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">main =</span> <span class="st">&quot;Celcius Degrees&quot;</span></span>
<span id="cb554-10"><a href="deep-learning-for-text-and-sequences.html#cb554-10" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">axes =</span> <span class="cn">FALSE</span></span>
<span id="cb554-11"><a href="deep-learning-for-text-and-sequences.html#cb554-11" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">panel.first =</span> <span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;white&quot;</span>,<span class="at">lty =</span> <span class="dv">1</span>))</span>
<span id="cb554-12"><a href="deep-learning-for-text-and-sequences.html#cb554-12" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>,<span class="at">col =</span> <span class="cn">NA</span>,<span class="at">tick =</span> <span class="cn">TRUE</span>,<span class="at">col.ticks =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb554-13"><a href="deep-learning-for-text-and-sequences.html#cb554-13" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">2</span>,<span class="at">col =</span> <span class="cn">NA</span>,<span class="at">tick =</span> <span class="cn">TRUE</span>,<span class="at">col.ticks =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><em>I changed it to base graphics.</em></p>
<p>We can also look at the temperature for the first 10 days.</p>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="deep-learning-for-text-and-sequences.html#cb555-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.30. Plotting the first 10 days of the temperature timeseries</span></span>
<span id="cb555-2"><a href="deep-learning-for-text-and-sequences.html#cb555-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.new</span>()</span>
<span id="cb555-3"><a href="deep-learning-for-text-and-sequences.html#cb555-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rect</span>(<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">1</span>],<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">3</span>],<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">2</span>],<span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)[<span class="dv">4</span>],<span class="at">col =</span> <span class="st">&quot;lightgray&quot;</span>,<span class="at">border =</span> <span class="st">&quot;white&quot;</span>)</span>
<span id="cb555-4"><a href="deep-learning-for-text-and-sequences.html#cb555-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new =</span> <span class="cn">TRUE</span>)</span>
<span id="cb555-5"><a href="deep-learning-for-text-and-sequences.html#cb555-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(<span class="st">`</span><span class="at">T (degC)</span><span class="st">`</span>[<span class="dv">1</span><span class="sc">:</span><span class="dv">1440</span>]))</span>
<span id="cb555-6"><a href="deep-learning-for-text-and-sequences.html#cb555-6" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">y =</span> <span class="st">`</span><span class="at">T (degC)</span><span class="st">`</span>[<span class="dv">1</span><span class="sc">:</span><span class="dv">1440</span>]</span>
<span id="cb555-7"><a href="deep-learning-for-text-and-sequences.html#cb555-7" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">type =</span> <span class="st">&quot;l&quot;</span></span>
<span id="cb555-8"><a href="deep-learning-for-text-and-sequences.html#cb555-8" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">main =</span> <span class="st">&quot;Celcius Degrees&quot;</span></span>
<span id="cb555-9"><a href="deep-learning-for-text-and-sequences.html#cb555-9" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">axes =</span> <span class="cn">FALSE</span></span>
<span id="cb555-10"><a href="deep-learning-for-text-and-sequences.html#cb555-10" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">panel.first =</span> <span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;white&quot;</span>,<span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb555-11"><a href="deep-learning-for-text-and-sequences.html#cb555-11" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlab =</span> <span class="st">&quot;1:1440&quot;</span></span>
<span id="cb555-12"><a href="deep-learning-for-text-and-sequences.html#cb555-12" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">ylab =</span> <span class="st">&quot;T (degC)&quot;</span>)</span>
<span id="cb555-13"><a href="deep-learning-for-text-and-sequences.html#cb555-13" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>,<span class="at">col =</span> <span class="cn">NA</span>,<span class="at">tick =</span> <span class="cn">TRUE</span>,<span class="at">col.ticks =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb555-14"><a href="deep-learning-for-text-and-sequences.html#cb555-14" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">2</span>,<span class="at">col =</span> <span class="cn">NA</span>,<span class="at">tick =</span> <span class="cn">TRUE</span>,<span class="at">col.ticks =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><em>I changed it to base graphics.</em></p>
<p>It looks like this is from some winter month, as we are having degrees below 0.</p>
<p>As weather often is kinda the same year to year, are we also able to forecast in the coming period.</p>
</div>
<div id="preparing-the-data-3" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Preparing the data</h3>
<p>First we are going to experiment with making predictions based on previous data. Hence we set up some prerequisites:</p>
<ol style="list-style-type: decimal">
<li>We want to evaluate the last 10 days</li>
<li>The amount of observations that will be sampled at on data point per hour.</li>
<li>We want to predict this amount of periods into the future</li>
</ol>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="deep-learning-for-text-and-sequences.html#cb556-1" aria-hidden="true" tabindex="-1"></a>lookback <span class="ot">&lt;-</span> <span class="dv">1440</span></span>
<span id="cb556-2"><a href="deep-learning-for-text-and-sequences.html#cb556-2" aria-hidden="true" tabindex="-1"></a>step <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb556-3"><a href="deep-learning-for-text-and-sequences.html#cb556-3" aria-hidden="true" tabindex="-1"></a>delay <span class="ot">&lt;-</span> <span class="dv">144</span></span></code></pre></div>
<p>We also need to do some further preprocessing, namely:</p>
<ol style="list-style-type: decimal">
<li>Normalize the data, so it is on the same scale</li>
<li>Make a data generator</li>
</ol>
<p>First we will make a matrix with the numeric values and the normalize</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="deep-learning-for-text-and-sequences.html#cb557-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.31. Converting the data into a floating-point matrix</span></span>
<span id="cb557-2"><a href="deep-learning-for-text-and-sequences.html#cb557-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(data[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb557-3"><a href="deep-learning-for-text-and-sequences.html#cb557-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb557-4"><a href="deep-learning-for-text-and-sequences.html#cb557-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.32. Normalizing the data</span></span>
<span id="cb557-5"><a href="deep-learning-for-text-and-sequences.html#cb557-5" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> data[<span class="dv">1</span><span class="sc">:</span><span class="dv">200000</span>,]</span>
<span id="cb557-6"><a href="deep-learning-for-text-and-sequences.html#cb557-6" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_data, <span class="dv">2</span>, mean) <span class="co">#calc. the mean of each variable</span></span>
<span id="cb557-7"><a href="deep-learning-for-text-and-sequences.html#cb557-7" aria-hidden="true" tabindex="-1"></a>std <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_data, <span class="dv">2</span>, sd) <span class="co">#calc. st.dev. of each variable</span></span>
<span id="cb557-8"><a href="deep-learning-for-text-and-sequences.html#cb557-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">scale</span>(data, <span class="at">center =</span> mean, <span class="at">scale =</span> std) <span class="co">#scales according to the mean and st.dev</span></span></code></pre></div>
<p>Now we will make the data generator:</p>
<p>In its essence we want to shuffle the train data, while we want to preserve the chronoligical order for the validation and test data. The following code is able to handle different scenarios in respect of the predefined information. I have put some information in the code chunk.</p>
<hr />
<p><em>Notice that the ifâs does not have curly brackets, that is because they are only followed by a single expression. It can be tested with just running the if statement and the hereafter the console is ready to run another line. The following is an example just to show this.</em></p>
<p><em>An example with the If statements</em></p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="deep-learning-for-text-and-sequences.html#cb558-1" aria-hidden="true" tabindex="-1"></a>l <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb558-2"><a href="deep-learning-for-text-and-sequences.html#cb558-2" aria-hidden="true" tabindex="-1"></a>o <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb558-3"><a href="deep-learning-for-text-and-sequences.html#cb558-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb558-4"><a href="deep-learning-for-text-and-sequences.html#cb558-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;First if, which = TRUE&quot;</span>)</span>
<span id="cb558-5"><a href="deep-learning-for-text-and-sequences.html#cb558-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(l <span class="sc">&lt;</span> o)</span>
<span id="cb558-6"><a href="deep-learning-for-text-and-sequences.html#cb558-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">&quot;l is smaller tan o&quot;</span>)</span>
<span id="cb558-7"><a href="deep-learning-for-text-and-sequences.html#cb558-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb558-8"><a href="deep-learning-for-text-and-sequences.html#cb558-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Second if, which = FALSE&quot;</span>)</span>
<span id="cb558-9"><a href="deep-learning-for-text-and-sequences.html#cb558-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(l <span class="sc">&gt;</span> o)</span>
<span id="cb558-10"><a href="deep-learning-for-text-and-sequences.html#cb558-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">&quot;l is smaller tan o&quot;</span>)</span>
<span id="cb558-11"><a href="deep-learning-for-text-and-sequences.html#cb558-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb558-12"><a href="deep-learning-for-text-and-sequences.html#cb558-12" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(l)</span>
<span id="cb558-13"><a href="deep-learning-for-text-and-sequences.html#cb558-13" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(o)</span></code></pre></div>
<p><em>We see that the statement is only printed in the first scenario, as that is a TRUE statement</em></p>
<hr />
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="deep-learning-for-text-and-sequences.html#cb559-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.33. Generator yielding timeseries samples and their targets</span></span>
<span id="cb559-2"><a href="deep-learning-for-text-and-sequences.html#cb559-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">128</span> <span class="co">#Originally 128</span></span>
<span id="cb559-3"><a href="deep-learning-for-text-and-sequences.html#cb559-3" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">&lt;-</span> </span>
<span id="cb559-4"><a href="deep-learning-for-text-and-sequences.html#cb559-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb559-5"><a href="deep-learning-for-text-and-sequences.html#cb559-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Starting the function and declaring the input</span></span>
<span id="cb559-6"><a href="deep-learning-for-text-and-sequences.html#cb559-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(data,lookback, delay, min_index, max_index,</span>
<span id="cb559-7"><a href="deep-learning-for-text-and-sequences.html#cb559-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">shuffle =</span> <span class="cn">FALSE</span>,<span class="co">#Notice that we set default to be FALSE</span></span>
<span id="cb559-8"><a href="deep-learning-for-text-and-sequences.html#cb559-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> batch_size, <span class="at">step =</span> step) {</span>
<span id="cb559-9"><a href="deep-learning-for-text-and-sequences.html#cb559-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb559-10"><a href="deep-learning-for-text-and-sequences.html#cb559-10" aria-hidden="true" tabindex="-1"></a>  <span class="do">##For test##</span></span>
<span id="cb559-11"><a href="deep-learning-for-text-and-sequences.html#cb559-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">is.null</span>(max_index)) <span class="co">#If max_index = NULL</span></span>
<span id="cb559-12"><a href="deep-learning-for-text-and-sequences.html#cb559-12" aria-hidden="true" tabindex="-1"></a>    max_index <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data) <span class="sc">-</span> delay <span class="sc">-</span> <span class="dv">1</span> <span class="co">#Notice in the test partition we set max index to NULL</span></span>
<span id="cb559-13"><a href="deep-learning-for-text-and-sequences.html#cb559-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb559-14"><a href="deep-learning-for-text-and-sequences.html#cb559-14" aria-hidden="true" tabindex="-1"></a>  i <span class="ot">&lt;-</span> min_index <span class="sc">+</span> lookback</span>
<span id="cb559-15"><a href="deep-learning-for-text-and-sequences.html#cb559-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb559-16"><a href="deep-learning-for-text-and-sequences.html#cb559-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># If we are not generating the test data, then the following will be done</span></span>
<span id="cb559-17"><a href="deep-learning-for-text-and-sequences.html#cb559-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb559-18"><a href="deep-learning-for-text-and-sequences.html#cb559-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>() {</span>
<span id="cb559-19"><a href="deep-learning-for-text-and-sequences.html#cb559-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb559-20"><a href="deep-learning-for-text-and-sequences.html#cb559-20" aria-hidden="true" tabindex="-1"></a>    <span class="do">##For training##</span></span>
<span id="cb559-21"><a href="deep-learning-for-text-and-sequences.html#cb559-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (shuffle) { <span class="co">#If shuffle = TRUE</span></span>
<span id="cb559-22"><a href="deep-learning-for-text-and-sequences.html#cb559-22" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb559-23"><a href="deep-learning-for-text-and-sequences.html#cb559-23" aria-hidden="true" tabindex="-1"></a>      rows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>((min_index<span class="sc">+</span>lookback)<span class="sc">:</span>max_index) <span class="co">#Random selection</span></span>
<span id="cb559-24"><a href="deep-learning-for-text-and-sequences.html#cb559-24" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">size =</span> batch_size) <span class="co">#We want as many selections as our batch size</span></span>
<span id="cb559-25"><a href="deep-learning-for-text-and-sequences.html#cb559-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb559-26"><a href="deep-learning-for-text-and-sequences.html#cb559-26" aria-hidden="true" tabindex="-1"></a>    <span class="do">##For validation and test##</span></span>
<span id="cb559-27"><a href="deep-learning-for-text-and-sequences.html#cb559-27" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> { <span class="co">#If shuffle is not defined, it is by default FALSE</span></span>
<span id="cb559-28"><a href="deep-learning-for-text-and-sequences.html#cb559-28" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (i <span class="sc">+</span> batch_size <span class="sc">&gt;=</span> max_index) <span class="co">#Evaluates if we are working within the scope</span></span>
<span id="cb559-29"><a href="deep-learning-for-text-and-sequences.html#cb559-29" aria-hidden="true" tabindex="-1"></a>        i <span class="ot">&lt;&lt;-</span> min_index <span class="sc">+</span> lookback</span>
<span id="cb559-30"><a href="deep-learning-for-text-and-sequences.html#cb559-30" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb559-31"><a href="deep-learning-for-text-and-sequences.html#cb559-31" aria-hidden="true" tabindex="-1"></a>      <span class="co">#We don&#39;t want to go over the limit we set on the data partition</span></span>
<span id="cb559-32"><a href="deep-learning-for-text-and-sequences.html#cb559-32" aria-hidden="true" tabindex="-1"></a>      rows <span class="ot">&lt;-</span> <span class="fu">c</span>(i<span class="sc">:</span><span class="fu">min</span>((i<span class="sc">+</span>batch_size)<span class="sc">-</span><span class="dv">1</span>, max_index))</span>
<span id="cb559-33"><a href="deep-learning-for-text-and-sequences.html#cb559-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb559-34"><a href="deep-learning-for-text-and-sequences.html#cb559-34" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Prepare i for the next batch</span></span>
<span id="cb559-35"><a href="deep-learning-for-text-and-sequences.html#cb559-35" aria-hidden="true" tabindex="-1"></a>      i <span class="ot">&lt;&lt;-</span> i <span class="sc">+</span> <span class="fu">length</span>(rows)</span>
<span id="cb559-36"><a href="deep-learning-for-text-and-sequences.html#cb559-36" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb559-37"><a href="deep-learning-for-text-and-sequences.html#cb559-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb559-38"><a href="deep-learning-for-text-and-sequences.html#cb559-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Create empty arrays for data ingestion</span></span>
<span id="cb559-39"><a href="deep-learning-for-text-and-sequences.html#cb559-39" aria-hidden="true" tabindex="-1"></a>    samples <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(rows),</span>
<span id="cb559-40"><a href="deep-learning-for-text-and-sequences.html#cb559-40" aria-hidden="true" tabindex="-1"></a>                                lookback <span class="sc">/</span> step,</span>
<span id="cb559-41"><a href="deep-learning-for-text-and-sequences.html#cb559-41" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]]))</span>
<span id="cb559-42"><a href="deep-learning-for-text-and-sequences.html#cb559-42" aria-hidden="true" tabindex="-1"></a>    targets <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(rows)))</span>
<span id="cb559-43"><a href="deep-learning-for-text-and-sequences.html#cb559-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb559-44"><a href="deep-learning-for-text-and-sequences.html#cb559-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Ingest data to the arrays</span></span>
<span id="cb559-45"><a href="deep-learning-for-text-and-sequences.html#cb559-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rows)) { <span class="co">#Looping up to the batch size</span></span>
<span id="cb559-46"><a href="deep-learning-for-text-and-sequences.html#cb559-46" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb559-47"><a href="deep-learning-for-text-and-sequences.html#cb559-47" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Subsetting the data we need</span></span>
<span id="cb559-48"><a href="deep-learning-for-text-and-sequences.html#cb559-48" aria-hidden="true" tabindex="-1"></a>      indices <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> rows[[j]] <span class="sc">-</span> lookback</span>
<span id="cb559-49"><a href="deep-learning-for-text-and-sequences.html#cb559-49" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">to =</span> rows[[j]]</span>
<span id="cb559-50"><a href="deep-learning-for-text-and-sequences.html#cb559-50" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">length.out =</span> <span class="fu">dim</span>(samples)[[<span class="dv">2</span>]]) <span class="co">#The columns</span></span>
<span id="cb559-51"><a href="deep-learning-for-text-and-sequences.html#cb559-51" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb559-52"><a href="deep-learning-for-text-and-sequences.html#cb559-52" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Inserting the data into the samples</span></span>
<span id="cb559-53"><a href="deep-learning-for-text-and-sequences.html#cb559-53" aria-hidden="true" tabindex="-1"></a>      samples[j,,] <span class="ot">&lt;-</span> data[indices,] <span class="co">#Subsetting on rows</span></span>
<span id="cb559-54"><a href="deep-learning-for-text-and-sequences.html#cb559-54" aria-hidden="true" tabindex="-1"></a>      targets[[j]] <span class="ot">&lt;-</span> data[rows[[j]] <span class="sc">+</span> delay,<span class="dv">2</span>] <span class="co">#Notice that DV is column 2</span></span>
<span id="cb559-55"><a href="deep-learning-for-text-and-sequences.html#cb559-55" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb559-56"><a href="deep-learning-for-text-and-sequences.html#cb559-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb559-57"><a href="deep-learning-for-text-and-sequences.html#cb559-57" aria-hidden="true" tabindex="-1"></a>    <span class="co">#We compute a list of the object</span></span>
<span id="cb559-58"><a href="deep-learning-for-text-and-sequences.html#cb559-58" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(samples, targets)</span>
<span id="cb559-59"><a href="deep-learning-for-text-and-sequences.html#cb559-59" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb559-60"><a href="deep-learning-for-text-and-sequences.html#cb559-60" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we want to use the function to generate data. First we can represent the dimensions of the data.</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="deep-learning-for-text-and-sequences.html#cb560-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(data)</span></code></pre></div>
<p>We see that we have 420451 observations on 13 variables + the dependent variable. In the following we will in each data generation step select what observations we want. Thus, we use the data generator that was specified above to get the data.</p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="deep-learning-for-text-and-sequences.html#cb561-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.34. Preparing the training, validation, and test generators</span></span>
<span id="cb561-2"><a href="deep-learning-for-text-and-sequences.html#cb561-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb561-3"><a href="deep-learning-for-text-and-sequences.html#cb561-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb561-4"><a href="deep-learning-for-text-and-sequences.html#cb561-4" aria-hidden="true" tabindex="-1"></a>train_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb561-5"><a href="deep-learning-for-text-and-sequences.html#cb561-5" aria-hidden="true" tabindex="-1"></a>  data, <span class="co">#The normalized data</span></span>
<span id="cb561-6"><a href="deep-learning-for-text-and-sequences.html#cb561-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback, <span class="co">#How far we look back</span></span>
<span id="cb561-7"><a href="deep-learning-for-text-and-sequences.html#cb561-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay, <span class="co">#How far in the future we want to look</span></span>
<span id="cb561-8"><a href="deep-learning-for-text-and-sequences.html#cb561-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">1</span>, <span class="co">#First train observation</span></span>
<span id="cb561-9"><a href="deep-learning-for-text-and-sequences.html#cb561-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">200000</span>, <span class="co">#Last train observation</span></span>
<span id="cb561-10"><a href="deep-learning-for-text-and-sequences.html#cb561-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="co">#Do we want to shuffle the data or take it chronoligical?</span></span>
<span id="cb561-11"><a href="deep-learning-for-text-and-sequences.html#cb561-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step, <span class="co">#The period (in timesteps)</span></span>
<span id="cb561-12"><a href="deep-learning-for-text-and-sequences.html#cb561-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size <span class="co">#The amount of samples pr. bach</span></span>
<span id="cb561-13"><a href="deep-learning-for-text-and-sequences.html#cb561-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb561-14"><a href="deep-learning-for-text-and-sequences.html#cb561-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb561-15"><a href="deep-learning-for-text-and-sequences.html#cb561-15" aria-hidden="true" tabindex="-1"></a>val_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb561-16"><a href="deep-learning-for-text-and-sequences.html#cb561-16" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb561-17"><a href="deep-learning-for-text-and-sequences.html#cb561-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb561-18"><a href="deep-learning-for-text-and-sequences.html#cb561-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb561-19"><a href="deep-learning-for-text-and-sequences.html#cb561-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">200001</span>, <span class="co">#We see that we extend from the train set</span></span>
<span id="cb561-20"><a href="deep-learning-for-text-and-sequences.html#cb561-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">300000</span>,</span>
<span id="cb561-21"><a href="deep-learning-for-text-and-sequences.html#cb561-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step,</span>
<span id="cb561-22"><a href="deep-learning-for-text-and-sequences.html#cb561-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size</span>
<span id="cb561-23"><a href="deep-learning-for-text-and-sequences.html#cb561-23" aria-hidden="true" tabindex="-1"></a>) <span class="co">#Notice that we don&#39;t shuffle</span></span>
<span id="cb561-24"><a href="deep-learning-for-text-and-sequences.html#cb561-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb561-25"><a href="deep-learning-for-text-and-sequences.html#cb561-25" aria-hidden="true" tabindex="-1"></a>test_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb561-26"><a href="deep-learning-for-text-and-sequences.html#cb561-26" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb561-27"><a href="deep-learning-for-text-and-sequences.html#cb561-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb561-28"><a href="deep-learning-for-text-and-sequences.html#cb561-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb561-29"><a href="deep-learning-for-text-and-sequences.html#cb561-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">300001</span>, <span class="co">#We extend from the validation data</span></span>
<span id="cb561-30"><a href="deep-learning-for-text-and-sequences.html#cb561-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="cn">NULL</span>,</span>
<span id="cb561-31"><a href="deep-learning-for-text-and-sequences.html#cb561-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step,</span>
<span id="cb561-32"><a href="deep-learning-for-text-and-sequences.html#cb561-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size</span>
<span id="cb561-33"><a href="deep-learning-for-text-and-sequences.html#cb561-33" aria-hidden="true" tabindex="-1"></a>) <span class="co">#Notice that we don&#39;t shuffle</span></span>
<span id="cb561-34"><a href="deep-learning-for-text-and-sequences.html#cb561-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb561-35"><a href="deep-learning-for-text-and-sequences.html#cb561-35" aria-hidden="true" tabindex="-1"></a><span class="co">#Identifying amount of timesteps we can validate on</span></span>
<span id="cb561-36"><a href="deep-learning-for-text-and-sequences.html#cb561-36" aria-hidden="true" tabindex="-1"></a>val_steps <span class="ot">&lt;-</span> (<span class="dv">300000</span> <span class="sc">-</span> <span class="dv">200001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span>
<span id="cb561-37"><a href="deep-learning-for-text-and-sequences.html#cb561-37" aria-hidden="true" tabindex="-1"></a>test_steps <span class="ot">&lt;-</span> (<span class="fu">nrow</span>(data) <span class="sc">-</span> <span class="dv">300001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span></code></pre></div>
<p>Notice that we subtract the lookback, as that is foundation for first predictions, and thus they</p>
</div>
<div id="a-common-sense-non-machine-learning-baseline" class="section level3" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> A common-sense, non-machine-learning baseline</h3>
<p>We want to get a simple baseline we can compare the model with. That is done in the following.</p>
<p>We want to predict that the temperature 24 hours from now is the same as now, hence the temperature is just assumed to be the same in each 10 minutes. Recall that we have observations for each 10 minutes and thus 128 observations make up for a 24 hour timespan.</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="deep-learning-for-text-and-sequences.html#cb562-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.35. Computing the common-sense baseline MAE</span></span>
<span id="cb562-2"><a href="deep-learning-for-text-and-sequences.html#cb562-2" aria-hidden="true" tabindex="-1"></a>evaluate_naive_method <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb562-3"><a href="deep-learning-for-text-and-sequences.html#cb562-3" aria-hidden="true" tabindex="-1"></a>  batch_maes <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb562-4"><a href="deep-learning-for-text-and-sequences.html#cb562-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (step <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>val_steps) {</span>
<span id="cb562-5"><a href="deep-learning-for-text-and-sequences.html#cb562-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(samples, targets) <span class="sc">%&lt;-%</span> <span class="fu">val_gen</span>() <span class="co">#Val_gen both contains samples and targets</span></span>
<span id="cb562-6"><a href="deep-learning-for-text-and-sequences.html#cb562-6" aria-hidden="true" tabindex="-1"></a>    preds <span class="ot">&lt;-</span> samples[,<span class="fu">dim</span>(samples)[[<span class="dv">2</span>]],<span class="dv">2</span>]</span>
<span id="cb562-7"><a href="deep-learning-for-text-and-sequences.html#cb562-7" aria-hidden="true" tabindex="-1"></a>    mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(preds <span class="sc">-</span> targets))</span>
<span id="cb562-8"><a href="deep-learning-for-text-and-sequences.html#cb562-8" aria-hidden="true" tabindex="-1"></a>    batch_maes <span class="ot">&lt;-</span> <span class="fu">c</span>(batch_maes, mae)</span>
<span id="cb562-9"><a href="deep-learning-for-text-and-sequences.html#cb562-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb562-10"><a href="deep-learning-for-text-and-sequences.html#cb562-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">mean</span>(batch_maes))</span>
<span id="cb562-11"><a href="deep-learning-for-text-and-sequences.html#cb562-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb562-12"><a href="deep-learning-for-text-and-sequences.html#cb562-12" aria-hidden="true" tabindex="-1"></a><span class="fu">evaluate_naive_method</span>()</span>
<span id="cb562-13"><a href="deep-learning-for-text-and-sequences.html#cb562-13" aria-hidden="true" tabindex="-1"></a>naive_mse <span class="ot">&lt;-</span> <span class="fu">evaluate_naive_method</span>()</span></code></pre></div>
<p>This yields an MSE of 0.28. Recall that we have normalized the data, hence this does not correspond to temperature, hence we can reform it to actual celcius degrees.</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="deep-learning-for-text-and-sequences.html#cb563-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.36. Converting the MAE back to a Celsius error</span></span>
<span id="cb563-2"><a href="deep-learning-for-text-and-sequences.html#cb563-2" aria-hidden="true" tabindex="-1"></a>celsius_mae <span class="ot">&lt;-</span> naive_mse <span class="sc">*</span> std[[<span class="dv">2</span>]] <span class="co">#Std. of the target variable</span></span>
<span id="cb563-3"><a href="deep-learning-for-text-and-sequences.html#cb563-3" aria-hidden="true" tabindex="-1"></a>celsius_mae</span></code></pre></div>
<p>We see that the mean absolute value = 2.46. That is now our reference.</p>
</div>
<div id="a-basic-machine-learning-approach" class="section level3" number="8.3.4">
<h3><span class="header-section-number">8.3.4</span> A basic machine-learning approach</h3>
<p>Now we are going to make a simple model. In general it is good practice to start with a simple model and then we can gradually make it more complicated.</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="deep-learning-for-text-and-sequences.html#cb564-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.37. Training and evaluating a densely connected model</span></span>
<span id="cb564-2"><a href="deep-learning-for-text-and-sequences.html#cb564-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb564-3"><a href="deep-learning-for-text-and-sequences.html#cb564-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>(<span class="at">input_shape =</span> <span class="fu">c</span>(lookback <span class="sc">/</span> step, <span class="fu">dim</span>(data)[<span class="sc">-</span><span class="dv">1</span>])) <span class="sc">%&gt;%</span> <span class="co">#=c(240,14)</span></span>
<span id="cb564-4"><a href="deep-learning-for-text-and-sequences.html#cb564-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb564-5"><a href="deep-learning-for-text-and-sequences.html#cb564-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb564-6"><a href="deep-learning-for-text-and-sequences.html#cb564-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb564-7"><a href="deep-learning-for-text-and-sequences.html#cb564-7" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb564-8"><a href="deep-learning-for-text-and-sequences.html#cb564-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb564-9"><a href="deep-learning-for-text-and-sequences.html#cb564-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb564-10"><a href="deep-learning-for-text-and-sequences.html#cb564-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb564-11"><a href="deep-learning-for-text-and-sequences.html#cb564-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb564-12"><a href="deep-learning-for-text-and-sequences.html#cb564-12" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb564-13"><a href="deep-learning-for-text-and-sequences.html#cb564-13" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb564-14"><a href="deep-learning-for-text-and-sequences.html#cb564-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb564-15"><a href="deep-learning-for-text-and-sequences.html#cb564-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb564-16"><a href="deep-learning-for-text-and-sequences.html#cb564-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb564-17"><a href="deep-learning-for-text-and-sequences.html#cb564-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb564-18"><a href="deep-learning-for-text-and-sequences.html#cb564-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="deep-learning-for-text-and-sequences.html#cb565-1" aria-hidden="true" tabindex="-1"></a><span class="fu">k_clear_session</span>()</span></code></pre></div>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="deep-learning-for-text-and-sequences.html#cb566-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>Recall that the baseline is 2,77 and our model is at best 0.3 on the validation data, meaning that we donât get anything out of having a complex model (even just a simple densely connected network).</p>
<p>The explanation from the book is that the intuition is that complex models search for complex patterns, and this will not find the simple solution, hence it cant see the forest for the mere trees. And thus this variate of the model cannot compete with just the simple algorithm that was performed earlier, because often simple solutions require simple models.</p>
<p>Hence it proves the rule of thumb that one must start simple and then hereafter move on to more complex models.</p>
</div>
<div id="a-first-recurrent-baseline" class="section level3" number="8.3.5">
<h3><span class="header-section-number">8.3.5</span> A first recurrent baseline</h3>
<p>Now we will try to extend the example above to see if we can make a model that is more competitive with the baseline, and hopefully outperform it.</p>
<p>We are now going to introduce the GRU. It stands for <strong><em>Gated recurrent unit</em></strong>. The overall principle is the same, while GRU is not as cumbersome to compute, although we may not get the same data representations as we do with LSTM.</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="deep-learning-for-text-and-sequences.html#cb567-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.39. Training and evaluating a model with layer_gru</span></span>
<span id="cb567-2"><a href="deep-learning-for-text-and-sequences.html#cb567-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb567-3"><a href="deep-learning-for-text-and-sequences.html#cb567-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]])) <span class="sc">%&gt;%</span></span>
<span id="cb567-4"><a href="deep-learning-for-text-and-sequences.html#cb567-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb567-5"><a href="deep-learning-for-text-and-sequences.html#cb567-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb567-6"><a href="deep-learning-for-text-and-sequences.html#cb567-6" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb567-7"><a href="deep-learning-for-text-and-sequences.html#cb567-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb567-8"><a href="deep-learning-for-text-and-sequences.html#cb567-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb567-9"><a href="deep-learning-for-text-and-sequences.html#cb567-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb567-10"><a href="deep-learning-for-text-and-sequences.html#cb567-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb567-11"><a href="deep-learning-for-text-and-sequences.html#cb567-11" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit_generator</span>(</span>
<span id="cb567-12"><a href="deep-learning-for-text-and-sequences.html#cb567-12" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb567-13"><a href="deep-learning-for-text-and-sequences.html#cb567-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb567-14"><a href="deep-learning-for-text-and-sequences.html#cb567-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb567-15"><a href="deep-learning-for-text-and-sequences.html#cb567-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb567-16"><a href="deep-learning-for-text-and-sequences.html#cb567-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb567-17"><a href="deep-learning-for-text-and-sequences.html#cb567-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="deep-learning-for-text-and-sequences.html#cb568-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>We see that already after a couple of epochs, we get a model that is more competitive with the baseline and even slightly outperform. Although it does look like we have an issue with overfitting to the train data.</p>
<p>So what is natural to do?</p>
<p><strong>We add dropout!</strong></p>
</div>
<div id="using-recurrent-dropout-to-fight-overfitting" class="section level3" number="8.3.6">
<h3><span class="header-section-number">8.3.6</span> using recurrent dropout to fight overfitting</h3>
<p>In this instance with RNN, we have two types of dropouts to add.</p>
<ol style="list-style-type: decimal">
<li>dropout, as we normally now, where activations are randomly set to 0</li>
<li>recurrent_dropout, same principle, just with the recurrent units. Hence we intentionally make it forget previous seen data.</li>
</ol>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="deep-learning-for-text-and-sequences.html#cb569-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.40. Training and evaluating a dropout-regularized GRU-based model</span></span>
<span id="cb569-2"><a href="deep-learning-for-text-and-sequences.html#cb569-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb569-3"><a href="deep-learning-for-text-and-sequences.html#cb569-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">32</span></span>
<span id="cb569-4"><a href="deep-learning-for-text-and-sequences.html#cb569-4" aria-hidden="true" tabindex="-1"></a>            ,<span class="at">dropout =</span> <span class="fl">0.2</span> <span class="co">#We see that 0.2 is used</span></span>
<span id="cb569-5"><a href="deep-learning-for-text-and-sequences.html#cb569-5" aria-hidden="true" tabindex="-1"></a>            ,<span class="at">recurrent_dropout =</span> <span class="fl">0.2</span>, <span class="co">#This does not have to be the saem</span></span>
<span id="cb569-6"><a href="deep-learning-for-text-and-sequences.html#cb569-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]])) <span class="sc">%&gt;%</span></span>
<span id="cb569-7"><a href="deep-learning-for-text-and-sequences.html#cb569-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb569-8"><a href="deep-learning-for-text-and-sequences.html#cb569-8" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb569-9"><a href="deep-learning-for-text-and-sequences.html#cb569-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb569-10"><a href="deep-learning-for-text-and-sequences.html#cb569-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb569-11"><a href="deep-learning-for-text-and-sequences.html#cb569-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb569-12"><a href="deep-learning-for-text-and-sequences.html#cb569-12" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit_generator</span>(</span>
<span id="cb569-13"><a href="deep-learning-for-text-and-sequences.html#cb569-13" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb569-14"><a href="deep-learning-for-text-and-sequences.html#cb569-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb569-15"><a href="deep-learning-for-text-and-sequences.html#cb569-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">40</span>, <span class="co">#more epochs as we learn more slowly</span></span>
<span id="cb569-16"><a href="deep-learning-for-text-and-sequences.html#cb569-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb569-17"><a href="deep-learning-for-text-and-sequences.html#cb569-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb569-18"><a href="deep-learning-for-text-and-sequences.html#cb569-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><em>We see that the dropout and recurrent dropout appears to be the same. It does not have to be the same. Also, there is no general rule, one must overfit and then try to get rid of it.</em></p>
<p>In general, we see that the dropout is aften put after the layer and we see that recurrent dropout is already inside the RNN cell.</p>
<p>What do we expect to see? we expect to see that the model is converging much more slowly, as we are making it more difficult for it to learn. Therefore we also need more epochs.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="deep-learning-for-text-and-sequences.html#cb570-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>We see that the model has improved and now we are not overfitting any more!</p>
</div>
<div id="stacking-recurrent-layers" class="section level3" number="8.3.7">
<h3><span class="header-section-number">8.3.7</span> Stacking recurrent layers</h3>
<p>So since we are no more overfitting, we are also at a place where the model has learned what it can, given the prerequisities we gave it.</p>
<p>To enable it to learn more, we introduce more recurrent layers.</p>
<hr />
<p>Fun fact, google is driving google translate on a model that has 7 recurrent layers stacked on-top of each other. That is pretty huge.</p>
<hr />
<p>Notice, as when we stacked the simple_rnnâs, we wanted it to output the 3D tensor, the same analogy applies here.</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="deep-learning-for-text-and-sequences.html#cb571-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.41. Training and evaluating a dropout-regularized, stacked GRU model </span></span>
<span id="cb571-2"><a href="deep-learning-for-text-and-sequences.html#cb571-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb571-3"><a href="deep-learning-for-text-and-sequences.html#cb571-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">32</span>,</span>
<span id="cb571-4"><a href="deep-learning-for-text-and-sequences.html#cb571-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">dropout =</span> <span class="fl">0.1</span>,</span>
<span id="cb571-5"><a href="deep-learning-for-text-and-sequences.html#cb571-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">recurrent_dropout =</span> <span class="fl">0.5</span>,</span>
<span id="cb571-6"><a href="deep-learning-for-text-and-sequences.html#cb571-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">return_sequences =</span> <span class="cn">TRUE</span>, <span class="co">#The 3D tensor the next layer is using</span></span>
<span id="cb571-7"><a href="deep-learning-for-text-and-sequences.html#cb571-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]])) <span class="sc">%&gt;%</span></span>
<span id="cb571-8"><a href="deep-learning-for-text-and-sequences.html#cb571-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>,</span>
<span id="cb571-9"><a href="deep-learning-for-text-and-sequences.html#cb571-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">dropout =</span> <span class="fl">0.1</span>,</span>
<span id="cb571-10"><a href="deep-learning-for-text-and-sequences.html#cb571-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">recurrent_dropout =</span> <span class="fl">0.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb571-11"><a href="deep-learning-for-text-and-sequences.html#cb571-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb571-12"><a href="deep-learning-for-text-and-sequences.html#cb571-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb571-13"><a href="deep-learning-for-text-and-sequences.html#cb571-13" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb571-14"><a href="deep-learning-for-text-and-sequences.html#cb571-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb571-15"><a href="deep-learning-for-text-and-sequences.html#cb571-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb571-16"><a href="deep-learning-for-text-and-sequences.html#cb571-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb571-17"><a href="deep-learning-for-text-and-sequences.html#cb571-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb571-18"><a href="deep-learning-for-text-and-sequences.html#cb571-18" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit_generator</span>(</span>
<span id="cb571-19"><a href="deep-learning-for-text-and-sequences.html#cb571-19" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb571-20"><a href="deep-learning-for-text-and-sequences.html#cb571-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb571-21"><a href="deep-learning-for-text-and-sequences.html#cb571-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">40</span>,</span>
<span id="cb571-22"><a href="deep-learning-for-text-and-sequences.html#cb571-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb571-23"><a href="deep-learning-for-text-and-sequences.html#cb571-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb571-24"><a href="deep-learning-for-text-and-sequences.html#cb571-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We see that we increase the amount of layers from the first to the second GRU layer. The analogy is the same as in conv, as we start extracting bigger patterns and then in further layers we will start looking more in the hidden patterns, hence it could be a good idea to increase the amount of units.</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="deep-learning-for-text-and-sequences.html#cb572-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>We see that the model does in fact not get better than what we have previously seen. Although they suggest in the book, that one may add more layers, as overfitting is not yet tooooo big of a problem, but it is still there though.</p>
</div>
<div id="using-bidirectional-rnns" class="section level3" number="8.3.8">
<h3><span class="header-section-number">8.3.8</span> Using bidirectional RNNs</h3>
<p>In general, we see that this makes sence, when you have the whole sentences and e.g., whole speaches, as we need the information in the end that is related to the beginning, if you for instance just take half of a movie review, we may see that bidirectional does not work too well, as it may get difficult for the algorithm to connect the begining and (the artificially created) end of the review, while it is actually not the true end of the review.</p>
<p><em>Another example, we see that in german for instance, that important words comes in the end or in a bisentence, hence it is importance that you have the whole scenario for analysis.</em></p>
<p>In the following (we will see that it does not improve the temperature prediction) example we want to see if we can improve the model by assessing newest information first, hence going in antichronoligical order. To do this, we merely have to iterate the datagenerator, that we have previously made.</p>
<p>After this we will run the same GRU-model to see how it performs.</p>
<p><strong>Creating the antichronoligical order data generator</strong></p>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="deep-learning-for-text-and-sequences.html#cb573-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Based on listings 6.33</span></span>
<span id="cb573-2"><a href="deep-learning-for-text-and-sequences.html#cb573-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">128</span> <span class="co">#Originally 128</span></span>
<span id="cb573-3"><a href="deep-learning-for-text-and-sequences.html#cb573-3" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">&lt;-</span> </span>
<span id="cb573-4"><a href="deep-learning-for-text-and-sequences.html#cb573-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb573-5"><a href="deep-learning-for-text-and-sequences.html#cb573-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Starting the function and declaring the input</span></span>
<span id="cb573-6"><a href="deep-learning-for-text-and-sequences.html#cb573-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(data,lookback, delay, min_index, max_index,</span>
<span id="cb573-7"><a href="deep-learning-for-text-and-sequences.html#cb573-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">shuffle =</span> <span class="cn">FALSE</span>,<span class="co">#Notice that we set default to be FALSE</span></span>
<span id="cb573-8"><a href="deep-learning-for-text-and-sequences.html#cb573-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> batch_size, <span class="at">step =</span> step) {</span>
<span id="cb573-9"><a href="deep-learning-for-text-and-sequences.html#cb573-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb573-10"><a href="deep-learning-for-text-and-sequences.html#cb573-10" aria-hidden="true" tabindex="-1"></a>  <span class="do">##For test##</span></span>
<span id="cb573-11"><a href="deep-learning-for-text-and-sequences.html#cb573-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">is.null</span>(max_index)) <span class="co">#If max_index = NULL</span></span>
<span id="cb573-12"><a href="deep-learning-for-text-and-sequences.html#cb573-12" aria-hidden="true" tabindex="-1"></a>    max_index <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data) <span class="sc">-</span> delay <span class="sc">-</span> <span class="dv">1</span> <span class="co">#Notice in the test partition we set max index to NULL</span></span>
<span id="cb573-13"><a href="deep-learning-for-text-and-sequences.html#cb573-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb573-14"><a href="deep-learning-for-text-and-sequences.html#cb573-14" aria-hidden="true" tabindex="-1"></a>  i <span class="ot">&lt;-</span> min_index <span class="sc">+</span> lookback</span>
<span id="cb573-15"><a href="deep-learning-for-text-and-sequences.html#cb573-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb573-16"><a href="deep-learning-for-text-and-sequences.html#cb573-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># If we are not generating the test data, then the following will be done</span></span>
<span id="cb573-17"><a href="deep-learning-for-text-and-sequences.html#cb573-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb573-18"><a href="deep-learning-for-text-and-sequences.html#cb573-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>() {</span>
<span id="cb573-19"><a href="deep-learning-for-text-and-sequences.html#cb573-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb573-20"><a href="deep-learning-for-text-and-sequences.html#cb573-20" aria-hidden="true" tabindex="-1"></a>    <span class="do">##For training##</span></span>
<span id="cb573-21"><a href="deep-learning-for-text-and-sequences.html#cb573-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (shuffle) { <span class="co">#If shuffle = TRUE</span></span>
<span id="cb573-22"><a href="deep-learning-for-text-and-sequences.html#cb573-22" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb573-23"><a href="deep-learning-for-text-and-sequences.html#cb573-23" aria-hidden="true" tabindex="-1"></a>      rows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>((min_index<span class="sc">+</span>lookback)<span class="sc">:</span>max_index) <span class="co">#Random selection</span></span>
<span id="cb573-24"><a href="deep-learning-for-text-and-sequences.html#cb573-24" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">size =</span> batch_size) <span class="co">#We want as many selections as our batch size</span></span>
<span id="cb573-25"><a href="deep-learning-for-text-and-sequences.html#cb573-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb573-26"><a href="deep-learning-for-text-and-sequences.html#cb573-26" aria-hidden="true" tabindex="-1"></a>    <span class="do">##For validation and test##</span></span>
<span id="cb573-27"><a href="deep-learning-for-text-and-sequences.html#cb573-27" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> { <span class="co">#If shuffle is not defined, it is by default FALSE</span></span>
<span id="cb573-28"><a href="deep-learning-for-text-and-sequences.html#cb573-28" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (i <span class="sc">+</span> batch_size <span class="sc">&gt;=</span> max_index) <span class="co">#Evaluates if we are working within the scope</span></span>
<span id="cb573-29"><a href="deep-learning-for-text-and-sequences.html#cb573-29" aria-hidden="true" tabindex="-1"></a>        i <span class="ot">&lt;&lt;-</span> min_index <span class="sc">+</span> lookback</span>
<span id="cb573-30"><a href="deep-learning-for-text-and-sequences.html#cb573-30" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb573-31"><a href="deep-learning-for-text-and-sequences.html#cb573-31" aria-hidden="true" tabindex="-1"></a>      <span class="co">#We don&#39;t want to go over the limit we set on the data partition</span></span>
<span id="cb573-32"><a href="deep-learning-for-text-and-sequences.html#cb573-32" aria-hidden="true" tabindex="-1"></a>      rows <span class="ot">&lt;-</span> <span class="fu">c</span>(i<span class="sc">:</span><span class="fu">min</span>((i<span class="sc">+</span>batch_size)<span class="sc">-</span><span class="dv">1</span>, max_index))</span>
<span id="cb573-33"><a href="deep-learning-for-text-and-sequences.html#cb573-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb573-34"><a href="deep-learning-for-text-and-sequences.html#cb573-34" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Prepare i for the next batch</span></span>
<span id="cb573-35"><a href="deep-learning-for-text-and-sequences.html#cb573-35" aria-hidden="true" tabindex="-1"></a>      i <span class="ot">&lt;&lt;-</span> i <span class="sc">+</span> <span class="fu">length</span>(rows)</span>
<span id="cb573-36"><a href="deep-learning-for-text-and-sequences.html#cb573-36" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb573-37"><a href="deep-learning-for-text-and-sequences.html#cb573-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb573-38"><a href="deep-learning-for-text-and-sequences.html#cb573-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Create empty arrays for data ingestion</span></span>
<span id="cb573-39"><a href="deep-learning-for-text-and-sequences.html#cb573-39" aria-hidden="true" tabindex="-1"></a>    samples <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(rows),</span>
<span id="cb573-40"><a href="deep-learning-for-text-and-sequences.html#cb573-40" aria-hidden="true" tabindex="-1"></a>                                lookback <span class="sc">/</span> step,</span>
<span id="cb573-41"><a href="deep-learning-for-text-and-sequences.html#cb573-41" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]]))</span>
<span id="cb573-42"><a href="deep-learning-for-text-and-sequences.html#cb573-42" aria-hidden="true" tabindex="-1"></a>    targets <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(rows)))</span>
<span id="cb573-43"><a href="deep-learning-for-text-and-sequences.html#cb573-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb573-44"><a href="deep-learning-for-text-and-sequences.html#cb573-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Ingest data to the arrays</span></span>
<span id="cb573-45"><a href="deep-learning-for-text-and-sequences.html#cb573-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rows)) { <span class="co">#Looping up to the batch size</span></span>
<span id="cb573-46"><a href="deep-learning-for-text-and-sequences.html#cb573-46" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb573-47"><a href="deep-learning-for-text-and-sequences.html#cb573-47" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Subsetting the data we need</span></span>
<span id="cb573-48"><a href="deep-learning-for-text-and-sequences.html#cb573-48" aria-hidden="true" tabindex="-1"></a>      indices <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> rows[[j]] <span class="sc">-</span> lookback</span>
<span id="cb573-49"><a href="deep-learning-for-text-and-sequences.html#cb573-49" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">to =</span> rows[[j]]</span>
<span id="cb573-50"><a href="deep-learning-for-text-and-sequences.html#cb573-50" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">length.out =</span> <span class="fu">dim</span>(samples)[[<span class="dv">2</span>]]) <span class="co">#The columns</span></span>
<span id="cb573-51"><a href="deep-learning-for-text-and-sequences.html#cb573-51" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb573-52"><a href="deep-learning-for-text-and-sequences.html#cb573-52" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Inserting the data into the samples</span></span>
<span id="cb573-53"><a href="deep-learning-for-text-and-sequences.html#cb573-53" aria-hidden="true" tabindex="-1"></a>      samples[j,,] <span class="ot">&lt;-</span> data[indices,] <span class="co">#Subsetting on rows</span></span>
<span id="cb573-54"><a href="deep-learning-for-text-and-sequences.html#cb573-54" aria-hidden="true" tabindex="-1"></a>      targets[[j]] <span class="ot">&lt;-</span> data[rows[[j]] <span class="sc">+</span> delay,<span class="dv">2</span>] <span class="co">#Notice that DV is column 2</span></span>
<span id="cb573-55"><a href="deep-learning-for-text-and-sequences.html#cb573-55" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb573-56"><a href="deep-learning-for-text-and-sequences.html#cb573-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb573-57"><a href="deep-learning-for-text-and-sequences.html#cb573-57" aria-hidden="true" tabindex="-1"></a>    <span class="co">#We compute a list of the object</span></span>
<span id="cb573-58"><a href="deep-learning-for-text-and-sequences.html#cb573-58" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(samples [,<span class="fu">ncol</span>(samples)<span class="sc">:</span><span class="dv">1</span>,], targets) <span class="do">#####NOTICE: THIS IS WHAT IS CHANGED####</span></span>
<span id="cb573-59"><a href="deep-learning-for-text-and-sequences.html#cb573-59" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb573-60"><a href="deep-learning-for-text-and-sequences.html#cb573-60" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>Generating the data</strong></p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="deep-learning-for-text-and-sequences.html#cb574-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb574-2"><a href="deep-learning-for-text-and-sequences.html#cb574-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb574-3"><a href="deep-learning-for-text-and-sequences.html#cb574-3" aria-hidden="true" tabindex="-1"></a>train_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb574-4"><a href="deep-learning-for-text-and-sequences.html#cb574-4" aria-hidden="true" tabindex="-1"></a>  data, <span class="co">#The normalized data</span></span>
<span id="cb574-5"><a href="deep-learning-for-text-and-sequences.html#cb574-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback, <span class="co">#How far we look back</span></span>
<span id="cb574-6"><a href="deep-learning-for-text-and-sequences.html#cb574-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay, <span class="co">#How far in the future we want to look</span></span>
<span id="cb574-7"><a href="deep-learning-for-text-and-sequences.html#cb574-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">1</span>, <span class="co">#First train observation</span></span>
<span id="cb574-8"><a href="deep-learning-for-text-and-sequences.html#cb574-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">200000</span>, <span class="co">#Last train observation</span></span>
<span id="cb574-9"><a href="deep-learning-for-text-and-sequences.html#cb574-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="co">#Do we want to shuffle the data or take it chronoligical?</span></span>
<span id="cb574-10"><a href="deep-learning-for-text-and-sequences.html#cb574-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step, <span class="co">#The period (in timesteps)</span></span>
<span id="cb574-11"><a href="deep-learning-for-text-and-sequences.html#cb574-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size <span class="co">#The amount of samples pr. bach</span></span>
<span id="cb574-12"><a href="deep-learning-for-text-and-sequences.html#cb574-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb574-13"><a href="deep-learning-for-text-and-sequences.html#cb574-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb574-14"><a href="deep-learning-for-text-and-sequences.html#cb574-14" aria-hidden="true" tabindex="-1"></a>val_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb574-15"><a href="deep-learning-for-text-and-sequences.html#cb574-15" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb574-16"><a href="deep-learning-for-text-and-sequences.html#cb574-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb574-17"><a href="deep-learning-for-text-and-sequences.html#cb574-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb574-18"><a href="deep-learning-for-text-and-sequences.html#cb574-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">200001</span>, <span class="co">#We see that we extend from the train set</span></span>
<span id="cb574-19"><a href="deep-learning-for-text-and-sequences.html#cb574-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">300000</span>,</span>
<span id="cb574-20"><a href="deep-learning-for-text-and-sequences.html#cb574-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step,</span>
<span id="cb574-21"><a href="deep-learning-for-text-and-sequences.html#cb574-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size</span>
<span id="cb574-22"><a href="deep-learning-for-text-and-sequences.html#cb574-22" aria-hidden="true" tabindex="-1"></a>) <span class="co">#Notice that we don&#39;t shuffle</span></span>
<span id="cb574-23"><a href="deep-learning-for-text-and-sequences.html#cb574-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb574-24"><a href="deep-learning-for-text-and-sequences.html#cb574-24" aria-hidden="true" tabindex="-1"></a>test_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb574-25"><a href="deep-learning-for-text-and-sequences.html#cb574-25" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb574-26"><a href="deep-learning-for-text-and-sequences.html#cb574-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb574-27"><a href="deep-learning-for-text-and-sequences.html#cb574-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb574-28"><a href="deep-learning-for-text-and-sequences.html#cb574-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">300001</span>, <span class="co">#We extend from the validation data</span></span>
<span id="cb574-29"><a href="deep-learning-for-text-and-sequences.html#cb574-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="cn">NULL</span>,</span>
<span id="cb574-30"><a href="deep-learning-for-text-and-sequences.html#cb574-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step,</span>
<span id="cb574-31"><a href="deep-learning-for-text-and-sequences.html#cb574-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size</span>
<span id="cb574-32"><a href="deep-learning-for-text-and-sequences.html#cb574-32" aria-hidden="true" tabindex="-1"></a>) <span class="co">#Notice that we don&#39;t shuffle</span></span>
<span id="cb574-33"><a href="deep-learning-for-text-and-sequences.html#cb574-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb574-34"><a href="deep-learning-for-text-and-sequences.html#cb574-34" aria-hidden="true" tabindex="-1"></a><span class="co">#Identifying amount of timesteps we can validate on</span></span>
<span id="cb574-35"><a href="deep-learning-for-text-and-sequences.html#cb574-35" aria-hidden="true" tabindex="-1"></a>val_steps <span class="ot">&lt;-</span> (<span class="dv">300000</span> <span class="sc">-</span> <span class="dv">200001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span>
<span id="cb574-36"><a href="deep-learning-for-text-and-sequences.html#cb574-36" aria-hidden="true" tabindex="-1"></a>test_steps <span class="ot">&lt;-</span> (<span class="fu">nrow</span>(data) <span class="sc">-</span> <span class="dv">300001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span></code></pre></div>
<p>Notice that the following is not run, because the Macbook is not able to.</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="deep-learning-for-text-and-sequences.html#cb575-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb575-2"><a href="deep-learning-for-text-and-sequences.html#cb575-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">32</span>,</span>
<span id="cb575-3"><a href="deep-learning-for-text-and-sequences.html#cb575-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">dropout =</span> <span class="fl">0.1</span>,</span>
<span id="cb575-4"><a href="deep-learning-for-text-and-sequences.html#cb575-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">recurrent_dropout =</span> <span class="fl">0.5</span>,</span>
<span id="cb575-5"><a href="deep-learning-for-text-and-sequences.html#cb575-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">return_sequences =</span> <span class="cn">TRUE</span>, <span class="co">#The 3D tensor the next layer is using</span></span>
<span id="cb575-6"><a href="deep-learning-for-text-and-sequences.html#cb575-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]])) <span class="sc">%&gt;%</span></span>
<span id="cb575-7"><a href="deep-learning-for-text-and-sequences.html#cb575-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>,</span>
<span id="cb575-8"><a href="deep-learning-for-text-and-sequences.html#cb575-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">dropout =</span> <span class="fl">0.1</span>,</span>
<span id="cb575-9"><a href="deep-learning-for-text-and-sequences.html#cb575-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">recurrent_dropout =</span> <span class="fl">0.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb575-10"><a href="deep-learning-for-text-and-sequences.html#cb575-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb575-11"><a href="deep-learning-for-text-and-sequences.html#cb575-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb575-12"><a href="deep-learning-for-text-and-sequences.html#cb575-12" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb575-13"><a href="deep-learning-for-text-and-sequences.html#cb575-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb575-14"><a href="deep-learning-for-text-and-sequences.html#cb575-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb575-15"><a href="deep-learning-for-text-and-sequences.html#cb575-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb575-16"><a href="deep-learning-for-text-and-sequences.html#cb575-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb575-17"><a href="deep-learning-for-text-and-sequences.html#cb575-17" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit_generator</span>(</span>
<span id="cb575-18"><a href="deep-learning-for-text-and-sequences.html#cb575-18" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb575-19"><a href="deep-learning-for-text-and-sequences.html#cb575-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb575-20"><a href="deep-learning-for-text-and-sequences.html#cb575-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">40</span>,</span>
<span id="cb575-21"><a href="deep-learning-for-text-and-sequences.html#cb575-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb575-22"><a href="deep-learning-for-text-and-sequences.html#cb575-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb575-23"><a href="deep-learning-for-text-and-sequences.html#cb575-23" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We are supposed to see that it does not improve and does much worse than the simple baseline.</p>
<p>One must remember that GRU will rule out information from previous steps quicker than for instance LSTM. Meaning that when we run GRU with data in antichronological order, then it will first see the newest information and work its way towards the beginning. Thus the newest information will start vanishing. Hence it intuitively makes sense that the model is underperforming, as we will base predictions primarily on older weather data.</p>
<p><strong>An example that works well</strong></p>
<p>We can look at an example where we will see that the antichronoligical order of the data will perform just as well.</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="deep-learning-for-text-and-sequences.html#cb576-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb576-2"><a href="deep-learning-for-text-and-sequences.html#cb576-2" aria-hidden="true" tabindex="-1"></a>max_features <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#We want 10.000 words</span></span>
<span id="cb576-3"><a href="deep-learning-for-text-and-sequences.html#cb576-3" aria-hidden="true" tabindex="-1"></a>maxlen <span class="ot">&lt;-</span> <span class="dv">500</span> <span class="co">#Cutting off text at 500 words.</span></span>
<span id="cb576-4"><a href="deep-learning-for-text-and-sequences.html#cb576-4" aria-hidden="true" tabindex="-1"></a>imdb <span class="ot">&lt;-</span> <span class="fu">dataset_imdb</span>(<span class="at">num_words =</span> max_features)</span>
<span id="cb576-5"><a href="deep-learning-for-text-and-sequences.html#cb576-5" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(x_train, y_train), <span class="fu">c</span>(x_test, y_test)) <span class="sc">%&lt;-%</span> imdb</span>
<span id="cb576-6"><a href="deep-learning-for-text-and-sequences.html#cb576-6" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">lapply</span>(x_train,<span class="at">FUN =</span>  rev) <span class="co">#Rev puts it in reverse order</span></span>
<span id="cb576-7"><a href="deep-learning-for-text-and-sequences.html#cb576-7" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">lapply</span>(x_test,<span class="at">FUN =</span>  rev) <span class="co">#Rev puts it in reverse order</span></span>
<span id="cb576-8"><a href="deep-learning-for-text-and-sequences.html#cb576-8" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_train, <span class="at">maxlen =</span> maxlen) <span class="co">#We apply padding again to have equal lengths</span></span>
<span id="cb576-9"><a href="deep-learning-for-text-and-sequences.html#cb576-9" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_test, <span class="at">maxlen =</span> maxlen) <span class="co">#We apply padding again to have equal lengths</span></span>
<span id="cb576-10"><a href="deep-learning-for-text-and-sequences.html#cb576-10" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb576-11"><a href="deep-learning-for-text-and-sequences.html#cb576-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_features, <span class="at">output_dim =</span> <span class="dv">128</span>) <span class="sc">%&gt;%</span></span>
<span id="cb576-12"><a href="deep-learning-for-text-and-sequences.html#cb576-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb576-13"><a href="deep-learning-for-text-and-sequences.html#cb576-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb576-14"><a href="deep-learning-for-text-and-sequences.html#cb576-14" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb576-15"><a href="deep-learning-for-text-and-sequences.html#cb576-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb576-16"><a href="deep-learning-for-text-and-sequences.html#cb576-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb576-17"><a href="deep-learning-for-text-and-sequences.html#cb576-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>)</span>
<span id="cb576-18"><a href="deep-learning-for-text-and-sequences.html#cb576-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb576-19"><a href="deep-learning-for-text-and-sequences.html#cb576-19" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb576-20"><a href="deep-learning-for-text-and-sequences.html#cb576-20" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb576-21"><a href="deep-learning-for-text-and-sequences.html#cb576-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb576-22"><a href="deep-learning-for-text-and-sequences.html#cb576-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb576-23"><a href="deep-learning-for-text-and-sequences.html#cb576-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb576-24"><a href="deep-learning-for-text-and-sequences.html#cb576-24" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>For the model it does not matter with text what order we present it. In fact this improves the model as we are able to present the same data, but merely with a new angle.</p>
<p>Then what we want to do with this, is to create an ensemble, so we both use chronological and antichornological order.</p>
</div>
<div id="going-even-further" class="section level3" number="8.3.9">
<h3><span class="header-section-number">8.3.9</span> Going even further</h3>
<p>What could we have done to improve the model even further? These are some suggestions from the book:</p>
<ul>
<li>Play around with with the amount of units in the stacked layers. These are just a bit random.</li>
<li>Adjusting the learning rate with RMSprop optimizer</li>
<li>Adding more densely connected layers or even making them bigger</li>
</ul>
</div>
<div id="wrap-up" class="section level3" number="8.3.10">
<h3><span class="header-section-number">8.3.10</span> Wrap up</h3>
<p>Notice that the different approaches presented in the previous sections are different approaches and they are not equally good for all situations, f.eks. bidirectional RNNs are not good if more recent data is important in contrast to older data. But when older information, f.eks. the first words of a sentence or a paragraph then the LSTM or GRU is typically better than the bidirectional approach.</p>
<p>Then it comes to GRU vs.Â LSTM, often it appears that LSTM is the best performing model, but it is also the most complicated to run.</p>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="deep-learning-for-text-and-sequences.html#cb577-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
</div>
</div>
<div id="sequence-processing-with-convnets" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Sequence processing with convnets</h2>
<p>Recall that with convoluted networks we took only snippets of the input and attempted to extract patterns from these snippets. Thus you are now making these snippets independent of the overall.</p>
<p>The same analogy applies here, for instance if you want to look at small snippets of text and extract the patterns from this. In fact they claim that these methods can be competitive with RNNs.</p>
<p>Also we see that the same max and averaging pooling applies here to downsample the data. Instead of the pooling to be 2D we are now working with 1D pooling.</p>
<div id="implementing-a-1d-convnet" class="section level3" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Implementing a 1D convnet</h3>
<p>We want to use <code>layer_conv_1d</code>, it takes 3D input, hence <em>(samples, time, features)</em>. We see that this is able to learn patterns in local regions, hence if it learn one pattern in on position, then this can be applied in another part of sentences.</p>
<p>We also see that convoluted evaluations with a max pooling operation is good at extracting the key words of a sentence and helps do the data preparation. The output is in this example feature vectors and not feature maps, as we are in a lower dimension.</p>
<p>In the following we buiÃ¦d a simple model.</p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="deep-learning-for-text-and-sequences.html#cb578-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.45. Preparing the IMDB data </span></span>
<span id="cb578-2"><a href="deep-learning-for-text-and-sequences.html#cb578-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb578-3"><a href="deep-learning-for-text-and-sequences.html#cb578-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb578-4"><a href="deep-learning-for-text-and-sequences.html#cb578-4" aria-hidden="true" tabindex="-1"></a>max_features <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#Max amount of words we want</span></span>
<span id="cb578-5"><a href="deep-learning-for-text-and-sequences.html#cb578-5" aria-hidden="true" tabindex="-1"></a>max_len <span class="ot">&lt;-</span> <span class="dv">500</span> <span class="co">#We take up to 500 words</span></span>
<span id="cb578-6"><a href="deep-learning-for-text-and-sequences.html#cb578-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Loading data...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb578-7"><a href="deep-learning-for-text-and-sequences.html#cb578-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb578-8"><a href="deep-learning-for-text-and-sequences.html#cb578-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Load data</span></span>
<span id="cb578-9"><a href="deep-learning-for-text-and-sequences.html#cb578-9" aria-hidden="true" tabindex="-1"></a>imdb <span class="ot">&lt;-</span> <span class="fu">dataset_imdb</span>(<span class="at">num_words =</span> max_features) <span class="co">#Downloads the tha</span></span>
<span id="cb578-10"><a href="deep-learning-for-text-and-sequences.html#cb578-10" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(x_train, y_train), <span class="fu">c</span>(x_test, y_test)) <span class="sc">%&lt;-%</span> imdb <span class="co">#Unpacks the data</span></span>
<span id="cb578-11"><a href="deep-learning-for-text-and-sequences.html#cb578-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">length</span>(x_train), <span class="st">&quot;train sequences</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb578-12"><a href="deep-learning-for-text-and-sequences.html#cb578-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">length</span>(x_test), <span class="st">&quot;test sequences&quot;</span>)</span>
<span id="cb578-13"><a href="deep-learning-for-text-and-sequences.html#cb578-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Pad sequences (samples x time)</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb578-14"><a href="deep-learning-for-text-and-sequences.html#cb578-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb578-15"><a href="deep-learning-for-text-and-sequences.html#cb578-15" aria-hidden="true" tabindex="-1"></a><span class="co">#Padding the sequences</span></span>
<span id="cb578-16"><a href="deep-learning-for-text-and-sequences.html#cb578-16" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_train, <span class="at">maxlen =</span> max_len)</span>
<span id="cb578-17"><a href="deep-learning-for-text-and-sequences.html#cb578-17" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_test, <span class="at">maxlen =</span> max_len)</span>
<span id="cb578-18"><a href="deep-learning-for-text-and-sequences.html#cb578-18" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;x_train shape:&quot;</span>, <span class="fu">dim</span>(x_train), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb578-19"><a href="deep-learning-for-text-and-sequences.html#cb578-19" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;x_test shape:&quot;</span>, <span class="fu">dim</span>(x_test), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>Now where the data is loaded we can construct the model.</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="deep-learning-for-text-and-sequences.html#cb579-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.46. Training and evaluating a simple 1D convnet on the IMDB data </span></span>
<span id="cb579-2"><a href="deep-learning-for-text-and-sequences.html#cb579-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb579-3"><a href="deep-learning-for-text-and-sequences.html#cb579-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb579-4"><a href="deep-learning-for-text-and-sequences.html#cb579-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb579-5"><a href="deep-learning-for-text-and-sequences.html#cb579-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Load word embeddings to be interpreted by the convolutions</span></span>
<span id="cb579-6"><a href="deep-learning-for-text-and-sequences.html#cb579-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_features, <span class="at">output_dim =</span> <span class="dv">128</span>,</span>
<span id="cb579-7"><a href="deep-learning-for-text-and-sequences.html#cb579-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">input_length =</span> max_len) <span class="sc">%&gt;%</span> </span>
<span id="cb579-8"><a href="deep-learning-for-text-and-sequences.html#cb579-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">7</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb579-9"><a href="deep-learning-for-text-and-sequences.html#cb579-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_1d</span>(<span class="at">pool_size =</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span> <span class="co">#Extract key words</span></span>
<span id="cb579-10"><a href="deep-learning-for-text-and-sequences.html#cb579-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">7</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb579-11"><a href="deep-learning-for-text-and-sequences.html#cb579-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_global_max_pooling_1d</span>() <span class="sc">%&gt;%</span> <span class="co">#One could also use layer_flatten()</span></span>
<span id="cb579-12"><a href="deep-learning-for-text-and-sequences.html#cb579-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb579-13"><a href="deep-learning-for-text-and-sequences.html#cb579-13" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb579-14"><a href="deep-learning-for-text-and-sequences.html#cb579-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb579-15"><a href="deep-learning-for-text-and-sequences.html#cb579-15" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb579-16"><a href="deep-learning-for-text-and-sequences.html#cb579-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(<span class="at">lr =</span> <span class="fl">1e-4</span>),</span>
<span id="cb579-17"><a href="deep-learning-for-text-and-sequences.html#cb579-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb579-18"><a href="deep-learning-for-text-and-sequences.html#cb579-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;acc&quot;</span>)</span>
<span id="cb579-19"><a href="deep-learning-for-text-and-sequences.html#cb579-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb579-20"><a href="deep-learning-for-text-and-sequences.html#cb579-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb579-21"><a href="deep-learning-for-text-and-sequences.html#cb579-21" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb579-22"><a href="deep-learning-for-text-and-sequences.html#cb579-22" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb579-23"><a href="deep-learning-for-text-and-sequences.html#cb579-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb579-24"><a href="deep-learning-for-text-and-sequences.html#cb579-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb579-25"><a href="deep-learning-for-text-and-sequences.html#cb579-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb579-26"><a href="deep-learning-for-text-and-sequences.html#cb579-26" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb580-1"><a href="deep-learning-for-text-and-sequences.html#cb580-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history,<span class="at">method =</span> <span class="st">&quot;base&quot;</span>)</span></code></pre></div>
<p>We see that we are not overftting and accuracy for validation and train data pretty much follow each other. We see that we are hitting</p>
<p>Hence we see that we have built what could be a competitive model.</p>
</div>
<div id="combining-cnns-and-rnns-to-process-long-sequences" class="section level3" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Combining CNNs and RNNs to process long sequences</h3>
<p>This section is split into several subsections:</p>
<ol style="list-style-type: decimal">
<li>Loading the data</li>
<li>Testing conv layers on the temperature sequence data</li>
<li>Testing using GRU and conv layers</li>
</ol>
<p><em>Summary: We observe that using only conv layers does not work well, as it is important with the time of the observation is in the right order, for instance a night temperature might look the same as the daily temperature during colder months. This it will not now how to</em></p>
<p>First we must load the jena data again</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="deep-learning-for-text-and-sequences.html#cb581-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tibble)</span>
<span id="cb581-2"><a href="deep-learning-for-text-and-sequences.html#cb581-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb581-3"><a href="deep-learning-for-text-and-sequences.html#cb581-3" aria-hidden="true" tabindex="-1"></a>dir.download <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="st">&quot;Data/3. Deep Learning/jena_climate&quot;</span>)</span>
<span id="cb581-4"><a href="deep-learning-for-text-and-sequences.html#cb581-4" aria-hidden="true" tabindex="-1"></a>data_dir <span class="ot">&lt;-</span> dir.download</span>
<span id="cb581-5"><a href="deep-learning-for-text-and-sequences.html#cb581-5" aria-hidden="true" tabindex="-1"></a>fname <span class="ot">&lt;-</span> <span class="fu">file.path</span>(data_dir, <span class="st">&quot;jena_climate_2009_2016.csv&quot;</span>)</span>
<span id="cb581-6"><a href="deep-learning-for-text-and-sequences.html#cb581-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(fname)</span>
<span id="cb581-7"><a href="deep-learning-for-text-and-sequences.html#cb581-7" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(data)</span>
<span id="cb581-8"><a href="deep-learning-for-text-and-sequences.html#cb581-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-9"><a href="deep-learning-for-text-and-sequences.html#cb581-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.31. Converting the data into a floating-point matrix</span></span>
<span id="cb581-10"><a href="deep-learning-for-text-and-sequences.html#cb581-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(data[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb581-11"><a href="deep-learning-for-text-and-sequences.html#cb581-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-12"><a href="deep-learning-for-text-and-sequences.html#cb581-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.32. Normalizing the data</span></span>
<span id="cb581-13"><a href="deep-learning-for-text-and-sequences.html#cb581-13" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> data[<span class="dv">1</span><span class="sc">:</span><span class="dv">200000</span>,]</span>
<span id="cb581-14"><a href="deep-learning-for-text-and-sequences.html#cb581-14" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_data, <span class="dv">2</span>, mean) <span class="co">#calc. the mean of each variable</span></span>
<span id="cb581-15"><a href="deep-learning-for-text-and-sequences.html#cb581-15" aria-hidden="true" tabindex="-1"></a>std <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_data, <span class="dv">2</span>, sd) <span class="co">#calc. st.dev. of each variable</span></span>
<span id="cb581-16"><a href="deep-learning-for-text-and-sequences.html#cb581-16" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">scale</span>(data, <span class="at">center =</span> mean, <span class="at">scale =</span> std) <span class="co">#scales according to the mean and st.dev</span></span>
<span id="cb581-17"><a href="deep-learning-for-text-and-sequences.html#cb581-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-18"><a href="deep-learning-for-text-and-sequences.html#cb581-18" aria-hidden="true" tabindex="-1"></a>lookback <span class="ot">&lt;-</span> <span class="dv">1440</span></span>
<span id="cb581-19"><a href="deep-learning-for-text-and-sequences.html#cb581-19" aria-hidden="true" tabindex="-1"></a>step <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb581-20"><a href="deep-learning-for-text-and-sequences.html#cb581-20" aria-hidden="true" tabindex="-1"></a>delay <span class="ot">&lt;-</span> <span class="dv">144</span></span>
<span id="cb581-21"><a href="deep-learning-for-text-and-sequences.html#cb581-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-22"><a href="deep-learning-for-text-and-sequences.html#cb581-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.33. Generator yielding timeseries samples and their targets</span></span>
<span id="cb581-23"><a href="deep-learning-for-text-and-sequences.html#cb581-23" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">128</span> <span class="co">#Originally 128</span></span>
<span id="cb581-24"><a href="deep-learning-for-text-and-sequences.html#cb581-24" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">&lt;-</span> </span>
<span id="cb581-25"><a href="deep-learning-for-text-and-sequences.html#cb581-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(data,lookback, delay, min_index, max_index,</span>
<span id="cb581-26"><a href="deep-learning-for-text-and-sequences.html#cb581-26" aria-hidden="true" tabindex="-1"></a>                      <span class="at">shuffle =</span> <span class="cn">FALSE</span>,</span>
<span id="cb581-27"><a href="deep-learning-for-text-and-sequences.html#cb581-27" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> batch_size, <span class="at">step =</span> step) {</span>
<span id="cb581-28"><a href="deep-learning-for-text-and-sequences.html#cb581-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">is.null</span>(max_index))</span>
<span id="cb581-29"><a href="deep-learning-for-text-and-sequences.html#cb581-29" aria-hidden="true" tabindex="-1"></a>    max_index <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data) <span class="sc">-</span> delay <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb581-30"><a href="deep-learning-for-text-and-sequences.html#cb581-30" aria-hidden="true" tabindex="-1"></a>  i <span class="ot">&lt;-</span> min_index <span class="sc">+</span> lookback</span>
<span id="cb581-31"><a href="deep-learning-for-text-and-sequences.html#cb581-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>() {</span>
<span id="cb581-32"><a href="deep-learning-for-text-and-sequences.html#cb581-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (shuffle) {</span>
<span id="cb581-33"><a href="deep-learning-for-text-and-sequences.html#cb581-33" aria-hidden="true" tabindex="-1"></a>      rows <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>((min_index<span class="sc">+</span>lookback)<span class="sc">:</span>max_index)</span>
<span id="cb581-34"><a href="deep-learning-for-text-and-sequences.html#cb581-34" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">size =</span> batch_size)</span>
<span id="cb581-35"><a href="deep-learning-for-text-and-sequences.html#cb581-35" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> { </span>
<span id="cb581-36"><a href="deep-learning-for-text-and-sequences.html#cb581-36" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (i <span class="sc">+</span> batch_size <span class="sc">&gt;=</span> max_index)</span>
<span id="cb581-37"><a href="deep-learning-for-text-and-sequences.html#cb581-37" aria-hidden="true" tabindex="-1"></a>        i <span class="ot">&lt;&lt;-</span> min_index <span class="sc">+</span> lookback</span>
<span id="cb581-38"><a href="deep-learning-for-text-and-sequences.html#cb581-38" aria-hidden="true" tabindex="-1"></a>      rows <span class="ot">&lt;-</span> <span class="fu">c</span>(i<span class="sc">:</span><span class="fu">min</span>((i<span class="sc">+</span>batch_size)<span class="sc">-</span><span class="dv">1</span>, max_index))</span>
<span id="cb581-39"><a href="deep-learning-for-text-and-sequences.html#cb581-39" aria-hidden="true" tabindex="-1"></a>      i <span class="ot">&lt;&lt;-</span> i <span class="sc">+</span> <span class="fu">length</span>(rows)</span>
<span id="cb581-40"><a href="deep-learning-for-text-and-sequences.html#cb581-40" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb581-41"><a href="deep-learning-for-text-and-sequences.html#cb581-41" aria-hidden="true" tabindex="-1"></a>    samples <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(rows),</span>
<span id="cb581-42"><a href="deep-learning-for-text-and-sequences.html#cb581-42" aria-hidden="true" tabindex="-1"></a>                                lookback <span class="sc">/</span> step,</span>
<span id="cb581-43"><a href="deep-learning-for-text-and-sequences.html#cb581-43" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]]))</span>
<span id="cb581-44"><a href="deep-learning-for-text-and-sequences.html#cb581-44" aria-hidden="true" tabindex="-1"></a>    targets <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(<span class="fu">length</span>(rows)))</span>
<span id="cb581-45"><a href="deep-learning-for-text-and-sequences.html#cb581-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rows)) {</span>
<span id="cb581-46"><a href="deep-learning-for-text-and-sequences.html#cb581-46" aria-hidden="true" tabindex="-1"></a>      indices <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> rows[[j]] <span class="sc">-</span> lookback</span>
<span id="cb581-47"><a href="deep-learning-for-text-and-sequences.html#cb581-47" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">to =</span> rows[[j]]</span>
<span id="cb581-48"><a href="deep-learning-for-text-and-sequences.html#cb581-48" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">length.out =</span> <span class="fu">dim</span>(samples)[[<span class="dv">2</span>]])</span>
<span id="cb581-49"><a href="deep-learning-for-text-and-sequences.html#cb581-49" aria-hidden="true" tabindex="-1"></a>      samples[j,,] <span class="ot">&lt;-</span> data[indices,]</span>
<span id="cb581-50"><a href="deep-learning-for-text-and-sequences.html#cb581-50" aria-hidden="true" tabindex="-1"></a>      targets[[j]] <span class="ot">&lt;-</span> data[rows[[j]] <span class="sc">+</span> delay,<span class="dv">2</span>] </span>
<span id="cb581-51"><a href="deep-learning-for-text-and-sequences.html#cb581-51" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb581-52"><a href="deep-learning-for-text-and-sequences.html#cb581-52" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(samples, targets)</span>
<span id="cb581-53"><a href="deep-learning-for-text-and-sequences.html#cb581-53" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb581-54"><a href="deep-learning-for-text-and-sequences.html#cb581-54" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb581-55"><a href="deep-learning-for-text-and-sequences.html#cb581-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-56"><a href="deep-learning-for-text-and-sequences.html#cb581-56" aria-hidden="true" tabindex="-1"></a>train_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb581-57"><a href="deep-learning-for-text-and-sequences.html#cb581-57" aria-hidden="true" tabindex="-1"></a>  data, <span class="co">#The normalized data</span></span>
<span id="cb581-58"><a href="deep-learning-for-text-and-sequences.html#cb581-58" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback, <span class="co">#How far we look back</span></span>
<span id="cb581-59"><a href="deep-learning-for-text-and-sequences.html#cb581-59" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay, <span class="co">#How far in the future we want to look</span></span>
<span id="cb581-60"><a href="deep-learning-for-text-and-sequences.html#cb581-60" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">1</span>, <span class="co">#First train observation</span></span>
<span id="cb581-61"><a href="deep-learning-for-text-and-sequences.html#cb581-61" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">200000</span>, <span class="co">#Last train observation</span></span>
<span id="cb581-62"><a href="deep-learning-for-text-and-sequences.html#cb581-62" aria-hidden="true" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="co">#Do we want to shuffle the data or take it chronoligical?</span></span>
<span id="cb581-63"><a href="deep-learning-for-text-and-sequences.html#cb581-63" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step, <span class="co">#The period (in timesteps)</span></span>
<span id="cb581-64"><a href="deep-learning-for-text-and-sequences.html#cb581-64" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size <span class="co">#The amount of samples pr. bach</span></span>
<span id="cb581-65"><a href="deep-learning-for-text-and-sequences.html#cb581-65" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb581-66"><a href="deep-learning-for-text-and-sequences.html#cb581-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-67"><a href="deep-learning-for-text-and-sequences.html#cb581-67" aria-hidden="true" tabindex="-1"></a>val_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb581-68"><a href="deep-learning-for-text-and-sequences.html#cb581-68" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb581-69"><a href="deep-learning-for-text-and-sequences.html#cb581-69" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb581-70"><a href="deep-learning-for-text-and-sequences.html#cb581-70" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb581-71"><a href="deep-learning-for-text-and-sequences.html#cb581-71" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">200001</span>, <span class="co">#We see that we extend from the train set</span></span>
<span id="cb581-72"><a href="deep-learning-for-text-and-sequences.html#cb581-72" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">300000</span>,</span>
<span id="cb581-73"><a href="deep-learning-for-text-and-sequences.html#cb581-73" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step,</span>
<span id="cb581-74"><a href="deep-learning-for-text-and-sequences.html#cb581-74" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size</span>
<span id="cb581-75"><a href="deep-learning-for-text-and-sequences.html#cb581-75" aria-hidden="true" tabindex="-1"></a>) <span class="co">#Notice that we don&#39;t shuffle</span></span>
<span id="cb581-76"><a href="deep-learning-for-text-and-sequences.html#cb581-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-77"><a href="deep-learning-for-text-and-sequences.html#cb581-77" aria-hidden="true" tabindex="-1"></a>test_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb581-78"><a href="deep-learning-for-text-and-sequences.html#cb581-78" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb581-79"><a href="deep-learning-for-text-and-sequences.html#cb581-79" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb581-80"><a href="deep-learning-for-text-and-sequences.html#cb581-80" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb581-81"><a href="deep-learning-for-text-and-sequences.html#cb581-81" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">300001</span>, <span class="co">#We extend from the validation data</span></span>
<span id="cb581-82"><a href="deep-learning-for-text-and-sequences.html#cb581-82" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="cn">NULL</span>,</span>
<span id="cb581-83"><a href="deep-learning-for-text-and-sequences.html#cb581-83" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step,</span>
<span id="cb581-84"><a href="deep-learning-for-text-and-sequences.html#cb581-84" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size</span>
<span id="cb581-85"><a href="deep-learning-for-text-and-sequences.html#cb581-85" aria-hidden="true" tabindex="-1"></a>) <span class="co">#Notice that we don&#39;t shuffle</span></span>
<span id="cb581-86"><a href="deep-learning-for-text-and-sequences.html#cb581-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb581-87"><a href="deep-learning-for-text-and-sequences.html#cb581-87" aria-hidden="true" tabindex="-1"></a><span class="co">#Identifying amount of timesteps we can validate on</span></span>
<span id="cb581-88"><a href="deep-learning-for-text-and-sequences.html#cb581-88" aria-hidden="true" tabindex="-1"></a>val_steps <span class="ot">&lt;-</span> (<span class="dv">300000</span> <span class="sc">-</span> <span class="dv">200001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span>
<span id="cb581-89"><a href="deep-learning-for-text-and-sequences.html#cb581-89" aria-hidden="true" tabindex="-1"></a>test_steps <span class="ot">&lt;-</span> (<span class="fu">nrow</span>(data) <span class="sc">-</span> <span class="dv">300001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span></code></pre></div>
<p>Now where we have loaded the data we can build the model with only using convoluted layers and max pooling.</p>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb582-1"><a href="deep-learning-for-text-and-sequences.html#cb582-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.47. Training and evaluating a simple 1D convnet on the Jena data </span></span>
<span id="cb582-2"><a href="deep-learning-for-text-and-sequences.html#cb582-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb582-3"><a href="deep-learning-for-text-and-sequences.html#cb582-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">5</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>,</span>
<span id="cb582-4"><a href="deep-learning-for-text-and-sequences.html#cb582-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]])) <span class="sc">%&gt;%</span></span>
<span id="cb582-5"><a href="deep-learning-for-text-and-sequences.html#cb582-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_1d</span>(<span class="at">pool_size =</span> <span class="dv">3</span>) <span class="sc">%&gt;%</span></span>
<span id="cb582-6"><a href="deep-learning-for-text-and-sequences.html#cb582-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">5</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb582-7"><a href="deep-learning-for-text-and-sequences.html#cb582-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_1d</span>(<span class="at">pool_size =</span> <span class="dv">3</span>) <span class="sc">%&gt;%</span></span>
<span id="cb582-8"><a href="deep-learning-for-text-and-sequences.html#cb582-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">5</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb582-9"><a href="deep-learning-for-text-and-sequences.html#cb582-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_global_max_pooling_1d</span>() <span class="sc">%&gt;%</span></span>
<span id="cb582-10"><a href="deep-learning-for-text-and-sequences.html#cb582-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb582-11"><a href="deep-learning-for-text-and-sequences.html#cb582-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb582-12"><a href="deep-learning-for-text-and-sequences.html#cb582-12" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb582-13"><a href="deep-learning-for-text-and-sequences.html#cb582-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb582-14"><a href="deep-learning-for-text-and-sequences.html#cb582-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb582-15"><a href="deep-learning-for-text-and-sequences.html#cb582-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb582-16"><a href="deep-learning-for-text-and-sequences.html#cb582-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb582-17"><a href="deep-learning-for-text-and-sequences.html#cb582-17" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit_generator</span>(</span>
<span id="cb582-18"><a href="deep-learning-for-text-and-sequences.html#cb582-18" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb582-19"><a href="deep-learning-for-text-and-sequences.html#cb582-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb582-20"><a href="deep-learning-for-text-and-sequences.html#cb582-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb582-21"><a href="deep-learning-for-text-and-sequences.html#cb582-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb582-22"><a href="deep-learning-for-text-and-sequences.html#cb582-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb582-23"><a href="deep-learning-for-text-and-sequences.html#cb582-23" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="deep-learning-for-text-and-sequences.html#cb583-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p>we see that the model starts overfitting immediately and is not even close to being competitive with the base line.</p>
<p>The reasoning is that now the patterns are more independent of each other. As with temperature it is very important what time the temperature is measured as when only looking at snippets we may not be able to distinguish between low degrees during night or during cold months.</p>
<p>In the next we are goint to combine ann RNN with a CNN. Notice that we are making the following changes:</p>
<ul>
<li>Making <code>step = 3</code> instead of <code>step = 6</code>, meaning that data is sampled at one observation pr. 30 minutes instead of 1 observation pr. 60 minutes (One step = 10 minutes).</li>
<li>Halving lookback</li>
</ul>
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb584-1"><a href="deep-learning-for-text-and-sequences.html#cb584-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.48. Preparing higher-resolution data generators for the Jena dataset </span></span>
<span id="cb584-2"><a href="deep-learning-for-text-and-sequences.html#cb584-2" aria-hidden="true" tabindex="-1"></a>step <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb584-3"><a href="deep-learning-for-text-and-sequences.html#cb584-3" aria-hidden="true" tabindex="-1"></a>lookback <span class="ot">&lt;-</span> <span class="dv">720</span></span>
<span id="cb584-4"><a href="deep-learning-for-text-and-sequences.html#cb584-4" aria-hidden="true" tabindex="-1"></a>delay <span class="ot">&lt;-</span> <span class="dv">144</span></span>
<span id="cb584-5"><a href="deep-learning-for-text-and-sequences.html#cb584-5" aria-hidden="true" tabindex="-1"></a>train_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb584-6"><a href="deep-learning-for-text-and-sequences.html#cb584-6" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb584-7"><a href="deep-learning-for-text-and-sequences.html#cb584-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb584-8"><a href="deep-learning-for-text-and-sequences.html#cb584-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb584-9"><a href="deep-learning-for-text-and-sequences.html#cb584-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">1</span>,</span>
<span id="cb584-10"><a href="deep-learning-for-text-and-sequences.html#cb584-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">200000</span>,</span>
<span id="cb584-11"><a href="deep-learning-for-text-and-sequences.html#cb584-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">TRUE</span>,</span>
<span id="cb584-12"><a href="deep-learning-for-text-and-sequences.html#cb584-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step</span>
<span id="cb584-13"><a href="deep-learning-for-text-and-sequences.html#cb584-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb584-14"><a href="deep-learning-for-text-and-sequences.html#cb584-14" aria-hidden="true" tabindex="-1"></a>val_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb584-15"><a href="deep-learning-for-text-and-sequences.html#cb584-15" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb584-16"><a href="deep-learning-for-text-and-sequences.html#cb584-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb584-17"><a href="deep-learning-for-text-and-sequences.html#cb584-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb584-18"><a href="deep-learning-for-text-and-sequences.html#cb584-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">200001</span>,</span>
<span id="cb584-19"><a href="deep-learning-for-text-and-sequences.html#cb584-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="dv">300000</span>,</span>
<span id="cb584-20"><a href="deep-learning-for-text-and-sequences.html#cb584-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step</span>
<span id="cb584-21"><a href="deep-learning-for-text-and-sequences.html#cb584-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb584-22"><a href="deep-learning-for-text-and-sequences.html#cb584-22" aria-hidden="true" tabindex="-1"></a>test_gen <span class="ot">&lt;-</span> <span class="fu">generator</span>(</span>
<span id="cb584-23"><a href="deep-learning-for-text-and-sequences.html#cb584-23" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb584-24"><a href="deep-learning-for-text-and-sequences.html#cb584-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">lookback =</span> lookback,</span>
<span id="cb584-25"><a href="deep-learning-for-text-and-sequences.html#cb584-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">delay =</span> delay,</span>
<span id="cb584-26"><a href="deep-learning-for-text-and-sequences.html#cb584-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_index =</span> <span class="dv">300001</span>,</span>
<span id="cb584-27"><a href="deep-learning-for-text-and-sequences.html#cb584-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_index =</span> <span class="cn">NULL</span>,</span>
<span id="cb584-28"><a href="deep-learning-for-text-and-sequences.html#cb584-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">step =</span> step</span>
<span id="cb584-29"><a href="deep-learning-for-text-and-sequences.html#cb584-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb584-30"><a href="deep-learning-for-text-and-sequences.html#cb584-30" aria-hidden="true" tabindex="-1"></a>val_steps <span class="ot">&lt;-</span> (<span class="dv">300000</span> <span class="sc">-</span> <span class="dv">200001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span>
<span id="cb584-31"><a href="deep-learning-for-text-and-sequences.html#cb584-31" aria-hidden="true" tabindex="-1"></a>test_steps <span class="ot">&lt;-</span> (<span class="fu">nrow</span>(data) <span class="sc">-</span> <span class="dv">300001</span> <span class="sc">-</span> lookback) <span class="sc">/</span> batch_size</span></code></pre></div>
<p>For some reason the following cannot run. <strong>NEEDS REVISIT</strong>.</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="deep-learning-for-text-and-sequences.html#cb585-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing 6.49. Model combining a 1D convolutional base and a GRU layer </span></span>
<span id="cb585-2"><a href="deep-learning-for-text-and-sequences.html#cb585-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb585-3"><a href="deep-learning-for-text-and-sequences.html#cb585-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">5</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>,</span>
<span id="cb585-4"><a href="deep-learning-for-text-and-sequences.html#cb585-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">dim</span>(data)[[<span class="sc">-</span><span class="dv">1</span>]])) <span class="sc">%&gt;%</span></span>
<span id="cb585-5"><a href="deep-learning-for-text-and-sequences.html#cb585-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_1d</span>(<span class="at">pool_size =</span> <span class="dv">3</span>) <span class="sc">%&gt;%</span></span>
<span id="cb585-6"><a href="deep-learning-for-text-and-sequences.html#cb585-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters =</span> <span class="dv">32</span>, <span class="at">kernel_size =</span> <span class="dv">5</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb585-7"><a href="deep-learning-for-text-and-sequences.html#cb585-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_gru</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.1</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb585-8"><a href="deep-learning-for-text-and-sequences.html#cb585-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb585-9"><a href="deep-learning-for-text-and-sequences.html#cb585-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb585-10"><a href="deep-learning-for-text-and-sequences.html#cb585-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb585-11"><a href="deep-learning-for-text-and-sequences.html#cb585-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb585-12"><a href="deep-learning-for-text-and-sequences.html#cb585-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span></span>
<span id="cb585-13"><a href="deep-learning-for-text-and-sequences.html#cb585-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb585-14"><a href="deep-learning-for-text-and-sequences.html#cb585-14" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb585-15"><a href="deep-learning-for-text-and-sequences.html#cb585-15" aria-hidden="true" tabindex="-1"></a>  train_gen,</span>
<span id="cb585-16"><a href="deep-learning-for-text-and-sequences.html#cb585-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">steps_per_epoch =</span> <span class="dv">500</span>,</span>
<span id="cb585-17"><a href="deep-learning-for-text-and-sequences.html#cb585-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb585-18"><a href="deep-learning-for-text-and-sequences.html#cb585-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> val_gen,</span>
<span id="cb585-19"><a href="deep-learning-for-text-and-sequences.html#cb585-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_steps =</span> val_steps</span>
<span id="cb585-20"><a href="deep-learning-for-text-and-sequences.html#cb585-20" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We see that the regularized GRU was perfoming better than this hybrid and the model is just at best as good as the baseline, but does not appear to outperform it.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter-5-deep-learning-for-computer-vision.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-deep-learning-best-practices.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
