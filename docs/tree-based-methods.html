<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Tree Based Methods | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="3 Tree Based Methods | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Tree Based Methods | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Tree Based Methods | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moving-beyond-linearity.html"/>
<link rel="next" href="support-vector-machines.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#models-beyond-linearity"><i class="fa fa-check"></i><b>2.1</b> Models Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#degrees-of-freedom"><i class="fa fa-check"></i><b>2.1.3.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="2.1.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.5</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#feature-selection"><i class="fa fa-check"></i><b>2.1.6.1</b> Feature Selection</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for regression problems</a>
<ul>
<li class="chapter" data-level="2.1.6.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#interpretation-of-output"><i class="fa fa-check"></i><b>2.1.6.2.1</b> Interpretation of output</a></li>
</ul></li>
<li class="chapter" data-level="2.1.6.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.3</b> GAM for classification problems</a></li>
</ul></li>
<li class="chapter" data-level="2.1.7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#model-assessment"><i class="fa fa-check"></i><b>2.1.7</b> Model assessment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lecture-notes"><i class="fa fa-check"></i><b>2.2</b> Lecture notes</a></li>
<li class="chapter" data-level="2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lab-section"><i class="fa fa-check"></i><b>2.3</b> Lab section</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression-and-step-functions"><i class="fa fa-check"></i><b>2.3.1</b> Polynomial Regression and Step Functions</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#continous-model"><i class="fa fa-check"></i><b>2.3.1.1</b> Continous model</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logarithmic-model"><i class="fa fa-check"></i><b>2.3.1.2</b> Logarithmic model</a></li>
<li class="chapter" data-level="2.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-function"><i class="fa fa-check"></i><b>2.3.1.3</b> Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#splines"><i class="fa fa-check"></i><b>2.3.2</b> Splines</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#basis-function-splines"><i class="fa fa-check"></i><b>2.3.2.1</b> Basis Function Splines</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#natural-splines"><i class="fa fa-check"></i><b>2.3.2.2</b> Natural Splines</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smooth-splines"><i class="fa fa-check"></i><b>2.3.2.3</b> Smooth Splines</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression-1"><i class="fa fa-check"></i><b>2.3.2.4</b> Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gams"><i class="fa fa-check"></i><b>2.3.3</b> GAMs</a>
<ul>
<li class="chapter" data-level="2.3.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-only-natural-splines"><i class="fa fa-check"></i><b>2.3.3.1</b> With only natural splines</a></li>
<li class="chapter" data-level="2.3.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-different-splines"><i class="fa fa-check"></i><b>2.3.3.2</b> With different splines</a></li>
<li class="chapter" data-level="2.3.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#but-what-variables-to-include"><i class="fa fa-check"></i><b>2.3.3.3</b> But what variables to include?</a></li>
<li class="chapter" data-level="2.3.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-with-local-regression"><i class="fa fa-check"></i><b>2.3.3.4</b> GAM with local regression</a></li>
<li class="chapter" data-level="2.3.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logistic-regression"><i class="fa fa-check"></i><b>2.3.3.5</b> Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a>
<ul>
<li class="chapter" data-level="2.4.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-polynomial-regression"><i class="fa fa-check"></i><b>2.4.1.1</b> 6.a Polynomial Regression</a></li>
<li class="chapter" data-level="2.4.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-step-function"><i class="fa fa-check"></i><b>2.4.1.2</b> 6.b Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a>
<ul>
<li class="chapter" data-level="2.4.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-using-poly-function-to-fit-cubic-polynomial-regression"><i class="fa fa-check"></i><b>2.4.4.1</b> (a) using poly function to fit cubic polynomial regression</a></li>
<li class="chapter" data-level="2.4.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-plotting-polynomial-fits-for-a-range-of-polynomials"><i class="fa fa-check"></i><b>2.4.4.2</b> (b) Plotting polynomial fits for a range of polynomials</a></li>
<li class="chapter" data-level="2.4.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-using-cv-to-select-best-degree-of-d"><i class="fa fa-check"></i><b>2.4.4.3</b> (c) Using CV to select best degree of d</a></li>
<li class="chapter" data-level="2.4.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-use-bs-to-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.4</b> (d) Use <code>bs()</code> to fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#e-now-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.5</b> (e) Now fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#f-perform-cross-validation-to-select-degrees"><i class="fa fa-check"></i><b>2.4.4.6</b> (f) Perform cross-validation, to select degrees</a></li>
</ul></li>
<li class="chapter" data-level="2.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a>
<ul>
<li class="chapter" data-level="2.4.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-partitioning-the-data"><i class="fa fa-check"></i><b>2.4.5.1</b> (a) Partitioning the data</a></li>
<li class="chapter" data-level="2.4.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-fitting-a-gam"><i class="fa fa-check"></i><b>2.4.5.2</b> (b) Fitting a GAM</a></li>
<li class="chapter" data-level="2.4.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-evaluating-on-the-test-set"><i class="fa fa-check"></i><b>2.4.5.3</b> (c) Evaluating on the test set</a></li>
<li class="chapter" data-level="2.4.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-which-variables-appear-to-have-a-non-linear-relationship"><i class="fa fa-check"></i><b>2.4.5.4</b> (d) Which variables appear to have a non linear relationship?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#FacebookCasestudy"><i class="fa fa-check"></i><b>2.5</b> Predicting the Return on Advertising Spent</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#background"><i class="fa fa-check"></i><b>2.5.1</b> 1. Background</a></li>
<li class="chapter" data-level="2.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#case-study-business-understanding-phase"><i class="fa fa-check"></i><b>2.5.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#the-data-data-understanding-phase"><i class="fa fa-check"></i><b>2.5.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#specific-requirements"><i class="fa fa-check"></i><b>2.5.4</b> 4. Specific requirements:</a>
<ul>
<li class="chapter" data-level="2.5.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-1---import-and-overview"><i class="fa fa-check"></i><b>2.5.4.1</b> 4.1 Task 1 - Import and overview</a></li>
<li class="chapter" data-level="2.5.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-2---data-inspection"><i class="fa fa-check"></i><b>2.5.4.2</b> 4.2 Task 2 - Data inspection</a></li>
<li class="chapter" data-level="2.5.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-3---building-different-models"><i class="fa fa-check"></i><b>2.5.4.3</b> 4.3 Task 3 - Building different models</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-generalized-additive-model-gam-to-predict-roas"><i class="fa fa-check"></i><b>2.5.4.3.1</b> A Generalized Additive Model (GAM) to predict ROAS</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c1-feature-selection-using-regsubsets"><i class="fa fa-check"></i><b>2.5.4.3.1.1</b> c1) Feature selection using regsubsets()</a></li>
<li class="chapter" data-level="2.5.4.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c2-feature-selection-using-step.gam"><i class="fa fa-check"></i><b>2.5.4.3.1.2</b> c2) Feature selection using step.GAM</a></li>
<li class="chapter" data-level="2.5.4.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c3-fetaure-selection-using-random-forest"><i class="fa fa-check"></i><b>2.5.4.3.1.3</b> c3) Fetaure selection using random forest</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>3.1</b> The Basics of Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-trees"><i class="fa fa-check"></i><b>3.1.1</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#how-to-make-the-decision-trees"><i class="fa fa-check"></i><b>3.1.1.1</b> How to make the decision trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-goal-of-regression"><i class="fa fa-check"></i><b>3.1.1.1.1</b> The goal of regression</a></li>
<li class="chapter" data-level="3.1.1.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning-algoritm"><i class="fa fa-check"></i><b>3.1.1.1.2</b> Tree Pruning &amp; Algoritm</a></li>
<li class="chapter" data-level="3.1.1.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#setting-contrains-of-the-tree-sise"><i class="fa fa-check"></i><b>3.1.1.1.3</b> Setting contrains of the tree sise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#classification-trees"><i class="fa fa-check"></i><b>3.1.2</b> Classification Trees</a></li>
<li class="chapter" data-level="3.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-vs.-lienar-models"><i class="fa fa-check"></i><b>3.1.3</b> Tree vs. Lienar Models</a></li>
<li class="chapter" data-level="3.1.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advantages-and-disadvantages-of-trees"><i class="fa fa-check"></i><b>3.1.4</b> Advantages and Disadvantages of Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-random-forests-boosting"><i class="fa fa-check"></i><b>3.2</b> Bagging, Random Forests, Boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-bootstrap-aggregation"><i class="fa fa-check"></i><b>3.2.1</b> Bagging (Bootstrap Aggregation)</a></li>
<li class="chapter" data-level="3.2.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="3.2.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-i.e.-gradient-boosting"><i class="fa fa-check"></i><b>3.2.3</b> Boosting (i.e. Gradient Boosting)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost"><i class="fa fa-check"></i><b>3.3</b> XGBoost</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#application-in-r"><i class="fa fa-check"></i><b>3.4</b> Application in R</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.4.1</b> Decision trees</a></li>
<li class="chapter" data-level="3.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.4.2</b> Bagging</a></li>
<li class="chapter" data-level="3.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-1"><i class="fa fa-check"></i><b>3.4.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4.4</b> Boosting</a></li>
<li class="chapter" data-level="3.4.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost-1"><i class="fa fa-check"></i><b>3.4.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#lab-section-1"><i class="fa fa-check"></i><b>3.5</b> Lab section</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#LabClassification"><i class="fa fa-check"></i><b>3.5.1</b> Fitting Classification Trees</a></li>
<li class="chapter" data-level="3.5.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#fitting-regression-trees"><i class="fa fa-check"></i><b>3.5.2</b> Fitting Regression Trees</a></li>
<li class="chapter" data-level="3.5.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-and-random-forests"><i class="fa fa-check"></i><b>3.5.3</b> Bagging and Random Forests</a>
<ul>
<li class="chapter" data-level="3.5.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-1"><i class="fa fa-check"></i><b>3.5.3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.5.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forest"><i class="fa fa-check"></i><b>3.5.3.2</b> Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="3.5.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-1"><i class="fa fa-check"></i><b>3.5.4</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercies"><i class="fa fa-check"></i><b>3.6</b> Exercies</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-7---decision-tree-assessment"><i class="fa fa-check"></i><b>3.6.1</b> Exercise 7 - Decision Tree Assessment</a></li>
<li class="chapter" data-level="3.6.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-8---dtrfboosting"><i class="fa fa-check"></i><b>3.6.2</b> Exercise 8 - DT/RF/Boosting</a></li>
<li class="chapter" data-level="3.6.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-9---decision-tree-pruning"><i class="fa fa-check"></i><b>3.6.3</b> Exercise 9 - Decision Tree / Pruning</a></li>
<li class="chapter" data-level="3.6.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-10---boostinggamlinearregbagging---comparison"><i class="fa fa-check"></i><b>3.6.4</b> Exercise 10 - Boosting/GAM/LinearReg/Bagging - Comparison</a></li>
<li class="chapter" data-level="3.6.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-11---boosting"><i class="fa fa-check"></i><b>3.6.5</b> Exercise 11 - Boosting</a></li>
<li class="chapter" data-level="3.6.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-12"><i class="fa fa-check"></i><b>3.6.6</b> Exercise 12</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#casestudy"><i class="fa fa-check"></i><b>3.7</b> Casestudy</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Deep Learning Fundamentals</a></li>
<li class="chapter" data-level="6" data-path="structuring-data-transformation-and-model-assessments.html"><a href="structuring-data-transformation-and-model-assessments.html"><i class="fa fa-check"></i><b>6</b> Structuring data transformation and model assessments</a></li>
<li class="chapter" data-level="7" data-path="managing-github.html"><a href="managing-github.html"><i class="fa fa-check"></i><b>7</b> Managing GitHub</a></li>
<li class="chapter" data-level="8" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>8</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-methods" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Tree Based Methods</h1>
<p>In its essence the tree based methods are decisions trees, which is a set of splitting rules, which can be drawn as a tree, hence the name - decision tree.</p>
<p>We have three types of trees:</p>
<ol style="list-style-type: decimal">
<li>Regression trees</li>
<li>Classification trees</li>
<li>Ensemble methods of trees, e.g., bagging, random forests and boosting</li>
</ol>
<p>In its initial form, the decision tree is typically not competitive with non linear models and shrinkage methods prediction wise. Although the decision trees can be improve through:</p>
<ol style="list-style-type: decimal">
<li>Bagging</li>
<li>Random forests</li>
<li>Boosting</li>
</ol>
<p><em>Those above are ensemble methods of trees, those above are controlled with hyper-parameters that counterfeits overfitting</em></p>
<p>The methods above yields several decision trees, which yields a single consensus prediction. We see that combining a large number of trees improves prediction power greatly but also decrease the interpretability.</p>
<div id="the-basics-of-decision-trees" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> The Basics of Decision Trees</h2>
<p>The decision trees can be applied both for regression and classification. The following sections will firstly be about regression and then classification. A decision tree looks like the following:</p>
<div class="figure" style="text-align: center"><span id="fig:SubtreeHitters"></span>
<img src="Images/DecisionTree.png" alt="Decision Tree" width="222" />
<p class="caption">
Figure 3.1: Decision Tree
</p>
</div>
<p>We see that the illustration has <strong>2 internal nodes</strong> - years &lt; 4.5 and hits &lt; 117.5 – and <strong>3 terminal nodes</strong> (i.e. leaf/leaves). The leaves consist of the mean of the response given the criterias in the decision tree. The lines connecting the nodes (internal and terminal) are called branches.</p>
<p><em>Notice, that the same variables can occurs several times in the tree.</em></p>
<p>Each leaf can also be written as:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R_1=\)</span>{X|Years&lt;4.5}</li>
<li><span class="math inline">\(R_2=\)</span>{X|Years&gt;=4.5,Hits&lt;117.5}</li>
<li><span class="math inline">\(R_2=\)</span>{X|Years&gt;=4.5,HIts&gt;=117.5}</li>
</ol>
<p><strong><em>Terminology:</em></strong></p>
<ul>
<li>Root: best predictor</li>
<li>Splitting: This is when we do the splitting on the internal nodes / decision node</li>
<li>Leaf: The leafs contain the mean or the mode of the respons from the observations which meet the circumstances that are set in the splits.</li>
</ul>
<div id="regression-trees" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Regression Trees</h3>
<p>We can take an example:</p>
<div class="figure" style="text-align: center"><span id="fig:DecisionTree"></span>
<img src="Images/DecisionTreeHitters.png" alt="Decision Tree Hitters data" width="295" />
<p class="caption">
Figure 3.2: Decision Tree Hitters data
</p>
</div>
<p><em>Note: based on the hitters data.</em></p>
<p>We see that the observations has been separated into regions which meet the criteria, one can then take the mean of the response variable of the observations in the regions to define the terminal nodes. This may be a simplification although, it is easily interpreted.</p>
<p>It should also be mentioned that the criteria, also shown in previous section, indicate that we are working with hard boundaries, hence the regions does not overlap with each other.</p>
<div id="how-to-make-the-decision-trees" class="section level4" number="3.1.1.1">
<h4><span class="header-section-number">3.1.1.1</span> How to make the decision trees</h4>
<p>Precedure:</p>
<ol style="list-style-type: decimal">
<li>Divide the predictor space into regions, as in the example above. We end up with J dostinct and non overlapping regions, we call these <span class="math inline">\(R_1,R_2,...,R_J\)</span></li>
<li>For all observations in region <span class="math inline">\(R_J\)</span>, we take the mean of the response variable.</li>
</ol>
<p>Notice, that the way of optimizing RSS is with an approach starting on one varaible and then spreading out. We call this the <em>recursive binary splitting</em>, that is a top-down approach which is said to be greedy. In the very initial phase of the splitting procedure, all observations are in the same region, and then we start splitting up the regions, figure <a href="tree-based-methods.html#fig:DTDims">3.3</a> show examples of having a more complex model and also including the mean of the response variable in the illustration.</p>
<p>Notice the top left illustration, this is not from a recursive binary splitting process, hence it yields more strange regions.</p>
<div id="the-goal-of-regression" class="section level5" number="3.1.1.1.1">
<h5><span class="header-section-number">3.1.1.1.1</span> The goal of regression</h5>
<p>We want to minimize the RSS. That can be written as:</p>
<p><span class="math display" id="eq:DTRSS">\[\begin{equation}
\text{}\sum_{j=1}^J\sum_{i\ \in R_j}^{ }\left(y_i-\hat{y}_{R_j}\right)^{^2}
\tag{3.1}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(J\)</span> = total number of regions</li>
<li><span class="math inline">\(j\)</span> = spefic number of regions in the range of 1 to J</li>
<li><span class="math inline">\(R_j\)</span> = each region, R for region i guess.</li>
<li><span class="math inline">\(i \in R_J\)</span> = the special e sign, means member ship off. Hence it can be seen as a filter on the specific regions.</li>
<li><span class="math inline">\(\hat{y}_{R_j}\)</span> = The predicted values in the specific regions, i.e. the mean response</li>
</ul>
<p>So what it says is that we take the sum of all squeard residuals. But then how do we split the regions? In general terms in can be written as the following:</p>
<p><span class="math display" id="eq:InternalNodesSplit">\[\begin{equation}
R_1\left(j,s\right)=\left\{X|X_j&lt;s\right\}\ AND\ R_2\left(j,s\right)=\left\{X|X_j\ge s\right\}
\tag{3.2}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(s\)</span> = cutpoint</li>
<li><span class="math inline">\(j\)</span> = reflectsion the regions, hence,</li>
<li><span class="math inline">\(X_j\)</span> = the x variable region.</li>
<li><span class="math inline">\((X|X_j&lt;s)\)</span> = the region of predictor space in which <span class="math inline">\(X_j\)</span> takes o a value less than s (the cutpoint)</li>
</ul>
<p>Thus in general terms we wish to select j and s that minimize the RSS, therefore we can also write the equation for RSS in general terms with:</p>
<p><span class="math display" id="eq:InternalNodesSplit">\[\begin{equation}
\sum_{i:\ e_i\in R_1\left(j,s\right)}^{ }\left(y_i-\hat{y}_{R_1}\right)^{^2}+\sum_{i:\ e_i\in R_2\left(j,s\right)}^{ }\left(y_i-\hat{y}_{R_2}\right)^{^2}
\tag{3.2}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(R_1(j,s)\)</span> define the training observations, which are a basis for <span class="math inline">\(\hat{y}_{R_1}\)</span></li>
</ul>
<p><strong><em>Adding third dimension - response variable:</em></strong></p>
<p>Basically software can quite quickly compute cutpoints of the x variables to optimize RSS. The example above in <a href="tree-based-methods.html#fig:DecisionTree">3.2</a> express two dimensions, one could also have added the response variable as a dimension. Where the fitted y (mean of response in region <span class="math inline">\(R_j\)</span>) will express the hight in the specifc region.</p>
<p><strong><em>Adding more cutpoints:</em></strong></p>
<p>Naturally we could also have hadded more cutpoints, to separate the regions even further. See an example in the following:</p>
<div class="figure" style="text-align: center"><span id="fig:DTDims"></span>
<img src="Images/DTDims.png" alt="Decision Tree Hitters data" width="455" />
<p class="caption">
Figure 3.3: Decision Tree Hitters data
</p>
</div>
<p><em>Note: the top left show regions that is not from the procedure listed above, namely the recusive binary splitting</em></p>
</div>
<div id="tree-pruning-algoritm" class="section level5" number="3.1.1.1.2">
<h5><span class="header-section-number">3.1.1.1.2</span> Tree Pruning &amp; Algoritm</h5>
<p>The more regions that you add, the more complexity and hence flexibilty. Thus, sometimes it is a good idea to have rather simple trees, to avoid fitting too much to specific observations. Also I recon, that in regions with few observations we are prone to overfitting in the specific regions.</p>
<p><strong><em>Considerations on making a stable tree</em></strong></p>
<p>One can perhaps add nodes until you don’t lower RSS, like forward selection, although sometimes a significant improvement of the model may come after a certain cutpoint, hence that is not a good approach.</p>
<p>A better approach is the opposite, where we start with making a huge tree (this we call <span class="math inline">\(T_0\)</span>) and cuts it down a subtree (merely cutting branches off). To do so, one can do <em>cost complexity pruning</em>, i.e. <em>weakest link pruning</em>. To do so we introduce <span class="math inline">\(\alpha\)</span>, which is a nonnegative tuning parameter. This can be compared with backward selection. See examples:</p>
<ul>
<li>T0: figure <a href="tree-based-methods.html#fig:T0Hitters">3.4</a></li>
<li>MSE with different alpha values in figure <a href="tree-based-methods.html#fig:TreePruningHitters">3.5</a></li>
<li>Selected subtree in figure <a href="tree-based-methods.html#fig:SubtreeHitters">3.1</a></li>
</ul>
<p><span class="math inline">\(\alpha\)</span> behaves in the following way:</p>
<ul>
<li>If <span class="math inline">\(\alpha\)</span> = 0, then the subtree = <span class="math inline">\(T_0\)</span>, hence the big tree and therefore controls the complexity of the model</li>
<li>As <span class="math inline">\(\alpha\)</span> increases we prune <span class="math inline">\(T_0\)</span> into a subtree.</li>
</ul>
<p>Thus the higher the tuning parameter, the smaller the tree. This can be compared with the lasso <span class="citation">(<a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al. 2013</a>)</span>. Thus, for each <span class="math inline">\(\alpha\)</span> there corresponds a subtree of <span class="math inline">\(T_0\)</span>, i.e. <span class="math inline">\(T \subset T_0\)</span>. This can be written as:</p>
<p><span class="math display" id="eq:Subtree">\[\begin{equation}
\sum_{m=1}^{\left|T\right|}\sum_{i:\ x_i\in R_m}^{ }\left(y_{i\ }-\hat{y}_{R_m}\right)^{^{2\ }}+\ \alpha\left|T\right|
\tag{3.3}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(|T|\)</span> = the number of terminal nodes of the tree <span class="math inline">\(T,R_m\)</span> (the subset of predictor space)</li>
</ul>
<p>Therefore we can write the procedure for building a regression tree with:</p>
<ol style="list-style-type: decimal">
<li>Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fwer than some minumum number of observations (to avoid overfitting). See an example of T0 in figure <a href="tree-based-methods.html#fig:T0Hitters">3.4</a></li>
<li>Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of <span class="math inline">\(\alpha\)</span></li>
<li>Use K-fold cross validation to choose <span class="math inline">\(\alpha\)</span>. That is, divide the training observation into K-folds. For each <span class="math inline">\(k=1,...,K\)</span>: Where we do the following:
<ol style="list-style-type: lower-alpha">
<li>Repat steps 1 and 2 on all but the kth fold of the training data</li>
<li>Evaluate the mena squared prediction error in the left-out kth fold, as a function of <span class="math inline">\(\alpha\)</span>. Average the results for each value of alpha and pick alpha to minimuzze the average error.</li>
</ol></li>
<li>Return the subtree from step 2 that corresponds to the chosen value of <span class="math inline">\(\alpha\)</span>.
<ol style="list-style-type: lower-alpha">
<li>It may be advantagous to plot the pruning process to actually see how the removed branches improve the model and select a subtree of reason instead of being blinded by the absolute lowest prediction error, see an example hereof in figure <a href="tree-based-methods.html#fig:TreePruningHitters">3.5</a></li>
</ol></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:T0Hitters"></span>
<img src="Images/T0Hitters.png" alt="T0 Hitters" width="592" />
<p class="caption">
Figure 3.4: T0 Hitters
</p>
</div>
<p><em>We see the big tree, that can be pruned to optimize prediction, the following illustration plots MSE using different tuning parameters</em></p>
<div class="figure" style="text-align: center"><span id="fig:TreePruningHitters"></span>
<img src="Images/TreePruningHitters.png" alt="Tree Pruning Hitters" width="420" />
<p class="caption">
Figure 3.5: Tree Pruning Hitters
</p>
</div>
<p><em>We see that the lowest train and CV MSE is at the tree size of 10, although between three terminal nodes and 10 terminal nodes appear to be equally good, hence we go for the most parsimonious model, see that tree in figure <a href="tree-based-methods.html#fig:SubtreeHitters">3.1</a></em></p>
<p><br />
</p>
</div>
<div id="setting-contrains-of-the-tree-sise" class="section level5" number="3.1.1.1.3">
<h5><span class="header-section-number">3.1.1.1.3</span> Setting contrains of the tree sise</h5>
<p>An alternative to tree pruning is contraining the size of the tree. That can be done with four approaches:</p>
<ol style="list-style-type: decimal">
<li>Maximum depth of the tree (vertical depth) - tuned using CV</li>
<li>Minimum observations for a node split - tuned using CV</li>
<li>Minimum observations for a terminal node (lead) - lower values for inbalanced classes</li>
<li>Maximum number of terminal nodes - can be defined instead of depth.</li>
</ol>
</div>
</div>
</div>
<div id="classification-trees" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Classification Trees</h3>
<p>The procedure is very much the same as what we saw for regression trees, where the same pruning process etc. In the classification setting we predict qualitative outcomes, e.g., yes/no. We can either select based on frequency (i.e. most commonly occuring i.e. mode) or the proportions.</p>
<p>In classification we use error rate instead of RSS, as we can’t do RSS in a classification setting. I guess we can construct a confusion matrix as well.</p>
<p>We introduce a new term: <em>node purity</em>. This is about including internal nodes that leads to two terminal nodes which has the same conclusion. It will not reduce the classification error, it does improve the certainty of the classification. Therefore, to account for the certainty and thus the node purity, one should also make the <em>Gini Index</em> and the <em>Entropy</em>, equations can be found in <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>, p. 312.</p>
<p><strong><em>More on purity: as we classify based on the mode of the response variable, we intentionally misclassify some observations. Naturally, we want to avoid this. Thus we can increase purity, that is often seen by splitting the same variable consecutive times or just several times. Hence to circle in the actual classifications. Although, doing this too much leads to overfitting to the data. That is why the Gini and Entropy adds a penalty to the in-sample error and thus accounting for purity. But one should also make an out of sample validation</em></strong></p>
<p>Thus the splits are made to optimize the Gini, Entropy or perhaps Chi-Squared.</p>
<p>If we want to write it in math, we can do the following:</p>
<p><span class="math display">\[y = f(x_1,x_2)\]</span></p>
<p>Where we see that y is a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span></p>
</div>
<div id="tree-vs.-lienar-models" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Tree vs. Lienar Models</h3>
<p>One is never strictlu better than the other. But in general, one should start with a simple model, such as linear regression, to have a baseline, which we can compare more advanced mdoels with.</p>
<p>If the actual varaince of the response variable can be explained by linear regression, then it is likely to outperform a tree model, but if one experience more complex data, then it is likely that the tree model will outperform the linear model.</p>
</div>
<div id="advantages-and-disadvantages-of-trees" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Advantages and Disadvantages of Trees</h3>
<p>This is compared to classical approaches, linear regression and classification <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>, CH3 and CH 4.</p>
<p><strong><em>Advantges:</em></strong></p>
<ol style="list-style-type: decimal">
<li>Trees are very easy to explain to people. And also easy to visualize and understand.</li>
<li>Some argues that decision trees better mimiic human reasoning and decision making.</li>
<li>Can be shown graphically</li>
<li>One don’t have to construct dummy variables.</li>
<li>Simple and useful for interpretation</li>
<li>No need to transform features</li>
<li>Less cleaning required and not so sensible to outliers</li>
<li>No need to transform features</li>
<li>No assumptions, purely data driven</li>
<li>Capturing interactions between features</li>
<li>Ensemble trees compete with neural networks in terms of predictions</li>
<li>Useful in data exploration and feature selection</li>
</ol>
<p><strong><em>Disadvantages:</em></strong></p>
<ol style="list-style-type: decimal">
<li>Trees is typically outperformed by more advanced methods, included in <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>.</li>
<li>Trees are not robust and small changes in the data may lead to big changes in the model.</li>
<li>It is easy to overfit the data, hence high variance and low bias. This is dealt with by using the ensemble approach, hereunder, bagging, random forests and boosting.</li>
</ol>
<p>Although decision trees can be advanced with:</p>
<ol style="list-style-type: decimal">
<li>Bagging</li>
<li>Random Forests</li>
<li>Boosting</li>
</ol>
<p>To improve prediction power and therefore compete with other advanced models.</p>
</div>
</div>
<div id="bagging-random-forests-boosting" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Bagging, Random Forests, Boosting</h2>
<p>These are merely building blocks to improve the prediction power.</p>
<div id="bagging-bootstrap-aggregation" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Bagging (Bootstrap Aggregation)</h3>
<p>We want to reduce the variance, and thus the instability of the model, by training the decision tree on different training data. To solve this without requiring a huge amount of data, we introduce bootstrapping. We then have the following procedure:</p>
<ol style="list-style-type: decimal">
<li>Sample from the train data B times</li>
<li>Train the model, making B amount of models, can be written as <span class="math inline">\(\hat{f}^{*b}(x)\)</span>
<em>a. Notice that we didn’t prune the tree, that is intentional.</em></li>
<li>Make predictions</li>
<li>Average the predictions - these are the final predictions. Hence we obtain:</li>
</ol>
<p><span class="math display" id="eq:BaggingTrees">\[\begin{equation}
\hat{y}_{bag}(x) = 1/B*\sum_{b=1}^{B}\hat{f}^{*b}(x)
\tag{3.4}
\end{equation}\]</span></p>
<p>We have therefore created B amount of models, that are each very flexible. But altogether we they create a consensus that acts like the principle estimation of the crowds. Therefore, by including B amount of models we avoid overfitting.</p>
<p>In a classification setting, it is very common to classify based on the majority vote.</p>
<p>Terms:</p>
<ul>
<li>OOB = out-of-bag. These are observations that are not included in the bagging process.</li>
<li>OOB Error Estimation: This is an estimation of the CV error on the test data.</li>
</ul>
</div>
<div id="random-forests" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Random Forests</h3>
<p>Illustrate the scenario of a clear ranking in the importance of the predictor variables, then the bagging method will almost everytime end up with the same trees, perhaps with different cutpoints, but in general, the trees are similar. This countereffects the reduction of variability that we are trying to impose.</p>
<p>With random forests we force the model only to select a subset of the predictors to avoid constructing trees that are very much alike, due to the independent selection. Thus, each time the algorithm considers a split, it is only able to choose among a random sample of m predictors among all of the p predictors. Notice, that we are still bagging the data.</p>
<p>The <strong>rule of thumb</strong> is <span class="math inline">\(p/3\)</span> that is a part of the variables that are randomly considered and <span class="math inline">\(\sqrt{p}\)</span> for classification.</p>
<p>By doings so, we are able to construct more different trees, that yields a far more stable model and also preserves the same prediction power through the bagging and hence averaging the consensus of each tree. Therefore, we don’t have the same risk of overfitting as we have seen previously and e.g., the case with boosting.</p>
</div>
<div id="boosting-i.e.-gradient-boosting" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Boosting (i.e. Gradient Boosting)</h3>
<p><em>Note, boosting is a general approach, that can be applied to many statistical learning methods</em></p>
<p>The whole idea, is that we can construct one model and based on the residuals hereof, we can construct a new tree which is fitted to the residuals on the prior model. Therefore, the boosting method allows for learning. To control the learning we have three different tuning parameters:</p>
<ol style="list-style-type: decimal">
<li><strong>B = the number of trees.</strong> As we constantly improve the model, by fitting to the residuals, we are now able to overfit the training data by iterating too much.</li>
<li><strong><span class="math inline">\(\lambda\)</span> = the shrinkage parameter.</strong> This is a small positive number. It controls at which rate the boosting works. Often the smaller lambda the higher does B need to be.</li>
<li><strong>d = number of d splits (d for depth).</strong> This controls the complexity of the boosted ensemble. <em>Note: often <span class="math inline">\(d = 1\)</span> works very well, this is a stump with only one split</em>. The higher the d, the more interactions between the variables will be reflected. Although, the reason that d = 1 works well, is that each tree accounts for previous trees.</li>
</ol>
<p>See the procedure on <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>, p. 323.</p>
<p><strong>Notice, boosting in classification is a bit different and more complex, the details hereof are omitted in the book</strong></p>
</div>
</div>
<div id="xgboost" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> XGBoost</h2>
<p><span style="color: red;">This is very similar to boosting as in the previous section. It has the following properties: </span></p>
<p><span style="border:2px solid powderblue;
            font-family: courier;
            background-color: green"></p>
<ul>
<li>More efficient than normal boosting</li>
<li>Inspired by Adaboosting and normal gradient boosting</li>
<li>Handels both continious and qualitative variables</li>
<li>Avoids overfitting through regularization</li>
</ul>
<p></span></p>
<p><span class="boxgreen"></p>
<p>The idea and the evolution of xgboost shortly explained:</p>
<ul>
<li>Boosting (sequentially building new classifiers using weights basing on outcomes of the previous ones)</li>
<li>Adaboost (ARCing, improving a predictor using iterative reweighting of incorrectly classified cases) –&gt;</li>
<li>Boosted trees (each model in a chain of predictor corrects mistakes of the previous one) –&gt;</li>
<li>Gradient Boosting (+ greedy optimization with gradient descent) –&gt;</li>
<li>eXtreme Gradient Boosting (+ regularization to prevent overfitting + an efficient implementation)</li>
</ul>
<p></span></p>
<p>The tuning parameters are:</p>
<ul>
<li>Number of Boosting Iterations (nrounds, numeric) - length of chain</li>
<li>Max Tree Depth (max_depth, numeric) - depth of a tree (default: 6, typical: 3-10)</li>
<li>Shrinkage (eta, numeric) - reducing contribution of subsequent models by shrinking the weights (default: 0.3, typical: 0.01-0.2)</li>
<li>Minimum Loss Reduction (gamma, numeric) - spliting nodes only when the split gives positive reduction in the loss function (default: 0)</li>
<li>Subsample Ratio of Columns (colsample_bytree, numeric) - fraction of columns to be sampled randomly for each of the trees (default: 1, typical: 0.5-1)</li>
<li>Minimum Sum of Instance Weight (min_child_weight, numeric) - minimum sum of weight for a child node; protects overfit (default: 1)</li>
<li>Subsample Percentage (subsample, numeric) - fraction of observations to be sampled randomly for each of the trees (default: 1, typical: 0.5-1)</li>
</ul>
</div>
<div id="application-in-r" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Application in R</h2>
<div id="decision-trees" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Decision trees</h3>
<ol style="list-style-type: decimal">
<li>In library <code>tree</code>
<ul>
<li>Use <code>tree(DV~IV,data = ...)</code></li>
</ul></li>
<li>In library <code>rpart</code>
<ul>
<li>Use <code>rpart(DV~IV,daata = ...)</code></li>
<li><em>Note, this has better visualization</em></li>
</ul></li>
<li>Check summary using <code>summary()</code></li>
<li>Plot the tree using <code>plot()</code></li>
<li>Use <code>predict()</code> and evaluate the test error of the unpruned tree</li>
<li>Evaluate optimal level of tree complexity using <code>cv.tree()</code></li>
<li>Prune the tree with <code>prune.misclass()</code> / <code>prune.tree()</code></li>
<li>Use <code>predict()</code> and evaluate the test error of the pruned tree</li>
</ol>
<p><strong>Interpreting summary()</strong></p>
<p>We want to look at:</p>
<ul>
<li>Variables actually used in the tree construction, to see what variables that are in there</li>
<li>Number of terminal nodes: count of the leafs</li>
<li>Residual mean deviance: that is the error</li>
<li>We also have distribution of the errors.</li>
</ul>
<p><strong>Interpreting the cv.tree() object</strong></p>
<p>We see following:
+ Size, amount of leaves
+ Deviance, that is the error. Where we want to find the model with the lowes deciance
+ We can get the alpha by calling <code>$k</code> on the <code>cv.tree()</code> model.</p>
<p><strong>Interpreting the prdict() object</strong></p>
<p>Here we find the predictions.</p>
</div>
<div id="bagging" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Bagging</h3>
<ol style="list-style-type: decimal">
<li>Un library <code>randomFores``     + Use</code>randomForest(DV~IV,mtry = p,ntree = …,importance = TRUE)</li>
<li>Use <code>predict()</code> evalutae the test error</li>
<li>Use <code>ìmportance()</code> to see the importance of each variable</li>
<li>Use <code>varImpPlot()</code> to plot the importance of each variable</li>
</ol>
<p><strong>Interpreteing importance()</strong></p>
<p>We get two coloumns:</p>
<ol style="list-style-type: decimal">
<li>%IncMSE: each of the numbers is the mean decrease in accuracy of prediction (i.e. increase error). Hence if we exclude a specific variable from the model we can read how much the error will increase.</li>
<li>IncNodePurity: This is a measure of total increase of node purity by splitting over the specific variable.</li>
</ol>
<p>Hence both coloumns return information about how the variables impact the performance of the model.</p>
<p>This can also be Plotted</p>
</div>
<div id="random-forests-1" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Random Forests</h3>
<ol style="list-style-type: decimal">
<li>Un library <code>randomForest``     + Use</code>randomForest(DV~IV,mtry = p,ntree = …,importance = TRUE)
<ul>
<li>For RF of regression tree default <em>mtry = p/3</em></li>
<li>FOr RF of classification tree fedault <em>mtry = <span class="math inline">\(\sqrt{p}\)</span></em></li>
</ul></li>
<li>Use <code>ìmportance()</code> to see the importance of each variable</li>
<li>Use <code>varImpPlot()</code> to plot the importance of each variable</li>
</ol>
<p>We see that this is basically the same as bagging, merely where mtry is different, as we define how many variables that should be randomly considered.</p>
<p>Interpretation of the variable importance is the same as in bagging.</p>
</div>
<div id="boosting" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Boosting</h3>
<ol style="list-style-type: decimal">
<li>In library <code>gbm</code>
<ul>
<li>Use <code>gm(DF~IV,shrinkage = ,n.trees = ,interaction.depth = ,)</code></li>
<li>We have to define the distribution, in exercise 8, she used Bernoulli</li>
</ul></li>
<li>Use <code>summary()</code> to see the relative influence of the variables</li>
<li>Use <code>plot()</code> to produce partial dependence plot for each variable</li>
<li>Use <code>predict()</code> to evaluate the test error</li>
</ol>
<p><strong>Interpreting summary()</strong></p>
<p>We get two columns:</p>
<ol style="list-style-type: decimal">
<li>var: that is the variable</li>
<li>Rel. inf: that is the relative influence: We see the highest values are the variables with the highest relationship with the</li>
</ol>
<p>We are also abe to plot the relationship between an X variable and the y, that is done with <code>plot(&lt;boost.model&gt;,i = "the variable")</code></p>
<p><strong>Interpreting predict()</strong></p>
<p>we have to define:</p>
<ul>
<li>Trained boosting model</li>
<li>The test data</li>
<li>The amount of trees, that is with n.trees</li>
<li>If classification, then type = “response.” Thus, one get probabilities.
<ul>
<li>If type is not = “class,” then we get the odds. We are interested in the probability, hence we must use type = “response”</li>
</ul></li>
</ul>
</div>
<div id="xgboost-1" class="section level3" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> XGBoost</h3>
<p>See the XGBoost file.</p>
</div>
</div>
<div id="lab-section-1" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Lab section</h2>
<div id="LabClassification" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Fitting Classification Trees</h3>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="tree-based-methods.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span></code></pre></div>
<p>We want to use the carseats data, where sales = Unit sales (in thousands) at each location. We want to predict whether they sold more or less than 8.000, i.e. 8 as the variable is encoded in thousands.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="tree-based-methods.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb217-2"><a href="tree-based-methods.html#cb217-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Carseats</span>
<span id="cb217-3"><a href="tree-based-methods.html#cb217-3" aria-hidden="true" tabindex="-1"></a>High <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(df<span class="sc">$</span>Sales <span class="sc">&lt;=</span> <span class="dv">8</span>,<span class="st">&quot;No&quot;</span>,<span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<p>Merging the vector and the data set.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="tree-based-methods.html#cb218-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Carseats,High)</span>
<span id="cb218-2"><a href="tree-based-methods.html#cb218-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>High <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(df<span class="sc">$</span>High)</span></code></pre></div>
<p>We now want to predict the High variable.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="tree-based-methods.html#cb219-1" aria-hidden="true" tabindex="-1"></a>tree.carseats <span class="ot">&lt;-</span> <span class="fu">tree</span>(<span class="at">formula =</span> High <span class="sc">~</span> . <span class="sc">-</span> Sales</span>
<span id="cb219-2"><a href="tree-based-methods.html#cb219-2" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">data =</span> df)</span>
<span id="cb219-3"><a href="tree-based-methods.html#cb219-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree.carseats)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = High ~ . - Sales, data = df)
## Variables actually used in tree construction:
## [1] &quot;ShelveLoc&quot;   &quot;Price&quot;       &quot;Income&quot;      &quot;CompPrice&quot;   &quot;Population&quot; 
## [6] &quot;Advertising&quot; &quot;Age&quot;         &quot;US&quot;         
## Number of terminal nodes:  27 
## Residual mean deviance:  0.4575 = 170.7 / 373 
## Misclassification error rate: 0.09 = 36 / 400</code></pre>
<ul>
<li>We see the variables that are included</li>
<li>We see that there are 27 terminal nodes</li>
<li>Information about the error (deviance) and the missclassification rate = 9%</li>
</ul>
<p>The deviance reported is from the calculation:</p>
<p><span class="math display" id="eq:deviance">\[\begin{equation}
-2\sum_m\sum_kn_{mk}log\hat{p}_{mk}
\tag{3.5}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(n_{mk}\)</span> = the number of observations in the <em>m</em>th terminal node, that belong to the <em>k</em>th class.</li>
</ul>
<p>A small deviance = low error, hence a good fit to the train data.</p>
<p><em>Thus similar to the RSS calculation that we have seen earlier.</em></p>
<p>we find the residual mean deviance to be:</p>
<p><span class="math display" id="eq:ResidualMeanDeviance">\[\begin{equation}
\frac{Deviance}{n-|T_0|}
\tag{3.6}
\end{equation}\]</span></p>
<p>Hence</p>
<p><span class="math display">\[\frac{170.7}{(400-27)}=373\]</span></p>
<p>Where <span class="math inline">\(T_0\)</span> is the unpruned tree, which we see is with 27 terminal nodes. And <span class="math inline">\(n\)</span> is merely the amount of observations.</p>
<p><strong>PLotting the tree:</strong></p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="tree-based-methods.html#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tree.carseats)</span>
<span id="cb221-2"><a href="tree-based-methods.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(tree.carseats</span>
<span id="cb221-3"><a href="tree-based-methods.html#cb221-3" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-150-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the tree. Notice on the right side, we see that price occurs twice. This is probably due to purity and more certainty on the predictions.</p>
<p>We see that Shelve Location Bad and Medium appear to be the best predictor. As we start out with that variable.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="tree-based-methods.html#cb222-1" aria-hidden="true" tabindex="-1"></a>tree.carseats[<span class="dv">1</span>] <span class="co">#Added [1] to get it in a table</span></span></code></pre></div>
<pre><code>## $frame
##             var   n        dev yval splits.cutleft splits.cutright   yprob.No
## 1     ShelveLoc 400 541.486837   No            :ac              :b 0.59000000
## 2         Price 315 390.591685   No          &lt;92.5           &gt;92.5 0.68888889
## 4        Income  46  56.534305  Yes            &lt;57             &gt;57 0.30434783
## 8     CompPrice  10  12.217286   No         &lt;110.5          &gt;110.5 0.70000000
## 16       &lt;leaf&gt;   5   0.000000   No                                1.00000000
## 17       &lt;leaf&gt;   5   6.730117  Yes                                0.40000000
## 9    Population  36  35.467463  Yes         &lt;207.5          &gt;207.5 0.19444444
## 18       &lt;leaf&gt;  16  21.170024  Yes                                0.37500000
## 19       &lt;leaf&gt;  20   7.940610  Yes                                0.05000000
## 5   Advertising 269 299.758669   No          &lt;13.5           &gt;13.5 0.75464684
## 10    CompPrice 224 213.232590   No         &lt;124.5          &gt;124.5 0.81696429
## 20        Price  96  44.887998   No         &lt;106.5          &gt;106.5 0.93750000
## 40   Population  38  33.148337   No           &lt;177            &gt;177 0.84210526
## 80       Income  12  16.300638   No          &lt;60.5           &gt;60.5 0.58333333
## 160      &lt;leaf&gt;   6   0.000000   No                                1.00000000
## 161      &lt;leaf&gt;   6   5.406735  Yes                                0.16666667
## 81       &lt;leaf&gt;  26   8.477229   No                                0.96153846
## 41       &lt;leaf&gt;  58   0.000000   No                                1.00000000
## 21        Price 128 150.181878   No         &lt;122.5          &gt;122.5 0.72656250
## 42    ShelveLoc  51  70.681403  Yes             :a              :c 0.49019608
## 84       &lt;leaf&gt;  11   6.701994   No                                0.90909091
## 85        Price  40  52.925059  Yes         &lt;109.5          &gt;109.5 0.37500000
## 170      &lt;leaf&gt;  16   7.481333  Yes                                0.06250000
## 171         Age  24  32.601277   No          &lt;49.5           &gt;49.5 0.58333333
## 342      &lt;leaf&gt;  13  16.048286  Yes                                0.30769231
## 343      &lt;leaf&gt;  11   6.701994   No                                0.90909091
## 43    CompPrice  77  55.542945   No         &lt;147.5          &gt;147.5 0.88311688
## 86       &lt;leaf&gt;  58  17.399411   No                                0.96551724
## 87        Price  19  25.008180   No           &lt;147            &gt;147 0.63157895
## 174   CompPrice  12  16.300638  Yes         &lt;152.5          &gt;152.5 0.41666667
## 348      &lt;leaf&gt;   7   5.741628  Yes                                0.14285714
## 349      &lt;leaf&gt;   5   5.004024   No                                0.80000000
## 175      &lt;leaf&gt;   7   0.000000   No                                1.00000000
## 11          Age  45  61.826542  Yes          &lt;54.5           &gt;54.5 0.44444444
## 22    CompPrice  25  25.020121  Yes         &lt;130.5          &gt;130.5 0.20000000
## 44       Income  14  18.249184  Yes           &lt;100            &gt;100 0.35714286
## 88       &lt;leaf&gt;   9  12.365308   No                                0.55555556
## 89       &lt;leaf&gt;   5   0.000000  Yes                                0.00000000
## 45       &lt;leaf&gt;  11   0.000000  Yes                                0.00000000
## 23    CompPrice  20  22.493406   No         &lt;122.5          &gt;122.5 0.75000000
## 46       &lt;leaf&gt;  10   0.000000   No                                1.00000000
## 47        Price  10  13.862944   No           &lt;125            &gt;125 0.50000000
## 94       &lt;leaf&gt;   5   0.000000  Yes                                0.00000000
## 95       &lt;leaf&gt;   5   0.000000   No                                1.00000000
## 3         Price  85  90.327606  Yes           &lt;135            &gt;135 0.22352941
## 6            US  68  49.260636  Yes             :a              :b 0.11764706
## 12        Price  17  22.074444  Yes           &lt;109            &gt;109 0.35294118
## 24       &lt;leaf&gt;   8   0.000000  Yes                                0.00000000
## 25       &lt;leaf&gt;   9  11.457255   No                                0.66666667
## 13       &lt;leaf&gt;  51  16.875237  Yes                                0.03921569
## 7        Income  17  22.074444   No            &lt;46             &gt;46 0.64705882
## 14       &lt;leaf&gt;   6   0.000000   No                                1.00000000
## 15       &lt;leaf&gt;  11  15.158203  Yes                                0.45454545
##      yprob.Yes
## 1   0.41000000
## 2   0.31111111
## 4   0.69565217
## 8   0.30000000
## 16  0.00000000
## 17  0.60000000
## 9   0.80555556
## 18  0.62500000
## 19  0.95000000
## 5   0.24535316
## 10  0.18303571
## 20  0.06250000
## 40  0.15789474
## 80  0.41666667
## 160 0.00000000
## 161 0.83333333
## 81  0.03846154
## 41  0.00000000
## 21  0.27343750
## 42  0.50980392
## 84  0.09090909
## 85  0.62500000
## 170 0.93750000
## 171 0.41666667
## 342 0.69230769
## 343 0.09090909
## 43  0.11688312
## 86  0.03448276
## 87  0.36842105
## 174 0.58333333
## 348 0.85714286
## 349 0.20000000
## 175 0.00000000
## 11  0.55555556
## 22  0.80000000
## 44  0.64285714
## 88  0.44444444
## 89  1.00000000
## 45  1.00000000
## 23  0.25000000
## 46  0.00000000
## 47  0.50000000
## 94  1.00000000
## 95  0.00000000
## 3   0.77647059
## 6   0.88235294
## 12  0.64705882
## 24  1.00000000
## 25  0.33333333
## 13  0.96078431
## 7   0.35294118
## 14  0.00000000
## 15  0.54545455</code></pre>
<p>We are able to see the following from the print:</p>
<ol style="list-style-type: decimal">
<li>Variable</li>
<li>Criterion</li>
<li>Observations in the criterion, i.e. branch</li>
<li>The deviance</li>
<li>The overall prediction for the branch</li>
<li>The fraction of observations in that branch that take on values 1 and 0 (Y/N).</li>
</ol>
<p>We also see that branches that leads to a terminal node is marked with an "*".</p>
<p><em>Notice, that the ShelveLoc Says a and c for cut left. That is because the categories are not ordered. We can inspect the order with contrasts()</em></p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="tree-based-methods.html#cb224-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(df<span class="sc">$</span>ShelveLoc)</span></code></pre></div>
<pre><code>##        Good Medium
## Bad       0      0
## Good      1      0
## Medium    0      1</code></pre>
<p><em>Where it is ranked, bad, good, medium. Perhaps it could have been ordered correctly. But it does not matter when we are working with a decision tree, as it is purely data driven.</em></p>
<p><strong>Using test and train data</strong></p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="tree-based-methods.html#cb226-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb226-2"><a href="tree-based-methods.html#cb226-2" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df),<span class="at">size =</span> <span class="dv">200</span>)</span>
<span id="cb226-3"><a href="tree-based-methods.html#cb226-3" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[index,]</span>
<span id="cb226-4"><a href="tree-based-methods.html#cb226-4" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>index,]</span>
<span id="cb226-5"><a href="tree-based-methods.html#cb226-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-6"><a href="tree-based-methods.html#cb226-6" aria-hidden="true" tabindex="-1"></a>tree.carseats <span class="ot">&lt;-</span> <span class="fu">tree</span>(High <span class="sc">~</span> . <span class="sc">-</span> Sales,<span class="at">data =</span> df.train)</span>
<span id="cb226-7"><a href="tree-based-methods.html#cb226-7" aria-hidden="true" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree.carseats,<span class="at">newdata =</span> df.test,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb226-8"><a href="tree-based-methods.html#cb226-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-9"><a href="tree-based-methods.html#cb226-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb226-10"><a href="tree-based-methods.html#cb226-10" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> tree.pred,<span class="at">reference =</span> df.test<span class="sc">$</span>High,<span class="at">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  No Yes
##        No  104  33
##        Yes  13  50
##                                           
##                Accuracy : 0.77            
##                  95% CI : (0.7054, 0.8264)
##     No Information Rate : 0.585           
##     P-Value [Acc &gt; NIR] : 0.00000002938   
##                                           
##                   Kappa : 0.5091          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.005088        
##                                           
##             Sensitivity : 0.6024          
##             Specificity : 0.8889          
##          Pos Pred Value : 0.7937          
##          Neg Pred Value : 0.7591          
##              Prevalence : 0.4150          
##          Detection Rate : 0.2500          
##    Detection Prevalence : 0.3150          
##       Balanced Accuracy : 0.7456          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>We see a sensitivity of 60% and specificity of 88%</p>
<p><strong>Pruning the tree</strong></p>
<p>It is likely that we are able to improve the model by pruning the tree. That is performed in the following.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="tree-based-methods.html#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb228-2"><a href="tree-based-methods.html#cb228-2" aria-hidden="true" tabindex="-1"></a>cv.carseats <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(<span class="at">object =</span> tree.carseats</span>
<span id="cb228-3"><a href="tree-based-methods.html#cb228-3" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">FUN =</span> prune.misclass) <span class="co"># we are pruning to optimize against misclassifications</span></span>
<span id="cb228-4"><a href="tree-based-methods.html#cb228-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb228-5"><a href="tree-based-methods.html#cb228-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Listing objects in the cv.carseats</span></span>
<span id="cb228-6"><a href="tree-based-methods.html#cb228-6" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(cv.carseats)</span></code></pre></div>
<pre><code>## [1] &quot;size&quot;   &quot;dev&quot;    &quot;k&quot;      &quot;method&quot;</code></pre>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="tree-based-methods.html#cb230-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-2"><a href="tree-based-methods.html#cb230-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Retrieving informaiton in the object</span></span>
<span id="cb230-3"><a href="tree-based-methods.html#cb230-3" aria-hidden="true" tabindex="-1"></a>cv.carseats</span></code></pre></div>
<pre><code>## $size
## [1] 21 19 14  9  8  5  3  2  1
## 
## $dev
## [1] 69 70 69 67 68 71 72 81 81
## 
## $k
## [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<p><strong>Size:</strong>
This is the size of the subtrees created</p>
<p><strong>dev:</strong>
The cross-validation error rate in this instance. We want to select the tree that yields the lowest error rate, that appear to be tree no. 4 with 9 terminal nodes.</p>
<p><strong>k:</strong>
Recall that we prune trees based on the tuning parameter <span class="math inline">\(\alpha\)</span>, see equation <a href="tree-based-methods.html#eq:Subtree">(3.3)</a>. This is corresponding to k.</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="tree-based-methods.html#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb232-2"><a href="tree-based-methods.html#cb232-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> cv.carseats<span class="sc">$</span>size,<span class="at">y =</span> cv.carseats<span class="sc">$</span>dev,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb232-3"><a href="tree-based-methods.html#cb232-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> cv.carseats<span class="sc">$</span>k,<span class="at">y =</span> cv.carseats<span class="sc">$</span>dev,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-155-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We new apply the <code>prune.misclass()</code> function in order to prune the tree to obtain the nine-node tree.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="tree-based-methods.html#cb233-1" aria-hidden="true" tabindex="-1"></a>prune.carseats <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(tree.carseats</span>
<span id="cb233-2"><a href="tree-based-methods.html#cb233-2" aria-hidden="true" tabindex="-1"></a>                                 ,<span class="at">best =</span> <span class="dv">9</span>) <span class="co">#We want to inspect a tree of size 9</span></span>
<span id="cb233-3"><a href="tree-based-methods.html#cb233-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb233-4"><a href="tree-based-methods.html#cb233-4" aria-hidden="true" tabindex="-1"></a>{<span class="fu">plot</span>(prune.carseats)</span>
<span id="cb233-5"><a href="tree-based-methods.html#cb233-5" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(prune.carseats,<span class="at">pretty =</span> <span class="dv">0</span>)}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-156-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="tree-based-methods.html#cb234-1" aria-hidden="true" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> prune.carseats,<span class="at">newdata =</span> df.test,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb234-2"><a href="tree-based-methods.html#cb234-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(tree.pred,<span class="at">reference =</span> df.test<span class="sc">$</span>High,<span class="at">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  97  25
##        Yes 20  58
##                                           
##                Accuracy : 0.775           
##                  95% CI : (0.7108, 0.8309)
##     No Information Rate : 0.585           
##     P-Value [Acc &gt; NIR] : 0.00000001206   
##                                           
##                   Kappa : 0.5325          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.551           
##                                           
##             Sensitivity : 0.6988          
##             Specificity : 0.8291          
##          Pos Pred Value : 0.7436          
##          Neg Pred Value : 0.7951          
##              Prevalence : 0.4150          
##          Detection Rate : 0.2900          
##    Detection Prevalence : 0.3900          
##       Balanced Accuracy : 0.7639          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>We see that we have both improved the sensitivity to 69.9% and the specificity to 82.9% from respectively 60% and 88%.</p>
<p>The overall accuracy is 77.5%.</p>
<p>We can show the accuracy of other trees as well. But these will perform worse.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="tree-based-methods.html#cb236-1" aria-hidden="true" tabindex="-1"></a>prune.carseats <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(tree.carseats,<span class="at">best =</span> <span class="dv">15</span>)</span>
<span id="cb236-2"><a href="tree-based-methods.html#cb236-2" aria-hidden="true" tabindex="-1"></a>{<span class="fu">plot</span>(prune.carseats)</span>
<span id="cb236-3"><a href="tree-based-methods.html#cb236-3" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(prune.carseats,<span class="at">pretty =</span> <span class="dv">0</span>)}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-158-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="tree-based-methods.html#cb237-1" aria-hidden="true" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> prune.carseats,<span class="at">newdata =</span> df.test,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb237-2"><a href="tree-based-methods.html#cb237-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Confusion matrix</span></span>
<span id="cb237-3"><a href="tree-based-methods.html#cb237-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> tree.pred,<span class="at">reference =</span> df.test<span class="sc">$</span>High,<span class="at">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  No Yes
##        No  102  30
##        Yes  15  53
##                                           
##                Accuracy : 0.775           
##                  95% CI : (0.7108, 0.8309)
##     No Information Rate : 0.585           
##     P-Value [Acc &gt; NIR] : 0.00000001206   
##                                           
##                   Kappa : 0.5241          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.03689         
##                                           
##             Sensitivity : 0.6386          
##             Specificity : 0.8718          
##          Pos Pred Value : 0.7794          
##          Neg Pred Value : 0.7727          
##              Prevalence : 0.4150          
##          Detection Rate : 0.2650          
##    Detection Prevalence : 0.3400          
##       Balanced Accuracy : 0.7552          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>We see that the overall accuracy is actually the same, but the sensitivity and specificity is worse.</p>
<p><br />
</p>
</div>
<div id="fitting-regression-trees" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Fitting Regression Trees</h3>
<p>We use the Boston data set.</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="tree-based-methods.html#cb239-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb239-2"><a href="tree-based-methods.html#cb239-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb239-3"><a href="tree-based-methods.html#cb239-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Boston</span>
<span id="cb239-4"><a href="tree-based-methods.html#cb239-4" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">nrow</span>(df),<span class="at">size =</span> <span class="fu">nrow</span>(df)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb239-5"><a href="tree-based-methods.html#cb239-5" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[index,]</span>
<span id="cb239-6"><a href="tree-based-methods.html#cb239-6" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>index,]</span>
<span id="cb239-7"><a href="tree-based-methods.html#cb239-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb239-8"><a href="tree-based-methods.html#cb239-8" aria-hidden="true" tabindex="-1"></a>tree.boston <span class="ot">&lt;-</span> <span class="fu">tree</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train)</span>
<span id="cb239-9"><a href="tree-based-methods.html#cb239-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree.boston)</span></code></pre></div>
<pre><code>## 
## Regression tree:
## tree(formula = medv ~ ., data = df.train)
## Variables actually used in tree construction:
## [1] &quot;rm&quot;    &quot;lstat&quot; &quot;crim&quot;  &quot;age&quot;  
## Number of terminal nodes:  7 
## Residual mean deviance:  10.38 = 2555 / 246 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800</code></pre>
<p>For the interpretation hereof, i refer to section <a href="tree-based-methods.html#LabClassification">3.5.1</a>. But notice, that we are now wirking with a continous response variable, hence deviance = RSS, also found equation <a href="tree-based-methods.html#eq:DTRSS">(3.1)</a>.</p>
<p>We can now plot the tree.</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="tree-based-methods.html#cb241-1" aria-hidden="true" tabindex="-1"></a>{<span class="fu">plot</span>(tree.boston)</span>
<span id="cb241-2"><a href="tree-based-methods.html#cb241-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(tree.boston,<span class="at">pretty =</span> <span class="dv">0</span>)}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-161-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Recall that the Y variable is median value of owner-occupied homes in $1000s. We see that the predicted median house price is 45,380 when the amount of rooms are over 7.5.</p>
<p>Let us now use cross validation to see whether pruning the tree will improve the model.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="tree-based-methods.html#cb242-1" aria-hidden="true" tabindex="-1"></a>cv.boston <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(tree.boston)</span>
<span id="cb242-2"><a href="tree-based-methods.html#cb242-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> cv.boston<span class="sc">$</span>size,<span class="at">y =</span> cv.boston<span class="sc">$</span>dev,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-162-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that as the tree size increases. This in this case it does not appear to improve the model with pruning the tree.</p>
<p>However, let us say, that we know that we want to prune the tree. Then we can do the following:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="tree-based-methods.html#cb243-1" aria-hidden="true" tabindex="-1"></a>prune.boston <span class="ot">&lt;-</span> <span class="fu">prune.tree</span>(<span class="at">tree =</span> tree.boston,<span class="at">best =</span> <span class="dv">5</span>) <span class="co">#5 terminal nodes</span></span>
<span id="cb243-2"><a href="tree-based-methods.html#cb243-2" aria-hidden="true" tabindex="-1"></a>{<span class="fu">plot</span>(prune.boston)</span>
<span id="cb243-3"><a href="tree-based-methods.html#cb243-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(prune.boston,<span class="at">pretty =</span> <span class="dv">0</span>)}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-163-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="tree-based-methods.html#cb244-1" aria-hidden="true" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> tree.boston,<span class="at">newdata =</span> df.train)</span>
<span id="cb244-2"><a href="tree-based-methods.html#cb244-2" aria-hidden="true" tabindex="-1"></a>{<span class="fu">plot</span>(tree.pred,df.test<span class="sc">$</span>medv)</span>
<span id="cb244-3"><a href="tree-based-methods.html#cb244-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>,<span class="at">b =</span> <span class="dv">1</span>)}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-164-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="tree-based-methods.html#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((tree.pred <span class="sc">-</span> df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 153.5446</code></pre>
<p>We see an MSE of 153.5.</p>
</div>
<div id="bagging-and-random-forests" class="section level3" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Bagging and Random Forests</h3>
<p>Notice that bagging and random forests are basically the same. Where random forests merely tweak the bagging trees with a random selection of IV, instead of the model always being able to select all models. Therefore, it can also be said that <span class="math inline">\(m = p\)</span>, and thus the same function can be applied to both bagging and random forests.</p>
<div id="bagging-1" class="section level4" number="3.5.3.1">
<h4><span class="header-section-number">3.5.3.1</span> Bagging</h4>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="tree-based-methods.html#cb247-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb247-2"><a href="tree-based-methods.html#cb247-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Boston</span>
<span id="cb247-3"><a href="tree-based-methods.html#cb247-3" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">nrow</span>(df),<span class="at">size =</span> <span class="fu">nrow</span>(df)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb247-4"><a href="tree-based-methods.html#cb247-4" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[index,]</span>
<span id="cb247-5"><a href="tree-based-methods.html#cb247-5" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>index,]</span></code></pre></div>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="tree-based-methods.html#cb248-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb248-2"><a href="tree-based-methods.html#cb248-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb248-3"><a href="tree-based-methods.html#cb248-3" aria-hidden="true" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">mtry =</span> <span class="dv">13</span>,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb248-4"><a href="tree-based-methods.html#cb248-4" aria-hidden="true" tabindex="-1"></a>bag.boston</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = df.train, mtry = 13,      importance = TRUE) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 13
## 
##           Mean of squared residuals: 14.45243
##                     % Var explained: 81.83</code></pre>
<p>We see that we have constructed 500 trees and tried 13 (all) variables at each split.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="tree-based-methods.html#cb250-1" aria-hidden="true" tabindex="-1"></a>bag.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> bag.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb250-2"><a href="tree-based-methods.html#cb250-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> bag.pred,<span class="at">y =</span> df.test<span class="sc">$</span>medv)</span>
<span id="cb250-3"><a href="tree-based-methods.html#cb250-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-169-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="tree-based-methods.html#cb251-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((bag.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span></code></pre></div>
<pre><code>## [1] 19.81565</code></pre>
<p>We see that the MSE is 19.8 which is an immense improvement.</p>
<p>Notice, that we also have specified how many trees we want to grow:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="tree-based-methods.html#cb253-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb253-2"><a href="tree-based-methods.html#cb253-2" aria-hidden="true" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">mtry =</span> <span class="dv">13</span>,<span class="at">ntrees =</span> <span class="dv">25</span>,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb253-3"><a href="tree-based-methods.html#cb253-3" aria-hidden="true" tabindex="-1"></a>bag.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> bag.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb253-4"><a href="tree-based-methods.html#cb253-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((bag.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span></code></pre></div>
<pre><code>## [1] 19.81565</code></pre>
<p>We see that the MSE is a bit higher than before. That is also intuitively correct, although with bagging we are actually also at risk of overfitting the data by growing too many trees, as it is likely that the trees are similar. To overcome this risk, we can grow a random forest instead, that is done in the following.</p>
</div>
<div id="random-forest" class="section level4" number="3.5.3.2">
<h4><span class="header-section-number">3.5.3.2</span> Random Forest</h4>
<p>We use the same procedure except that we use fewer variables under evaluation. By default R follows a rule of thumb with <span class="math inline">\(\frac{p}{3}\)</span> for regression and <span class="math inline">\(\sqrt{p}\)</span> for classification. In this example, we use 6 variables.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="tree-based-methods.html#cb255-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb255-2"><a href="tree-based-methods.html#cb255-2" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">mtry =</span> <span class="dv">6</span>,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb255-3"><a href="tree-based-methods.html#cb255-3" aria-hidden="true" tabindex="-1"></a>rf.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> rf.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb255-4"><a href="tree-based-methods.html#cb255-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((rf.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span></code></pre></div>
<pre><code>## [1] 16.4081</code></pre>
<p>Now we see that the model is even better, with MSE of 16.4.</p>
<p>We can use <code>importance()</code> to interprete the variables.</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="tree-based-methods.html#cb257-1" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf.boston)</span></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## crim    13.732304    1107.67668
## zn       4.410510      95.63934
## indus    5.670870     443.23264
## chas     1.353168      39.59940
## nox     16.662050     923.03154
## rm      33.368987    7865.44385
## age     13.600352     626.64334
## dis     11.469812     765.61445
## rad      4.417249     108.77438
## tax      9.424190     550.22440
## ptratio 13.150528    1353.27803
## black    7.213603     310.30963
## lstat   25.921218    5666.64169</code></pre>
<p>We see that rm (rooms pr. dwelling) has the highest influence on the model performance, and we see by removing this variable, the error increase by 32.3.</p>
<p>We also see that rm leads to the highest purity impact.</p>
<p><em>Notice, that these metrics are based on the train data.</em></p>
<p>The data above can also be plotted with.</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="tree-based-methods.html#cb259-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-173-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Where we see that the varaibles are ranked according to the metrics.</p>
</div>
</div>
<div id="boosting-1" class="section level3" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Boosting</h3>
<p>Recall that this model initially fits a tree to the data and then afterwards fits consecutive models to the residuals of that model. This is why it is also called gradient boosting.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="tree-based-methods.html#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb260-2"><a href="tree-based-methods.html#cb260-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb260-3"><a href="tree-based-methods.html#cb260-3" aria-hidden="true" tabindex="-1"></a>boost.boston <span class="ot">&lt;-</span> <span class="fu">gbm</span>(medv <span class="sc">~</span> .</span>
<span id="cb260-4"><a href="tree-based-methods.html#cb260-4" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">data =</span> df.train</span>
<span id="cb260-5"><a href="tree-based-methods.html#cb260-5" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span> <span class="co">#Since it is regression</span></span>
<span id="cb260-6"><a href="tree-based-methods.html#cb260-6" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">n.trees =</span> <span class="dv">5000</span></span>
<span id="cb260-7"><a href="tree-based-methods.html#cb260-7" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">interaction.depth =</span> <span class="dv">4</span></span>
<span id="cb260-8"><a href="tree-based-methods.html#cb260-8" aria-hidden="true" tabindex="-1"></a>                    ) <span class="co">#Lambda is by default = 0.001</span></span>
<span id="cb260-9"><a href="tree-based-methods.html#cb260-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boost.boston)</span></code></pre></div>
<img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-174-1.png" width="720" style="display: block; margin: auto;" />
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">var</th>
<th align="right">rel.inf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">rm</td>
<td align="left">rm</td>
<td align="right">35.8641661</td>
</tr>
<tr class="even">
<td align="left">lstat</td>
<td align="left">lstat</td>
<td align="right">33.1527531</td>
</tr>
<tr class="odd">
<td align="left">dis</td>
<td align="left">dis</td>
<td align="right">5.9907969</td>
</tr>
<tr class="even">
<td align="left">nox</td>
<td align="left">nox</td>
<td align="right">4.8861341</td>
</tr>
<tr class="odd">
<td align="left">crim</td>
<td align="left">crim</td>
<td align="right">4.5736239</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="left">age</td>
<td align="right">4.2570248</td>
</tr>
<tr class="odd">
<td align="left">ptratio</td>
<td align="left">ptratio</td>
<td align="right">4.2345152</td>
</tr>
<tr class="even">
<td align="left">black</td>
<td align="left">black</td>
<td align="right">3.5107293</td>
</tr>
<tr class="odd">
<td align="left">tax</td>
<td align="left">tax</td>
<td align="right">1.7363601</td>
</tr>
<tr class="even">
<td align="left">rad</td>
<td align="left">rad</td>
<td align="right">0.8599312</td>
</tr>
<tr class="odd">
<td align="left">indus</td>
<td align="left">indus</td>
<td align="right">0.7808803</td>
</tr>
<tr class="even">
<td align="left">chas</td>
<td align="left">chas</td>
<td align="right">0.0800761</td>
</tr>
<tr class="odd">
<td align="left">zn</td>
<td align="left">zn</td>
<td align="right">0.0730089</td>
</tr>
</tbody>
</table>
</div>
<p>We see that the model ranks relative influence, which is also plotted. Again we see that rm is the most important variable. lstat is also noticable.</p>
<p>We can plot <em>partial dependence plots</em> for these variables.</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="tree-based-methods.html#cb261-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i.var =</span> <span class="st">&quot;rm&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-175-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="tree-based-methods.html#cb262-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i.var =</span> <span class="st">&quot;lstat&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-175-2.png" width="720" style="display: block; margin: auto;" /></p>
<p>We can now predict medv for the test data.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="tree-based-methods.html#cb263-1" aria-hidden="true" tabindex="-1"></a>boost.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> boost.boston</span>
<span id="cb263-2"><a href="tree-based-methods.html#cb263-2" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">newdata =</span> df.test</span>
<span id="cb263-3"><a href="tree-based-methods.html#cb263-3" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">n.trees =</span> <span class="dv">5000</span> <span class="co">#Notice that we specify amount of trees</span></span>
<span id="cb263-4"><a href="tree-based-methods.html#cb263-4" aria-hidden="true" tabindex="-1"></a>                      )</span>
<span id="cb263-5"><a href="tree-based-methods.html#cb263-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((boost.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) </span></code></pre></div>
<pre><code>## [1] 14.63457</code></pre>
<p>We see that the MSE is lowered further by 14.6.</p>
<p>We can also fit a tree that has another tuning parameter, hence it is learning faster.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="tree-based-methods.html#cb265-1" aria-hidden="true" tabindex="-1"></a>boost.boston <span class="ot">&lt;-</span> <span class="fu">gbm</span>(medv <span class="sc">~</span> .</span>
<span id="cb265-2"><a href="tree-based-methods.html#cb265-2" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">data =</span> df.train</span>
<span id="cb265-3"><a href="tree-based-methods.html#cb265-3" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span></span>
<span id="cb265-4"><a href="tree-based-methods.html#cb265-4" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">n.trees =</span> <span class="dv">5000</span></span>
<span id="cb265-5"><a href="tree-based-methods.html#cb265-5" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">interaction.depth =</span> <span class="dv">4</span></span>
<span id="cb265-6"><a href="tree-based-methods.html#cb265-6" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">shrinkage =</span> <span class="fl">0.2</span> <span class="co">#Lambda, tuning parameter</span></span>
<span id="cb265-7"><a href="tree-based-methods.html#cb265-7" aria-hidden="true" tabindex="-1"></a>                    ,<span class="at">verbose =</span> <span class="cn">FALSE</span> <span class="co">#If = T, then it plots the progress</span></span>
<span id="cb265-8"><a href="tree-based-methods.html#cb265-8" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb265-9"><a href="tree-based-methods.html#cb265-9" aria-hidden="true" tabindex="-1"></a>boost.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost.boston</span>
<span id="cb265-10"><a href="tree-based-methods.html#cb265-10" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">newdata =</span> df.test</span>
<span id="cb265-11"><a href="tree-based-methods.html#cb265-11" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb265-12"><a href="tree-based-methods.html#cb265-12" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((boost.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 17.22542</code></pre>
<p>We see that the MSE is 17.22, with lambda = standard value of 0.001, hence it is not learning as fast.</p>
</div>
</div>
<div id="exercies" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Exercies</h2>
<div id="exercise-7---decision-tree-assessment" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Exercise 7 - Decision Tree Assessment</h3>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="tree-based-methods.html#cb267-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb267-2"><a href="tree-based-methods.html#cb267-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb267-3"><a href="tree-based-methods.html#cb267-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb267-4"><a href="tree-based-methods.html#cb267-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb267-5"><a href="tree-based-methods.html#cb267-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb267-6"><a href="tree-based-methods.html#cb267-6" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Boston</span>
<span id="cb267-7"><a href="tree-based-methods.html#cb267-7" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">nrow</span>(df),<span class="at">size =</span> <span class="fu">nrow</span>(df)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb267-8"><a href="tree-based-methods.html#cb267-8" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[index,]</span>
<span id="cb267-9"><a href="tree-based-methods.html#cb267-9" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>index,]</span>
<span id="cb267-10"><a href="tree-based-methods.html#cb267-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb267-11"><a href="tree-based-methods.html#cb267-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb267-12"><a href="tree-based-methods.html#cb267-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterating number of trees</span></span>
<span id="cb267-13"><a href="tree-based-methods.html#cb267-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb267-14"><a href="tree-based-methods.html#cb267-14" aria-hidden="true" tabindex="-1"></a>MSE.in <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)))</span>
<span id="cb267-15"><a href="tree-based-methods.html#cb267-15" aria-hidden="true" tabindex="-1"></a>MSE.out <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)))</span>
<span id="cb267-16"><a href="tree-based-methods.html#cb267-16" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb267-17"><a href="tree-based-methods.html#cb267-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (B <span class="cf">in</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)) {</span>
<span id="cb267-18"><a href="tree-based-methods.html#cb267-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(B)</span>
<span id="cb267-19"><a href="tree-based-methods.html#cb267-19" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb267-20"><a href="tree-based-methods.html#cb267-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb267-21"><a href="tree-based-methods.html#cb267-21" aria-hidden="true" tabindex="-1"></a>  rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">ntree =</span> B,<span class="at">mtry =</span> <span class="dv">6</span>,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb267-22"><a href="tree-based-methods.html#cb267-22" aria-hidden="true" tabindex="-1"></a>  rf.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> rf.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb267-23"><a href="tree-based-methods.html#cb267-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb267-24"><a href="tree-based-methods.html#cb267-24" aria-hidden="true" tabindex="-1"></a>  MSE.in[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((rf.boston<span class="sc">$</span>predicted<span class="sc">-</span>df.train<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span>
<span id="cb267-25"><a href="tree-based-methods.html#cb267-25" aria-hidden="true" tabindex="-1"></a>  MSE.out[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((rf.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span>
<span id="cb267-26"><a href="tree-based-methods.html#cb267-26" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="tree-based-methods.html#cb268-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(MSE.out,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&#39;darkred&#39;</span>,<span class="at">xlab =</span> <span class="st">&quot;Number of trees&quot;</span>,<span class="at">ylab =</span> <span class="st">&quot;MSE&quot;</span>)</span>
<span id="cb268-2"><a href="tree-based-methods.html#cb268-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(rf.boston<span class="sc">$</span>mse,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&#39;limegreen&#39;</span>)</span>
<span id="cb268-3"><a href="tree-based-methods.html#cb268-3" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb268-4"><a href="tree-based-methods.html#cb268-4" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Out&quot;</span>,<span class="st">&quot;In&quot;</span>),<span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;darkred&#39;</span>,<span class="st">&quot;limegreen&quot;</span>))</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-179-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>NOTICE; due to the OOB, it is not rare to see in sample error that is greater than the out of sample error.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="tree-based-methods.html#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb269-2"><a href="tree-based-methods.html#cb269-2" aria-hidden="true" tabindex="-1"></a>CoreCount  <span class="ot">&lt;-</span> <span class="fu">makePSOCKcluster</span>(<span class="fu">detectCores</span>()<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb269-3"><a href="tree-based-methods.html#cb269-3" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(CoreCount)</span>
<span id="cb269-4"><a href="tree-based-methods.html#cb269-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb269-5"><a href="tree-based-methods.html#cb269-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterating number of trees and parameters under evaluation</span></span>
<span id="cb269-6"><a href="tree-based-methods.html#cb269-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb269-7"><a href="tree-based-methods.html#cb269-7" aria-hidden="true" tabindex="-1"></a>MSE<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)))</span>
<span id="cb269-8"><a href="tree-based-methods.html#cb269-8" aria-hidden="true" tabindex="-1"></a>MSE<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)))</span>
<span id="cb269-9"><a href="tree-based-methods.html#cb269-9" aria-hidden="true" tabindex="-1"></a>MSE<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)))</span>
<span id="cb269-10"><a href="tree-based-methods.html#cb269-10" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">length</span>(df)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb269-11"><a href="tree-based-methods.html#cb269-11" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> p<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb269-12"><a href="tree-based-methods.html#cb269-12" aria-hidden="true" tabindex="-1"></a>sqrtp <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(p) <span class="co">#Notice that randomForest will automatically round</span></span>
<span id="cb269-13"><a href="tree-based-methods.html#cb269-13" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb269-14"><a href="tree-based-methods.html#cb269-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (B <span class="cf">in</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">5</span>,<span class="at">to =</span> <span class="dv">500</span>,<span class="at">by =</span> <span class="dv">5</span>)) {</span>
<span id="cb269-15"><a href="tree-based-methods.html#cb269-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(B)</span>
<span id="cb269-16"><a href="tree-based-methods.html#cb269-16" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb269-17"><a href="tree-based-methods.html#cb269-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-18"><a href="tree-based-methods.html#cb269-18" aria-hidden="true" tabindex="-1"></a>  rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">ntree =</span> B,<span class="at">mtry =</span> p,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb269-19"><a href="tree-based-methods.html#cb269-19" aria-hidden="true" tabindex="-1"></a>  rf.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> rf.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb269-20"><a href="tree-based-methods.html#cb269-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-21"><a href="tree-based-methods.html#cb269-21" aria-hidden="true" tabindex="-1"></a>  MSE<span class="fl">.1</span>[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((rf.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span>
<span id="cb269-22"><a href="tree-based-methods.html#cb269-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-23"><a href="tree-based-methods.html#cb269-23" aria-hidden="true" tabindex="-1"></a>  rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">ntree =</span> B,<span class="at">mtry =</span> p2,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb269-24"><a href="tree-based-methods.html#cb269-24" aria-hidden="true" tabindex="-1"></a>  rf.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> rf.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb269-25"><a href="tree-based-methods.html#cb269-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-26"><a href="tree-based-methods.html#cb269-26" aria-hidden="true" tabindex="-1"></a>  MSE<span class="fl">.2</span>[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((rf.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span>
<span id="cb269-27"><a href="tree-based-methods.html#cb269-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-28"><a href="tree-based-methods.html#cb269-28" aria-hidden="true" tabindex="-1"></a>  rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> .,<span class="at">data =</span> df.train,<span class="at">ntree =</span> B,<span class="at">mtry =</span> sqrtp,<span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb269-29"><a href="tree-based-methods.html#cb269-29" aria-hidden="true" tabindex="-1"></a>  rf.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> rf.boston,<span class="at">newdata =</span> df.test)</span>
<span id="cb269-30"><a href="tree-based-methods.html#cb269-30" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-31"><a href="tree-based-methods.html#cb269-31" aria-hidden="true" tabindex="-1"></a>  MSE<span class="fl">.3</span>[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((rf.pred<span class="sc">-</span>df.test<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#MSE</span></span>
<span id="cb269-32"><a href="tree-based-methods.html#cb269-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb269-33"><a href="tree-based-methods.html#cb269-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb269-34"><a href="tree-based-methods.html#cb269-34" aria-hidden="true" tabindex="-1"></a><span class="fu">stopCluster</span>(CoreCount)</span>
<span id="cb269-35"><a href="tree-based-methods.html#cb269-35" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoSEQ</span>()</span></code></pre></div>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="tree-based-methods.html#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(MSE<span class="fl">.1</span>,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&#39;darkred&#39;</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">12</span>))</span>
<span id="cb270-2"><a href="tree-based-methods.html#cb270-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(MSE<span class="fl">.2</span>,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&#39;limegreen&#39;</span>)</span>
<span id="cb270-3"><a href="tree-based-methods.html#cb270-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(MSE<span class="fl">.3</span>,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&#39;coral&#39;</span>)</span>
<span id="cb270-4"><a href="tree-based-methods.html#cb270-4" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb270-5"><a href="tree-based-methods.html#cb270-5" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;m = p&quot;</span>,<span class="st">&quot;m = p/2&quot;</span>,<span class="st">&quot;m = sqrt(p)&quot;</span>),<span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;darkred&#39;</span>,<span class="st">&quot;limegreen&quot;</span>,<span class="st">&quot;coral&quot;</span>))</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-181-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="tree-based-methods.html#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
</div>
<div id="exercise-8---dtrfboosting" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Exercise 8 - DT/RF/Boosting</h3>
<p><strong>(a) splitting the data</strong></p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="tree-based-methods.html#cb272-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Carseats</span>
<span id="cb272-2"><a href="tree-based-methods.html#cb272-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb272-3"><a href="tree-based-methods.html#cb272-3" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df),<span class="at">size =</span> <span class="fu">nrow</span>(df)<span class="sc">*</span><span class="fl">0.75</span>)</span>
<span id="cb272-4"><a href="tree-based-methods.html#cb272-4" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[index,]</span>
<span id="cb272-5"><a href="tree-based-methods.html#cb272-5" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>index,]</span>
<span id="cb272-6"><a href="tree-based-methods.html#cb272-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(index)</span></code></pre></div>
<p><strong>(b) fitting a regression tree and plotting</strong></p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="tree-based-methods.html#cb273-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb273-2"><a href="tree-based-methods.html#cb273-2" aria-hidden="true" tabindex="-1"></a>fit.tree <span class="ot">&lt;-</span> <span class="fu">tree</span>(Sales <span class="sc">~</span> .,<span class="at">data =</span> df.train)</span>
<span id="cb273-3"><a href="tree-based-methods.html#cb273-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.tree) ; <span class="fu">text</span>(fit.tree,<span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-184-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see the fitted values above. The values a each terminal node represent the mean value of y given the different splits.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="tree-based-methods.html#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb274-2"><a href="tree-based-methods.html#cb274-2" aria-hidden="true" tabindex="-1"></a>mse.test.t0 <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(<span class="at">object =</span> fit.tree,<span class="at">newdata =</span> df.test) <span class="sc">-</span> df.test<span class="sc">$</span>Sales)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb274-3"><a href="tree-based-methods.html#cb274-3" aria-hidden="true" tabindex="-1"></a>mse.test.t0</span></code></pre></div>
<pre><code>## [1] 3.893676</code></pre>
<p>We see that the mean squared error = 4.06</p>
<p><strong>(c) Pruning the tree</strong></p>
<p>We must do the following:</p>
<ol style="list-style-type: decimal">
<li>Assess how big the tree should be. Done through cross validation</li>
<li>Fit the pruned tree.</li>
<li>Predict and calculate MSE</li>
</ol>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="tree-based-methods.html#cb276-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb276-2"><a href="tree-based-methods.html#cb276-2" aria-hidden="true" tabindex="-1"></a>fit.cv.tree <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(<span class="at">object =</span> fit.tree)</span>
<span id="cb276-3"><a href="tree-based-methods.html#cb276-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> fit.cv.tree<span class="sc">$</span>size,<span class="at">y =</span> fit.cv.tree<span class="sc">$</span>dev,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&quot;darkblue&quot;</span>,<span class="at">xlab =</span> <span class="st">&quot;Tree size&quot;</span>,<span class="at">ylab =</span> <span class="st">&quot;Deviance&quot;</span>,<span class="at">main =</span> <span class="st">&quot;CV Tree&quot;</span>)</span>
<span id="cb276-4"><a href="tree-based-methods.html#cb276-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">min</span>(fit.cv.tree<span class="sc">$</span>dev),<span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>,<span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb276-5"><a href="tree-based-methods.html#cb276-5" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-186-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="tree-based-methods.html#cb277-1" aria-hidden="true" tabindex="-1"></a>fit.cv.tree<span class="sc">$</span>size[<span class="fu">which.min</span>(fit.cv.tree<span class="sc">$</span>dev)]</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<p>With a seed of 123, we see that the best tree size appear to be best 8.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="tree-based-methods.html#cb279-1" aria-hidden="true" tabindex="-1"></a>fit.tree.prune <span class="ot">&lt;-</span> <span class="fu">prune.tree</span>(<span class="at">tree =</span> fit.tree</span>
<span id="cb279-2"><a href="tree-based-methods.html#cb279-2" aria-hidden="true" tabindex="-1"></a>                             ,<span class="at">best =</span> fit.cv.tree<span class="sc">$</span>size[<span class="fu">which.min</span>(fit.cv.tree<span class="sc">$</span>dev)])</span>
<span id="cb279-3"><a href="tree-based-methods.html#cb279-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-4"><a href="tree-based-methods.html#cb279-4" aria-hidden="true" tabindex="-1"></a>mse.test.t1 <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(<span class="at">object =</span> fit.tree.prune,<span class="at">newdata =</span> df.test) <span class="sc">-</span> df.test<span class="sc">$</span>Sales)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb279-5"><a href="tree-based-methods.html#cb279-5" aria-hidden="true" tabindex="-1"></a>mse.test.t1</span></code></pre></div>
<pre><code>## [1] 4.516324</code></pre>
<p>So we see that the MSE appear to be worse after pruning.</p>
<p><strong>(d) Using bagging</strong></p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="tree-based-methods.html#cb281-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb281-2"><a href="tree-based-methods.html#cb281-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb281-3"><a href="tree-based-methods.html#cb281-3" aria-hidden="true" tabindex="-1"></a>fit.bag <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Sales <span class="sc">~</span> .</span>
<span id="cb281-4"><a href="tree-based-methods.html#cb281-4" aria-hidden="true" tabindex="-1"></a>                        ,<span class="at">data =</span> df.train</span>
<span id="cb281-5"><a href="tree-based-methods.html#cb281-5" aria-hidden="true" tabindex="-1"></a>                        ,<span class="at">mtry =</span> <span class="fu">length</span>(df.train)<span class="sc">-</span><span class="dv">1</span> <span class="co">#Merely identifies the amount of predictors</span></span>
<span id="cb281-6"><a href="tree-based-methods.html#cb281-6" aria-hidden="true" tabindex="-1"></a>                        ,<span class="at">importance =</span> <span class="cn">TRUE</span></span>
<span id="cb281-7"><a href="tree-based-methods.html#cb281-7" aria-hidden="true" tabindex="-1"></a>                        ,<span class="at">ntree =</span> <span class="dv">500</span> <span class="co">#This is also the default value</span></span>
<span id="cb281-8"><a href="tree-based-methods.html#cb281-8" aria-hidden="true" tabindex="-1"></a>                        )</span>
<span id="cb281-9"><a href="tree-based-methods.html#cb281-9" aria-hidden="true" tabindex="-1"></a>mse.test.bag <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(<span class="at">object =</span> fit.bag,<span class="at">newdata =</span> df.test) <span class="sc">-</span> df.test<span class="sc">$</span>Sales)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb281-10"><a href="tree-based-methods.html#cb281-10" aria-hidden="true" tabindex="-1"></a>mse.test.bag</span></code></pre></div>
<pre><code>## [1] 2.253202</code></pre>
<p>We see that the MSE is now 2.25. Which is much better than previously.</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="tree-based-methods.html#cb283-1" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(fit.bag)</span></code></pre></div>
<pre><code>##                %IncMSE IncNodePurity
## CompPrice   37.1354164    281.825832
## Income      10.1378593    128.955945
## Advertising 21.5736372    151.157032
## Population  -0.1362661     71.539003
## Price       72.3036488    698.000814
## ShelveLoc   77.3615535    736.724942
## Age         20.2693017    187.902438
## Education    1.9415069     68.272112
## Urban        0.4779681     10.140546
## US           0.5347757      8.054079</code></pre>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="tree-based-methods.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(fit.bag)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-190-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the most important variables are ShelveLocation and Price, that was also what we saw in the trees earlier, where it started with the shelve location.</p>
<p>We notice that the Population is actually inverse, hence by removing it, it appears that the MSE will improve slightly.</p>
<p><strong>(e) Random Forest</strong></p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="tree-based-methods.html#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb286-2"><a href="tree-based-methods.html#cb286-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb286-3"><a href="tree-based-methods.html#cb286-3" aria-hidden="true" tabindex="-1"></a>fit.rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Sales <span class="sc">~</span> .</span>
<span id="cb286-4"><a href="tree-based-methods.html#cb286-4" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">data =</span> df.train</span>
<span id="cb286-5"><a href="tree-based-methods.html#cb286-5" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">mtry =</span> <span class="fu">length</span>(df.train)<span class="sc">/</span><span class="dv">3</span> <span class="co">#Merely identifies the amount of predictors / 3</span></span>
<span id="cb286-6"><a href="tree-based-methods.html#cb286-6" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">importance =</span> <span class="cn">TRUE</span></span>
<span id="cb286-7"><a href="tree-based-methods.html#cb286-7" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">ntree =</span> <span class="dv">500</span> <span class="co">#This is the default value</span></span>
<span id="cb286-8"><a href="tree-based-methods.html#cb286-8" aria-hidden="true" tabindex="-1"></a>                       )</span>
<span id="cb286-9"><a href="tree-based-methods.html#cb286-9" aria-hidden="true" tabindex="-1"></a>mse.test.rf <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(<span class="at">object =</span> fit.rf,<span class="at">newdata =</span> df.test) <span class="sc">-</span> df.test<span class="sc">$</span>Sales)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb286-10"><a href="tree-based-methods.html#cb286-10" aria-hidden="true" tabindex="-1"></a>mse.test.rf</span></code></pre></div>
<pre><code>## [1] 2.549939</code></pre>
<p>We see that the test MSE is in fact higher than the bagging model.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="tree-based-methods.html#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(fit.rf)</span></code></pre></div>
<pre><code>##                %IncMSE IncNodePurity
## CompPrice   22.8640496     243.17255
## Income       8.7362027     168.34606
## Advertising 18.2804426     181.31384
## Population   0.9108472     117.37501
## Price       51.3986491     591.42959
## ShelveLoc   54.3842439     598.76271
## Age         18.8119973     259.15144
## Education    3.2342193      94.47983
## Urban        1.8279348      17.25155
## US           2.3714796      18.45708</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="tree-based-methods.html#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(fit.rf)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-192-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Again we see that the shelve location and price are the mosts important variables just as when we were doing bagging.</p>
<p><strong>Summary</strong></p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="tree-based-methods.html#cb291-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(mse.test.t0</span>
<span id="cb291-2"><a href="tree-based-methods.html#cb291-2" aria-hidden="true" tabindex="-1"></a>      ,mse.test.t1</span>
<span id="cb291-3"><a href="tree-based-methods.html#cb291-3" aria-hidden="true" tabindex="-1"></a>      ,mse.test.bag</span>
<span id="cb291-4"><a href="tree-based-methods.html#cb291-4" aria-hidden="true" tabindex="-1"></a>      ,mse.test.rf</span>
<span id="cb291-5"><a href="tree-based-methods.html#cb291-5" aria-hidden="true" tabindex="-1"></a>      )</span></code></pre></div>
<pre><code>##                  [,1]
## mse.test.t0  3.893676
## mse.test.t1  4.516324
## mse.test.bag 2.253202
## mse.test.rf  2.549939</code></pre>
</div>
<div id="exercise-9---decision-tree-pruning" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Exercise 9 - Decision Tree / Pruning</h3>
<p><strong>(a) loading the data</strong></p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="tree-based-methods.html#cb293-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb293-2"><a href="tree-based-methods.html#cb293-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> OJ</span>
<span id="cb293-3"><a href="tree-based-methods.html#cb293-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb293-4"><a href="tree-based-methods.html#cb293-4" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df),<span class="at">size =</span> <span class="dv">800</span>)</span>
<span id="cb293-5"><a href="tree-based-methods.html#cb293-5" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[index,]</span>
<span id="cb293-6"><a href="tree-based-methods.html#cb293-6" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>index,]</span>
<span id="cb293-7"><a href="tree-based-methods.html#cb293-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(index)</span></code></pre></div>
<p><strong>(b) fitting a tree to the train data</strong></p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="tree-based-methods.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb294-2"><a href="tree-based-methods.html#cb294-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb294-3"><a href="tree-based-methods.html#cb294-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb294-4"><a href="tree-based-methods.html#cb294-4" aria-hidden="true" tabindex="-1"></a>fit.tree <span class="ot">&lt;-</span> <span class="fu">tree</span>(Purchase <span class="sc">~</span> .,<span class="at">data =</span> df.train)</span>
<span id="cb294-5"><a href="tree-based-methods.html#cb294-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.tree)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Purchase ~ ., data = df.train)
## Variables actually used in tree construction:
## [1] &quot;LoyalCH&quot;   &quot;PriceDiff&quot;
## Number of terminal nodes:  8 
## Residual mean deviance:  0.7625 = 603.9 / 792 
## Misclassification error rate: 0.165 = 132 / 800</code></pre>
<p>We see that the size of the tree is 8 terminal nodes. Where the error rate is 16.5% hence an accuracy is 83.5%.</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="tree-based-methods.html#cb296-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(df.train<span class="sc">$</span>Purchase)</span></code></pre></div>
<pre><code>##    MM
## CH  0
## MM  1</code></pre>
<p>We see that MM is the positive result. Hence we can check the null rate of the data.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="tree-based-methods.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(df.train<span class="sc">$</span>Purchase)[<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">table</span>(df.train<span class="sc">$</span>Purchase))</span></code></pre></div>
<pre><code>##      MM 
## 0.39125</code></pre>
<p>We see that if we predicted all purchases to be MM, then we would roughly have an accuracy of 40%. Hence the tree is far superior to the null model.</p>
<p><strong>(c) Interpreting results for a node</strong></p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="tree-based-methods.html#cb300-1" aria-hidden="true" tabindex="-1"></a>fit.tree[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## $frame
##          var   n        dev yval splits.cutleft splits.cutright   yprob.CH
## 1    LoyalCH 800 1070.88631   CH        &lt;0.5036         &gt;0.5036 0.60875000
## 2    LoyalCH 350  415.06732   MM      &lt;0.276142       &gt;0.276142 0.28000000
## 4    LoyalCH 170  130.99077   MM     &lt;0.0356415      &gt;0.0356415 0.12941176
## 8     &lt;leaf&gt;  56   10.03274   MM                                0.01785714
## 9     &lt;leaf&gt; 114  108.91980   MM                                0.18421053
## 5  PriceDiff 180  245.15969   MM          &lt;0.05           &gt;0.05 0.42222222
## 10    &lt;leaf&gt;  74   74.61071   MM                                0.20270270
## 11    &lt;leaf&gt; 106  144.52285   CH                                0.57547170
## 3  PriceDiff 450  357.13148   CH         &lt;-0.39          &gt;-0.39 0.86444444
## 6     &lt;leaf&gt;  27   32.81544   MM                                0.29629630
## 7    LoyalCH 423  273.69949   CH      &lt;0.705326       &gt;0.705326 0.90070922
## 14 PriceDiff 130  135.46105   CH         &lt;0.145          &gt;0.145 0.78461538
## 28    &lt;leaf&gt;  43   58.46604   CH                                0.58139535
## 29    &lt;leaf&gt;  87   62.07028   CH                                0.88505747
## 15    &lt;leaf&gt; 293  112.47137   CH                                0.95221843
##      yprob.MM
## 1  0.39125000
## 2  0.72000000
## 4  0.87058824
## 8  0.98214286
## 9  0.81578947
## 5  0.57777778
## 10 0.79729730
## 11 0.42452830
## 3  0.13555556
## 6  0.70370370
## 7  0.09929078
## 14 0.21538462
## 28 0.41860465
## 29 0.11494253
## 15 0.04778157</code></pre>
<p>We see the 8 leafs (terminal nodes) and their probabilities for CH and MM. E.g., leaf no. 15 is almost always CH (95%), as out of the 293 observations that meet the conditions. Hence the purity of the leaf is rather high.</p>
<p>We have more indecisive nodes, such as leaf no. 11, where 57% are CH and 43% are MM.</p>
<p><strong>(d) Creating a plot of the tree</strong></p>
<p>We create the plot with the <code>rpart</code> package in stead, which can produce a nicer plot.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="tree-based-methods.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rattle)</span>
<span id="cb302-2"><a href="tree-based-methods.html#cb302-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb302-3"><a href="tree-based-methods.html#cb302-3" aria-hidden="true" tabindex="-1"></a>fit.tree <span class="ot">&lt;-</span> rpart<span class="sc">::</span><span class="fu">rpart</span>(Purchase <span class="sc">~</span> .,<span class="at">data =</span> df.train)</span>
<span id="cb302-4"><a href="tree-based-methods.html#cb302-4" aria-hidden="true" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(<span class="at">model =</span> fit.tree)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-199-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the tone of the color reflects the purity of the leads, where e.g., we have a node with 100% CH and 0% MM, although it only contain 1% of the overall data. But it is certainly pure. But perhaps it is overfitted?</p>
<p><strong>(e) Predicting the test data</strong></p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="tree-based-methods.html#cb303-1" aria-hidden="true" tabindex="-1"></a>preds.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.tree,<span class="at">newdata =</span> df.test,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb303-2"><a href="tree-based-methods.html#cb303-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-3"><a href="tree-based-methods.html#cb303-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb303-4"><a href="tree-based-methods.html#cb303-4" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="at">data =</span> preds.tree,df.test<span class="sc">$</span>Purchase)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 141  24
##         MM  25  80
##                                           
##                Accuracy : 0.8185          
##                  95% CI : (0.7673, 0.8626)
##     No Information Rate : 0.6148          
##     P-Value [Acc &gt; NIR] : 3.407e-13       
##                                           
##                   Kappa : 0.6175          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.8494          
##             Specificity : 0.7692          
##          Pos Pred Value : 0.8545          
##          Neg Pred Value : 0.7619          
##              Prevalence : 0.6148          
##          Detection Rate : 0.5222          
##    Detection Prevalence : 0.6111          
##       Balanced Accuracy : 0.8093          
##                                           
##        &#39;Positive&#39; Class : CH              
## </code></pre>
<p>We see a sensitivity of 85% and specificity of 77%. The overall error rate = 18,15%, which is higher (as expected) than the in sample error.</p>
<p><strong>(d) Apply the cv.tree() to select optimal tree size</strong></p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="tree-based-methods.html#cb305-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb305-2"><a href="tree-based-methods.html#cb305-2" aria-hidden="true" tabindex="-1"></a>fit.tree <span class="ot">&lt;-</span> <span class="fu">tree</span>(Purchase <span class="sc">~</span> .,<span class="at">data =</span> df.train)</span>
<span id="cb305-3"><a href="tree-based-methods.html#cb305-3" aria-hidden="true" tabindex="-1"></a>fit.cv.tree <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(<span class="at">object =</span> fit.tree,<span class="at">K =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>The following plots both the deviance and the cost complexity in the same plot with double y axis’</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="tree-based-methods.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>) <span class="sc">+</span> <span class="fl">0.3</span>)  <span class="co">#Additional space for second y-axis</span></span>
<span id="cb306-2"><a href="tree-based-methods.html#cb306-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-3"><a href="tree-based-methods.html#cb306-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting first series</span></span>
<span id="cb306-4"><a href="tree-based-methods.html#cb306-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> fit.cv.tree<span class="sc">$</span>size,<span class="at">y =</span> fit.cv.tree<span class="sc">$</span>dev</span>
<span id="cb306-5"><a href="tree-based-methods.html#cb306-5" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">type =</span> <span class="st">&quot;l&quot;</span></span>
<span id="cb306-6"><a href="tree-based-methods.html#cb306-6" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span> <span class="st">&quot;darkblue&quot;</span></span>
<span id="cb306-7"><a href="tree-based-methods.html#cb306-7" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">ylab =</span> <span class="st">&quot;&quot;</span></span>
<span id="cb306-8"><a href="tree-based-methods.html#cb306-8" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlab =</span> <span class="st">&quot;Size&quot;</span></span>
<span id="cb306-9"><a href="tree-based-methods.html#cb306-9" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">main =</span> <span class="st">&quot;CV Tree Comparisons&quot;</span></span>
<span id="cb306-10"><a href="tree-based-methods.html#cb306-10" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">sub =</span> <span class="st">&quot;A doubled axis interpretation&quot;</span></span>
<span id="cb306-11"><a href="tree-based-methods.html#cb306-11" aria-hidden="true" tabindex="-1"></a>     )</span>
<span id="cb306-12"><a href="tree-based-methods.html#cb306-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-13"><a href="tree-based-methods.html#cb306-13" aria-hidden="true" tabindex="-1"></a><span class="co">#Adding y axis 1 name with colours</span></span>
<span id="cb306-14"><a href="tree-based-methods.html#cb306-14" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Deviance&quot;</span>,<span class="at">side =</span> <span class="dv">2</span>,<span class="at">line =</span> <span class="dv">3</span>,<span class="at">col =</span> <span class="st">&quot;darkblue&quot;</span>)</span>
<span id="cb306-15"><a href="tree-based-methods.html#cb306-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-16"><a href="tree-based-methods.html#cb306-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Other details</span></span>
<span id="cb306-17"><a href="tree-based-methods.html#cb306-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> fit.cv.tree<span class="sc">$</span>dev[<span class="fu">which.min</span>(fit.cv.tree<span class="sc">$</span>dev)],<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">lty =</span> <span class="dv">4</span>)</span>
<span id="cb306-18"><a href="tree-based-methods.html#cb306-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-19"><a href="tree-based-methods.html#cb306-19" aria-hidden="true" tabindex="-1"></a><span class="co">#Adding a new plot on-top of the first</span></span>
<span id="cb306-20"><a href="tree-based-methods.html#cb306-20" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new =</span> <span class="cn">TRUE</span>)</span>
<span id="cb306-21"><a href="tree-based-methods.html#cb306-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-22"><a href="tree-based-methods.html#cb306-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Adding the second series accompanied by a new axis</span></span>
<span id="cb306-23"><a href="tree-based-methods.html#cb306-23" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> fit.cv.tree<span class="sc">$</span>size,<span class="at">y =</span> fit.cv.tree<span class="sc">$</span>k,<span class="at">col =</span> <span class="st">&quot;darkred&quot;</span></span>
<span id="cb306-24"><a href="tree-based-methods.html#cb306-24" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">type =</span> <span class="st">&quot;l&quot;</span></span>
<span id="cb306-25"><a href="tree-based-methods.html#cb306-25" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">axes =</span> <span class="cn">FALSE</span>,<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,<span class="at">ylab =</span> <span class="st">&quot;&quot;</span> <span class="co">#We want to avoid creating axis&#39; and names on-top of what we already have</span></span>
<span id="cb306-26"><a href="tree-based-methods.html#cb306-26" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">lty =</span> <span class="dv">2</span></span>
<span id="cb306-27"><a href="tree-based-methods.html#cb306-27" aria-hidden="true" tabindex="-1"></a>     )</span>
<span id="cb306-28"><a href="tree-based-methods.html#cb306-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-29"><a href="tree-based-methods.html#cb306-29" aria-hidden="true" tabindex="-1"></a><span class="co">#Adding y axis 2 ticks and labels</span></span>
<span id="cb306-30"><a href="tree-based-methods.html#cb306-30" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">4</span> <span class="co">#This is right, counting clockwise from the bottom</span></span>
<span id="cb306-31"><a href="tree-based-methods.html#cb306-31" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">at =</span> <span class="fu">pretty</span>(<span class="fu">range</span>(fit.cv.tree<span class="sc">$</span>k[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(fit.cv.tree<span class="sc">$</span>k)])) <span class="co">#I remove the first obs, as it is -Inf, that is not workgin</span></span>
<span id="cb306-32"><a href="tree-based-methods.html#cb306-32" aria-hidden="true" tabindex="-1"></a>     )</span>
<span id="cb306-33"><a href="tree-based-methods.html#cb306-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-34"><a href="tree-based-methods.html#cb306-34" aria-hidden="true" tabindex="-1"></a><span class="co">#And axis name</span></span>
<span id="cb306-35"><a href="tree-based-methods.html#cb306-35" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Cost Complexity&quot;</span>,<span class="at">side =</span> <span class="dv">4</span>,<span class="at">line =</span> <span class="dv">3</span>,<span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>)</span>
<span id="cb306-36"><a href="tree-based-methods.html#cb306-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-37"><a href="tree-based-methods.html#cb306-37" aria-hidden="true" tabindex="-1"></a><span class="co">#Other details</span></span>
<span id="cb306-38"><a href="tree-based-methods.html#cb306-38" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb306-39"><a href="tree-based-methods.html#cb306-39" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Deviance&quot;</span>,<span class="st">&quot;TuningParam&quot;</span>),<span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkblue&quot;</span>,<span class="st">&quot;darkred&quot;</span>),<span class="at">cex =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-202"></span>
<img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-202-1.png" alt="CV Tree atttributes" width="720" />
<p class="caption">
Figure 3.6: CV Tree atttributes
</p>
</div>
<p>We see that the optimal model appear to be of size 6. We could now train a pruned tree with 6 leaves. It is not done in this exercise.</p>
<p><strong>(g) Plotting Misclassification rate (on train) against treesize</strong></p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="tree-based-methods.html#cb307-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb307-2"><a href="tree-based-methods.html#cb307-2" aria-hidden="true" tabindex="-1"></a>mcr <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(<span class="dv">0</span>) <span class="co">#misclassification rate</span></span>
<span id="cb307-3"><a href="tree-based-methods.html#cb307-3" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb307-4"><a href="tree-based-methods.html#cb307-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (T <span class="cf">in</span> <span class="fu">sort</span>(fit.cv.tree<span class="sc">$</span>size,<span class="at">decreasing =</span> <span class="cn">FALSE</span>)[<span class="sc">-</span><span class="dv">1</span>]) { <span class="co">#The first is removed as apparantly we need at least two leaves</span></span>
<span id="cb307-5"><a href="tree-based-methods.html#cb307-5" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb307-6"><a href="tree-based-methods.html#cb307-6" aria-hidden="true" tabindex="-1"></a>  fit.tree.prune <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(<span class="at">tree =</span> fit.tree,<span class="at">best =</span> T)</span>
<span id="cb307-7"><a href="tree-based-methods.html#cb307-7" aria-hidden="true" tabindex="-1"></a>  preds.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.tree.prune,<span class="at">newdata =</span> df.train,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb307-8"><a href="tree-based-methods.html#cb307-8" aria-hidden="true" tabindex="-1"></a>  mcr[idx] <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">-</span><span class="fu">mean</span>(preds.tree<span class="sc">==</span>df.train<span class="sc">$</span>Purchase))</span>
<span id="cb307-9"><a href="tree-based-methods.html#cb307-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb307-10"><a href="tree-based-methods.html#cb307-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-11"><a href="tree-based-methods.html#cb307-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="fu">sort</span>(fit.cv.tree<span class="sc">$</span>size,<span class="at">decreasing =</span> <span class="cn">FALSE</span>)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb307-12"><a href="tree-based-methods.html#cb307-12" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">y =</span> mcr,<span class="at">type =</span> <span class="st">&quot;l&quot;</span></span>
<span id="cb307-13"><a href="tree-based-methods.html#cb307-13" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlab =</span> <span class="st">&quot;Size&quot;</span></span>
<span id="cb307-14"><a href="tree-based-methods.html#cb307-14" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">ylab =</span> <span class="st">&quot;Misclassification rate&quot;</span></span>
<span id="cb307-15"><a href="tree-based-methods.html#cb307-15" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">main =</span> <span class="st">&quot;Misclassification rate at different sizes&quot;</span></span>
<span id="cb307-16"><a href="tree-based-methods.html#cb307-16" aria-hidden="true" tabindex="-1"></a>     )</span>
<span id="cb307-17"><a href="tree-based-methods.html#cb307-17" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-203-1.png" width="720" style="display: block; margin: auto;" /></p>
<p><strong>(h) Interpreting the plot above</strong></p>
<p>We see that the model actually does not improve after a size of 4. Indicating that we should prune even more than previously discovered. That is a bit odd.</p>
<p><strong>(i) Producing pruned tree</strong></p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="tree-based-methods.html#cb308-1" aria-hidden="true" tabindex="-1"></a>fit.tree.t4 <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(<span class="at">tree =</span> fit.tree,<span class="at">best =</span> <span class="dv">4</span>)</span>
<span id="cb308-2"><a href="tree-based-methods.html#cb308-2" aria-hidden="true" tabindex="-1"></a>preds.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.tree.t4,<span class="at">newdata =</span> df.train,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb308-3"><a href="tree-based-methods.html#cb308-3" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="sc">-</span><span class="fu">mean</span>(preds.tree<span class="sc">==</span>df.train<span class="sc">$</span>Purchase))</span></code></pre></div>
<pre><code>## [1] 0.165</code></pre>
<p>We see a misclassification rate of 16.5% for the pruned tree, where <span class="math inline">\(T_0\)</span> had a misclassification rate of 18.15%, which is higher than the pruned tree. Hence, we improved the model.</p>
<p><strong>(j) comparing the results</strong></p>
<p>We see that the pruned tree is performing better than the unpruned tree.</p>
<p><strong>(k) Comparing test partition performance</strong></p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="tree-based-methods.html#cb310-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb310-2"><a href="tree-based-methods.html#cb310-2" aria-hidden="true" tabindex="-1"></a>  preds.test.tree.t0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.tree,<span class="at">newdata =</span> df.test,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb310-3"><a href="tree-based-methods.html#cb310-3" aria-hidden="true" tabindex="-1"></a>  preds.test.tree.t4 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.tree.t4,<span class="at">newdata =</span> df.test,<span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb310-4"><a href="tree-based-methods.html#cb310-4" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">-</span><span class="fu">mean</span>(preds.test.tree.t0<span class="sc">==</span>df.test<span class="sc">$</span>Purchase)) <span class="sc">%&gt;%</span> <span class="fu">print</span>() </span>
<span id="cb310-5"><a href="tree-based-methods.html#cb310-5" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">-</span><span class="fu">mean</span>(preds.test.tree.t4<span class="sc">==</span>df.test<span class="sc">$</span>Purchase)) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb310-6"><a href="tree-based-methods.html#cb310-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] 0.1851852
## [1] 0.1851852</code></pre>
<p>The models appear to perform equally good on the test partition. Hence we would prefer the pruned tree, as it is simpler, hence less flexibility.</p>
</div>
<div id="exercise-10---boostinggamlinearregbagging---comparison" class="section level3" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Exercise 10 - Boosting/GAM/LinearReg/Bagging - Comparison</h3>
<p><strong>(a) Loading the data</strong></p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="tree-based-methods.html#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb312-2"><a href="tree-based-methods.html#cb312-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters</span>
<span id="cb312-3"><a href="tree-based-methods.html#cb312-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(Hitters<span class="sc">$</span>Salary))</span></code></pre></div>
<pre><code>## [1] 59</code></pre>
<p>We see that there are 59 observations without salary data. That we want to remove.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="tree-based-methods.html#cb314-1" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">is.na</span>(Hitters<span class="sc">$</span>Salary)</span>
<span id="cb314-2"><a href="tree-based-methods.html#cb314-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="sc">!</span>index,]</span>
<span id="cb314-3"><a href="tree-based-methods.html#cb314-3" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>Salary <span class="ot">&lt;-</span> <span class="fu">log</span>(df<span class="sc">$</span>Salary) <span class="co">#Log of the salaries</span></span></code></pre></div>
<p><strong>(b) Creating a train data set</strong></p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="tree-based-methods.html#cb315-1" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[<span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>,]</span>
<span id="cb315-2"><a href="tree-based-methods.html#cb315-2" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span><span class="dv">1</span><span class="sc">:-</span><span class="dv">200</span>,]</span></code></pre></div>
<p><strong>(c) Performing boosting</strong></p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="tree-based-methods.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb316-2"><a href="tree-based-methods.html#cb316-2" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb316-3"><a href="tree-based-methods.html#cb316-3" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.001</span>,<span class="at">to =</span> <span class="fl">0.1</span>,<span class="at">by =</span> <span class="fl">0.00099</span>)</span>
<span id="cb316-4"><a href="tree-based-methods.html#cb316-4" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb316-5"><a href="tree-based-methods.html#cb316-5" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(lambda))</span>
<span id="cb316-6"><a href="tree-based-methods.html#cb316-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-7"><a href="tree-based-methods.html#cb316-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (l <span class="cf">in</span> lambda) {</span>
<span id="cb316-8"><a href="tree-based-methods.html#cb316-8" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb316-9"><a href="tree-based-methods.html#cb316-9" aria-hidden="true" tabindex="-1"></a>  fit.boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary <span class="sc">~</span> .</span>
<span id="cb316-10"><a href="tree-based-methods.html#cb316-10" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">data =</span> df.train</span>
<span id="cb316-11"><a href="tree-based-methods.html#cb316-11" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">n.trees =</span> B</span>
<span id="cb316-12"><a href="tree-based-methods.html#cb316-12" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span></span>
<span id="cb316-13"><a href="tree-based-methods.html#cb316-13" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">shrinkage =</span> l</span>
<span id="cb316-14"><a href="tree-based-methods.html#cb316-14" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb316-15"><a href="tree-based-methods.html#cb316-15" aria-hidden="true" tabindex="-1"></a>  mse[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((fit.boost<span class="sc">$</span>fit <span class="sc">-</span> df.train<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb316-16"><a href="tree-based-methods.html#cb316-16" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>In the loop above we have tested the model with different lambda values. This hyper parameter adjusts how quick the model learns. We can now plot the MSE against the tuning parameter.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="tree-based-methods.html#cb317-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> lambda,<span class="at">y =</span> mse,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">main =</span> <span class="st">&quot;Shrinkage&quot;</span>)</span>
<span id="cb317-2"><a href="tree-based-methods.html#cb317-2" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-211-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see as lambda increases, the MSE lowers. This makes sense, as the higher lambda, the quicker the model learns and the more it fits to the residuals of the previous model. It can often be seen as, the lower we have lambda the more models are required. As the model is fitted on the train data, it is also expected to perform better, as we gradually allow it to learn faster.</p>
<p><em>Notice, that the model is actually able to perfectly fit the train data</em></p>
<p><strong>(d) Testing the model</strong></p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="tree-based-methods.html#cb318-1" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb318-2"><a href="tree-based-methods.html#cb318-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.001</span>,<span class="at">to =</span> <span class="fl">0.1</span>,<span class="at">by =</span> <span class="fl">0.00099</span>)</span>
<span id="cb318-3"><a href="tree-based-methods.html#cb318-3" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb318-4"><a href="tree-based-methods.html#cb318-4" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(lambda))</span>
<span id="cb318-5"><a href="tree-based-methods.html#cb318-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb318-6"><a href="tree-based-methods.html#cb318-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (l <span class="cf">in</span> lambda) {</span>
<span id="cb318-7"><a href="tree-based-methods.html#cb318-7" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb318-8"><a href="tree-based-methods.html#cb318-8" aria-hidden="true" tabindex="-1"></a>  fit.boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary <span class="sc">~</span> .</span>
<span id="cb318-9"><a href="tree-based-methods.html#cb318-9" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">data =</span> df.train</span>
<span id="cb318-10"><a href="tree-based-methods.html#cb318-10" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">n.trees =</span> B</span>
<span id="cb318-11"><a href="tree-based-methods.html#cb318-11" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span></span>
<span id="cb318-12"><a href="tree-based-methods.html#cb318-12" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">shrinkage =</span> l</span>
<span id="cb318-13"><a href="tree-based-methods.html#cb318-13" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb318-14"><a href="tree-based-methods.html#cb318-14" aria-hidden="true" tabindex="-1"></a>  preds.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.boost,<span class="at">newdata =</span> df.test,<span class="at">n.trees =</span> B)</span>
<span id="cb318-15"><a href="tree-based-methods.html#cb318-15" aria-hidden="true" tabindex="-1"></a>  mse[idx] <span class="ot">&lt;-</span> <span class="fu">mean</span>((preds.boost <span class="sc">-</span> df.test<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb318-16"><a href="tree-based-methods.html#cb318-16" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we can plot this.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="tree-based-methods.html#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> lambda,<span class="at">y =</span> mse,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">main =</span> <span class="st">&quot;Shrinkage&quot;</span>)</span>
<span id="cb319-2"><a href="tree-based-methods.html#cb319-2" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-213-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Here we see that we have a minimum around a tuning parameter of 0.04, hence somewhere in the middle.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="tree-based-methods.html#cb320-1" aria-hidden="true" tabindex="-1"></a>fit.boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary <span class="sc">~</span> .</span>
<span id="cb320-2"><a href="tree-based-methods.html#cb320-2" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">data =</span> df.train</span>
<span id="cb320-3"><a href="tree-based-methods.html#cb320-3" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">n.trees =</span> B</span>
<span id="cb320-4"><a href="tree-based-methods.html#cb320-4" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span></span>
<span id="cb320-5"><a href="tree-based-methods.html#cb320-5" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">shrinkage =</span> lambda[<span class="fu">which.min</span>(mse)]</span>
<span id="cb320-6"><a href="tree-based-methods.html#cb320-6" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb320-7"><a href="tree-based-methods.html#cb320-7" aria-hidden="true" tabindex="-1"></a>preds.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.boost,<span class="at">newdata =</span> df.test,<span class="at">n.trees =</span> B)</span>
<span id="cb320-8"><a href="tree-based-methods.html#cb320-8" aria-hidden="true" tabindex="-1"></a>mse.boost <span class="ot">&lt;-</span> <span class="fu">mean</span>((preds.boost <span class="sc">-</span> df.test<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p><strong>(e) Comparing to regression</strong></p>
<p>Linear regression</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="tree-based-methods.html#cb321-1" aria-hidden="true" tabindex="-1"></a>fit.lin <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary <span class="sc">~</span> .,<span class="at">data =</span> df.train)</span>
<span id="cb321-2"><a href="tree-based-methods.html#cb321-2" aria-hidden="true" tabindex="-1"></a>preds.lin <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.lin,<span class="at">newdata =</span> df.test)</span>
<span id="cb321-3"><a href="tree-based-methods.html#cb321-3" aria-hidden="true" tabindex="-1"></a>mse.lin <span class="ot">&lt;-</span> <span class="fu">mean</span>((preds.lin <span class="sc">-</span> df.test<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>We also choose a smoothing smoothing spline with the three most correlating variables, hence we construct a gam.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="tree-based-methods.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Assessing correlation</span></span>
<span id="cb322-2"><a href="tree-based-methods.html#cb322-2" aria-hidden="true" tabindex="-1"></a>correl <span class="ot">&lt;-</span> <span class="fu">cor</span>(<span class="at">y =</span> df.train<span class="sc">$</span>Salary,<span class="at">x =</span> df.train[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">14</span>,<span class="dv">15</span>,<span class="dv">19</span>,<span class="dv">20</span>)])</span>
<span id="cb322-3"><a href="tree-based-methods.html#cb322-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb322-4"><a href="tree-based-methods.html#cb322-4" aria-hidden="true" tabindex="-1"></a>gam.df <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co">#Tested different levels, and 2 appear to be the best</span></span>
<span id="cb322-5"><a href="tree-based-methods.html#cb322-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb322-6"><a href="tree-based-methods.html#cb322-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gam)</span>
<span id="cb322-7"><a href="tree-based-methods.html#cb322-7" aria-hidden="true" tabindex="-1"></a>fit.gam <span class="ot">&lt;-</span> <span class="fu">gam</span>(Salary <span class="sc">~</span> <span class="fu">s</span>(CRBI,gam.df) <span class="sc">+</span> <span class="fu">s</span>(CRuns,gam.df) <span class="sc">+</span> <span class="fu">s</span>(CHits,gam.df)</span>
<span id="cb322-8"><a href="tree-based-methods.html#cb322-8" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">data =</span> df.train)</span>
<span id="cb322-9"><a href="tree-based-methods.html#cb322-9" aria-hidden="true" tabindex="-1"></a>preds.gam <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.gam,<span class="at">newdata =</span> df.test)</span>
<span id="cb322-10"><a href="tree-based-methods.html#cb322-10" aria-hidden="true" tabindex="-1"></a>mse.gam <span class="ot">&lt;-</span> <span class="fu">mean</span>((preds.gam <span class="sc">-</span> df.test<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>Comparison</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="tree-based-methods.html#cb323-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setNames</span>(<span class="fu">as.data.frame</span>(<span class="fu">rbind</span>(mse.boost,mse.lin,mse.gam)),<span class="at">nm =</span> <span class="st">&quot;MSE&quot;</span>)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mse.boost</td>
<td align="right">0.2538902</td>
</tr>
<tr class="even">
<td align="left">mse.lin</td>
<td align="right">0.4917959</td>
</tr>
<tr class="odd">
<td align="left">mse.gam</td>
<td align="right">0.3129716</td>
</tr>
</tbody>
</table>
</div>
<p>We see that boosting is the better model, then gam and lastly the linear model.</p>
<p><strong>(f) Variable importance boosting model</strong></p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="tree-based-methods.html#cb324-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.boost)</span></code></pre></div>
<img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-218-1.png" width="720" style="display: block; margin: auto;" />
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">var</th>
<th align="right">rel.inf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">CAtBat</td>
<td align="left">CAtBat</td>
<td align="right">15.0748709</td>
</tr>
<tr class="even">
<td align="left">CHits</td>
<td align="left">CHits</td>
<td align="right">12.1847777</td>
</tr>
<tr class="odd">
<td align="left">CWalks</td>
<td align="left">CWalks</td>
<td align="right">8.4211213</td>
</tr>
<tr class="even">
<td align="left">CRBI</td>
<td align="left">CRBI</td>
<td align="right">8.3944209</td>
</tr>
<tr class="odd">
<td align="left">PutOuts</td>
<td align="left">PutOuts</td>
<td align="right">7.8331550</td>
</tr>
<tr class="even">
<td align="left">Years</td>
<td align="left">Years</td>
<td align="right">6.6926906</td>
</tr>
<tr class="odd">
<td align="left">Walks</td>
<td align="left">Walks</td>
<td align="right">5.7274660</td>
</tr>
<tr class="even">
<td align="left">Hits</td>
<td align="left">Hits</td>
<td align="right">5.4847269</td>
</tr>
<tr class="odd">
<td align="left">CHmRun</td>
<td align="left">CHmRun</td>
<td align="right">5.1226050</td>
</tr>
<tr class="even">
<td align="left">Assists</td>
<td align="left">Assists</td>
<td align="right">5.1106103</td>
</tr>
<tr class="odd">
<td align="left">AtBat</td>
<td align="left">AtBat</td>
<td align="right">4.3317039</td>
</tr>
<tr class="even">
<td align="left">CRuns</td>
<td align="left">CRuns</td>
<td align="right">3.8104777</td>
</tr>
<tr class="odd">
<td align="left">HmRun</td>
<td align="left">HmRun</td>
<td align="right">3.6299786</td>
</tr>
<tr class="even">
<td align="left">RBI</td>
<td align="left">RBI</td>
<td align="right">2.8746393</td>
</tr>
<tr class="odd">
<td align="left">Runs</td>
<td align="left">Runs</td>
<td align="right">2.1118339</td>
</tr>
<tr class="even">
<td align="left">Errors</td>
<td align="left">Errors</td>
<td align="right">1.9850110</td>
</tr>
<tr class="odd">
<td align="left">Division</td>
<td align="left">Division</td>
<td align="right">0.4982340</td>
</tr>
<tr class="even">
<td align="left">League</td>
<td align="left">League</td>
<td align="right">0.3564011</td>
</tr>
<tr class="odd">
<td align="left">NewLeague</td>
<td align="left">NewLeague</td>
<td align="right">0.3552757</td>
</tr>
</tbody>
</table>
</div>
<p>We see that CAtBat is far more important than the other variables. That is interesting, as that was merely the variable with the fourth most correlation with y. We do infact see that this model yields an importance that is rather different from assessing correalation, as we normally would.</p>
<p>We can add that variable to the gam model, and see how it performs:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="tree-based-methods.html#cb325-1" aria-hidden="true" tabindex="-1"></a>gam.df <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co">#Tested different levels, and 2 appear to be the best</span></span>
<span id="cb325-2"><a href="tree-based-methods.html#cb325-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb325-3"><a href="tree-based-methods.html#cb325-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gam)</span>
<span id="cb325-4"><a href="tree-based-methods.html#cb325-4" aria-hidden="true" tabindex="-1"></a>fit.gam <span class="ot">&lt;-</span> <span class="fu">gam</span>(Salary <span class="sc">~</span> <span class="fu">s</span>(CRBI,gam.df) <span class="sc">+</span> <span class="fu">s</span>(CRuns,gam.df) <span class="sc">+</span> <span class="fu">s</span>(CHits,gam.df) <span class="sc">+</span> <span class="fu">s</span>(CAtBat,gam.df)</span>
<span id="cb325-5"><a href="tree-based-methods.html#cb325-5" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">data =</span> df.train)</span>
<span id="cb325-6"><a href="tree-based-methods.html#cb325-6" aria-hidden="true" tabindex="-1"></a>preds.gam <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.gam,<span class="at">newdata =</span> df.test)</span>
<span id="cb325-7"><a href="tree-based-methods.html#cb325-7" aria-hidden="true" tabindex="-1"></a>mse.gam2 <span class="ot">&lt;-</span> <span class="fu">mean</span>((preds.gam <span class="sc">-</span> df.test<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>Now we see, that the model does improve, and gam is actually performing better than Boosting.</p>
<p><strong>(g) Applying bagging</strong></p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="tree-based-methods.html#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb326-2"><a href="tree-based-methods.html#cb326-2" aria-hidden="true" tabindex="-1"></a>fit.bag <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Salary <span class="sc">~</span> .,<span class="at">data =</span> df.train</span>
<span id="cb326-3"><a href="tree-based-methods.html#cb326-3" aria-hidden="true" tabindex="-1"></a>                        ,<span class="at">mtry =</span> <span class="fu">length</span>(df.train)<span class="sc">-</span><span class="dv">1</span>) <span class="co">#No. of predictors</span></span>
<span id="cb326-4"><a href="tree-based-methods.html#cb326-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-5"><a href="tree-based-methods.html#cb326-5" aria-hidden="true" tabindex="-1"></a>preds.bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.bag,<span class="at">newdata =</span> df.test)</span>
<span id="cb326-6"><a href="tree-based-methods.html#cb326-6" aria-hidden="true" tabindex="-1"></a>mse.bag <span class="ot">&lt;-</span> <span class="fu">mean</span>((preds.bag <span class="sc">-</span> df.test<span class="sc">$</span>Salary)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p><strong>Comparison</strong></p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="tree-based-methods.html#cb327-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setNames</span>(<span class="fu">as.data.frame</span>(<span class="fu">rbind</span>(mse.boost</span>
<span id="cb327-2"><a href="tree-based-methods.html#cb327-2" aria-hidden="true" tabindex="-1"></a>                             ,mse.lin</span>
<span id="cb327-3"><a href="tree-based-methods.html#cb327-3" aria-hidden="true" tabindex="-1"></a>                             ,mse.gam</span>
<span id="cb327-4"><a href="tree-based-methods.html#cb327-4" aria-hidden="true" tabindex="-1"></a>                             ,mse.gam2</span>
<span id="cb327-5"><a href="tree-based-methods.html#cb327-5" aria-hidden="true" tabindex="-1"></a>                             ,mse.bag)),<span class="at">nm =</span> <span class="st">&quot;MSE&quot;</span>)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mse.boost</td>
<td align="right">0.2538902</td>
</tr>
<tr class="even">
<td align="left">mse.lin</td>
<td align="right">0.4917959</td>
</tr>
<tr class="odd">
<td align="left">mse.gam</td>
<td align="right">0.3129716</td>
</tr>
<tr class="even">
<td align="left">mse.gam2</td>
<td align="right">0.2750028</td>
</tr>
<tr class="odd">
<td align="left">mse.bag</td>
<td align="right">0.2281286</td>
</tr>
</tbody>
</table>
</div>
<p>We see that the bagging model is actually superior to all models.</p>
</div>
<div id="exercise-11---boosting" class="section level3" number="3.6.5">
<h3><span class="header-section-number">3.6.5</span> Exercise 11 - Boosting</h3>
<p><strong>(a) Loading and partitioning</strong></p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="tree-based-methods.html#cb328-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Caravan</span>
<span id="cb328-2"><a href="tree-based-methods.html#cb328-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>Purchase <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(df<span class="sc">$</span>Purchase <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>,<span class="at">yes =</span> <span class="dv">1</span>,<span class="at">no =</span> <span class="dv">0</span>)</span>
<span id="cb328-3"><a href="tree-based-methods.html#cb328-3" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,]</span>
<span id="cb328-4"><a href="tree-based-methods.html#cb328-4" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span><span class="dv">1</span><span class="sc">:-</span><span class="dv">1000</span>,]</span></code></pre></div>
<p><strong>(b) Boosting</strong></p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="tree-based-methods.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df<span class="sc">$</span>Purchase)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-224-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the response vector is a binary vector. It is also listed as a factor.</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="tree-based-methods.html#cb330-1" aria-hidden="true" tabindex="-1"></a>fit.boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Purchase <span class="sc">~</span> .</span>
<span id="cb330-2"><a href="tree-based-methods.html#cb330-2" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">data =</span> df.train</span>
<span id="cb330-3"><a href="tree-based-methods.html#cb330-3" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">n.trees =</span> <span class="dv">1000</span></span>
<span id="cb330-4"><a href="tree-based-methods.html#cb330-4" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">shrinkage =</span> <span class="fl">0.01</span></span>
<span id="cb330-5"><a href="tree-based-methods.html#cb330-5" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">distribution =</span> <span class="st">&quot;bernoulli&quot;</span> <span class="co">#Because we have binary outcome</span></span>
<span id="cb330-6"><a href="tree-based-methods.html#cb330-6" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb330-7"><a href="tree-based-methods.html#cb330-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.boost)</span></code></pre></div>
<img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-225-1.png" width="720" style="display: block; margin: auto;" />
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">var</th>
<th align="right">rel.inf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">PPERSAUT</td>
<td align="left">PPERSAUT</td>
<td align="right">14.3295534</td>
</tr>
<tr class="even">
<td align="left">MKOOPKLA</td>
<td align="left">MKOOPKLA</td>
<td align="right">9.9736081</td>
</tr>
<tr class="odd">
<td align="left">MOPLHOOG</td>
<td align="left">MOPLHOOG</td>
<td align="right">7.5872449</td>
</tr>
<tr class="even">
<td align="left">MBERMIDD</td>
<td align="left">MBERMIDD</td>
<td align="right">5.7808384</td>
</tr>
<tr class="odd">
<td align="left">PBRAND</td>
<td align="left">PBRAND</td>
<td align="right">5.6205711</td>
</tr>
<tr class="even">
<td align="left">MGODGE</td>
<td align="left">MGODGE</td>
<td align="right">4.3738956</td>
</tr>
<tr class="odd">
<td align="left">ABRAND</td>
<td align="left">ABRAND</td>
<td align="right">4.0703606</td>
</tr>
<tr class="even">
<td align="left">MINK3045</td>
<td align="left">MINK3045</td>
<td align="right">3.7510230</td>
</tr>
<tr class="odd">
<td align="left">MOSTYPE</td>
<td align="left">MOSTYPE</td>
<td align="right">2.7040406</td>
</tr>
<tr class="even">
<td align="left">MSKC</td>
<td align="left">MSKC</td>
<td align="right">2.6133035</td>
</tr>
<tr class="odd">
<td align="left">MSKA</td>
<td align="left">MSKA</td>
<td align="right">2.4577241</td>
</tr>
<tr class="even">
<td align="left">PWAPART</td>
<td align="left">PWAPART</td>
<td align="right">2.3120642</td>
</tr>
<tr class="odd">
<td align="left">MAUT1</td>
<td align="left">MAUT1</td>
<td align="right">2.2603361</td>
</tr>
<tr class="even">
<td align="left">MAUT2</td>
<td align="left">MAUT2</td>
<td align="right">2.1396541</td>
</tr>
<tr class="odd">
<td align="left">MGODPR</td>
<td align="left">MGODPR</td>
<td align="right">2.0178413</td>
</tr>
<tr class="even">
<td align="left">MBERHOOG</td>
<td align="left">MBERHOOG</td>
<td align="right">1.9200909</td>
</tr>
<tr class="odd">
<td align="left">MGODOV</td>
<td align="left">MGODOV</td>
<td align="right">1.7982818</td>
</tr>
<tr class="even">
<td align="left">PBYSTAND</td>
<td align="left">PBYSTAND</td>
<td align="right">1.7594279</td>
</tr>
<tr class="odd">
<td align="left">MSKB1</td>
<td align="left">MSKB1</td>
<td align="right">1.7450868</td>
</tr>
<tr class="even">
<td align="left">MINKGEM</td>
<td align="left">MINKGEM</td>
<td align="right">1.7126814</td>
</tr>
<tr class="odd">
<td align="left">MBERARBG</td>
<td align="left">MBERARBG</td>
<td align="right">1.5861912</td>
</tr>
<tr class="even">
<td align="left">MRELGE</td>
<td align="left">MRELGE</td>
<td align="right">1.5142353</td>
</tr>
<tr class="odd">
<td align="left">MINK7512</td>
<td align="left">MINK7512</td>
<td align="right">1.5015654</td>
</tr>
<tr class="even">
<td align="left">MFWEKIND</td>
<td align="left">MFWEKIND</td>
<td align="right">1.4075311</td>
</tr>
<tr class="odd">
<td align="left">MRELOV</td>
<td align="left">MRELOV</td>
<td align="right">1.3362888</td>
</tr>
<tr class="even">
<td align="left">APERSAUT</td>
<td align="left">APERSAUT</td>
<td align="right">0.9343820</td>
</tr>
<tr class="odd">
<td align="left">MSKD</td>
<td align="left">MSKD</td>
<td align="right">0.8297606</td>
</tr>
<tr class="even">
<td align="left">MINKM30</td>
<td align="left">MINKM30</td>
<td align="right">0.8041630</td>
</tr>
<tr class="odd">
<td align="left">MBERBOER</td>
<td align="left">MBERBOER</td>
<td align="right">0.7576037</td>
</tr>
<tr class="even">
<td align="left">MHHUUR</td>
<td align="left">MHHUUR</td>
<td align="right">0.7320049</td>
</tr>
<tr class="odd">
<td align="left">MOSHOOFD</td>
<td align="left">MOSHOOFD</td>
<td align="right">0.7028847</td>
</tr>
<tr class="even">
<td align="left">MINK4575</td>
<td align="left">MINK4575</td>
<td align="right">0.7028427</td>
</tr>
<tr class="odd">
<td align="left">MAUT0</td>
<td align="left">MAUT0</td>
<td align="right">0.6648342</td>
</tr>
<tr class="even">
<td align="left">MGODRK</td>
<td align="left">MGODRK</td>
<td align="right">0.6249523</td>
</tr>
<tr class="odd">
<td align="left">MOPLMIDD</td>
<td align="left">MOPLMIDD</td>
<td align="right">0.6170707</td>
</tr>
<tr class="even">
<td align="left">MZPART</td>
<td align="left">MZPART</td>
<td align="right">0.4735141</td>
</tr>
<tr class="odd">
<td align="left">PMOTSCO</td>
<td align="left">PMOTSCO</td>
<td align="right">0.4727968</td>
</tr>
<tr class="even">
<td align="left">MHKOOP</td>
<td align="left">MHKOOP</td>
<td align="right">0.4639800</td>
</tr>
<tr class="odd">
<td align="left">MFGEKIND</td>
<td align="left">MFGEKIND</td>
<td align="right">0.4436668</td>
</tr>
<tr class="even">
<td align="left">MZFONDS</td>
<td align="left">MZFONDS</td>
<td align="right">0.3718382</td>
</tr>
<tr class="odd">
<td align="left">PLEVEN</td>
<td align="left">PLEVEN</td>
<td align="right">0.3357793</td>
</tr>
<tr class="even">
<td align="left">MGEMLEEF</td>
<td align="left">MGEMLEEF</td>
<td align="right">0.3116748</td>
</tr>
<tr class="odd">
<td align="left">MINK123M</td>
<td align="left">MINK123M</td>
<td align="right">0.2425446</td>
</tr>
<tr class="even">
<td align="left">MBERARBO</td>
<td align="left">MBERARBO</td>
<td align="right">0.2302805</td>
</tr>
<tr class="odd">
<td align="left">MRELSA</td>
<td align="left">MRELSA</td>
<td align="right">0.2213198</td>
</tr>
<tr class="even">
<td align="left">MSKB2</td>
<td align="left">MSKB2</td>
<td align="right">0.2203454</td>
</tr>
<tr class="odd">
<td align="left">MFALLEEN</td>
<td align="left">MFALLEEN</td>
<td align="right">0.1905751</td>
</tr>
<tr class="even">
<td align="left">MGEMOMV</td>
<td align="left">MGEMOMV</td>
<td align="right">0.1364351</td>
</tr>
<tr class="odd">
<td align="left">MBERZELF</td>
<td align="left">MBERZELF</td>
<td align="right">0.0660080</td>
</tr>
<tr class="even">
<td align="left">MOPLLAAG</td>
<td align="left">MOPLLAAG</td>
<td align="right">0.0634171</td>
</tr>
<tr class="odd">
<td align="left">PAANHANG</td>
<td align="left">PAANHANG</td>
<td align="right">0.0565708</td>
</tr>
<tr class="even">
<td align="left">MAANTHUI</td>
<td align="left">MAANTHUI</td>
<td align="right">0.0553153</td>
</tr>
<tr class="odd">
<td align="left">PWABEDR</td>
<td align="left">PWABEDR</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PWALAND</td>
<td align="left">PWALAND</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">PBESAUT</td>
<td align="left">PBESAUT</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PVRAAUT</td>
<td align="left">PVRAAUT</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">PTRACTOR</td>
<td align="left">PTRACTOR</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PWERKT</td>
<td align="left">PWERKT</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">PBROM</td>
<td align="left">PBROM</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PPERSONG</td>
<td align="left">PPERSONG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">PGEZONG</td>
<td align="left">PGEZONG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PWAOREG</td>
<td align="left">PWAOREG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">PZEILPL</td>
<td align="left">PZEILPL</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PPLEZIER</td>
<td align="left">PPLEZIER</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">PFIETS</td>
<td align="left">PFIETS</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">PINBOED</td>
<td align="left">PINBOED</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AWAPART</td>
<td align="left">AWAPART</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">AWABEDR</td>
<td align="left">AWABEDR</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AWALAND</td>
<td align="left">AWALAND</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">ABESAUT</td>
<td align="left">ABESAUT</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AMOTSCO</td>
<td align="left">AMOTSCO</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">AVRAAUT</td>
<td align="left">AVRAAUT</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AAANHANG</td>
<td align="left">AAANHANG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">ATRACTOR</td>
<td align="left">ATRACTOR</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AWERKT</td>
<td align="left">AWERKT</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">ABROM</td>
<td align="left">ABROM</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">ALEVEN</td>
<td align="left">ALEVEN</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">APERSONG</td>
<td align="left">APERSONG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AGEZONG</td>
<td align="left">AGEZONG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">AWAOREG</td>
<td align="left">AWAOREG</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AZEILPL</td>
<td align="left">AZEILPL</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">APLEZIER</td>
<td align="left">APLEZIER</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">AFIETS</td>
<td align="left">AFIETS</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">AINBOED</td>
<td align="left">AINBOED</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">ABYSTAND</td>
<td align="left">ABYSTAND</td>
<td align="right">0.0000000</td>
</tr>
</tbody>
</table>
</div>
<p>We see that PPERSAUT, MKOOPLKA and MOPLHOOG are specifically important wher the rest gradually decreases. We also see a lot of variables that are of no importance.</p>
<p><strong>(d) Predicting posterior probabilities</strong></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="tree-based-methods.html#cb331-1" aria-hidden="true" tabindex="-1"></a>probs.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.boost,<span class="at">newdata =</span> df.test</span>
<span id="cb331-2"><a href="tree-based-methods.html#cb331-2" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">type =</span> <span class="st">&quot;response&quot;</span> <span class="co">#Response yields posterior probabilities</span></span>
<span id="cb331-3"><a href="tree-based-methods.html#cb331-3" aria-hidden="true" tabindex="-1"></a>                       ,<span class="at">n.trees =</span> <span class="dv">1000</span>)</span>
<span id="cb331-4"><a href="tree-based-methods.html#cb331-4" aria-hidden="true" tabindex="-1"></a>preds.boost <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs.boost <span class="sc">&gt;</span> <span class="fl">0.2</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb331-5"><a href="tree-based-methods.html#cb331-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> <span class="fu">as.factor</span>(preds.boost)</span>
<span id="cb331-6"><a href="tree-based-methods.html#cb331-6" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">reference =</span> <span class="fu">as.factor</span>(df.test<span class="sc">$</span>Purchase)</span>
<span id="cb331-7"><a href="tree-based-methods.html#cb331-7" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">positive =</span> <span class="st">&quot;1&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 4412  256
##          1  121   33
##                                           
##                Accuracy : 0.9218          
##                  95% CI : (0.9139, 0.9292)
##     No Information Rate : 0.9401          
##     P-Value [Acc &gt; NIR] : 1               
##                                           
##                   Kappa : 0.112           
##                                           
##  Mcnemar&#39;s Test P-Value : 5.151e-12       
##                                           
##             Sensitivity : 0.114187        
##             Specificity : 0.973307        
##          Pos Pred Value : 0.214286        
##          Neg Pred Value : 0.945159        
##              Prevalence : 0.059934        
##          Detection Rate : 0.006844        
##    Detection Prevalence : 0.031937        
##       Balanced Accuracy : 0.543747        
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<p>We see that we have a high accuracy, but the sensitivity (true positive) is really low. We can check the null rate.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="tree-based-methods.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(df.test<span class="sc">$</span>Purchase)[<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">table</span>(df.test<span class="sc">$</span>Purchase))</span></code></pre></div>
<pre><code>##         0 
## 0.9400664</code></pre>
<p>We see that the prior probability for 0 (no purchase) is 94%, hence if we predicted everyone to not buy, then the overall accuracy would be better, although the sensitivity would be 0.</p>
<p>We see that <span class="math inline">\(258+31 = 289\)</span> and we predicted 31 to make a purchase. Hence we cought <span class="math inline">\(31/289*100=10.7%\)</span>, i.e., the sensitivity (true positive rate).</p>
<p>We can now try to use KNN to see how that performs.</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="tree-based-methods.html#cb335-1" aria-hidden="true" tabindex="-1"></a><span class="co">#KNN</span></span>
<span id="cb335-2"><a href="tree-based-methods.html#cb335-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb335-3"><a href="tree-based-methods.html#cb335-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb335-4"><a href="tree-based-methods.html#cb335-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Constructing Model Matrix to remove y</span></span>
<span id="cb335-5"><a href="tree-based-methods.html#cb335-5" aria-hidden="true" tabindex="-1"></a>mm.train <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Purchase <span class="sc">~</span> .,<span class="at">data =</span> df.train)[,<span class="sc">-</span><span class="dv">1</span>] <span class="co">#Removing intercept</span></span>
<span id="cb335-6"><a href="tree-based-methods.html#cb335-6" aria-hidden="true" tabindex="-1"></a>mm.test <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Purchase <span class="sc">~</span> .,<span class="at">data =</span> df.test)[,<span class="sc">-</span><span class="dv">1</span>] <span class="co">#Removing intercept</span></span>
<span id="cb335-7"><a href="tree-based-methods.html#cb335-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb335-8"><a href="tree-based-methods.html#cb335-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Making the predictions</span></span>
<span id="cb335-9"><a href="tree-based-methods.html#cb335-9" aria-hidden="true" tabindex="-1"></a>knn.pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> mm.train</span>
<span id="cb335-10"><a href="tree-based-methods.html#cb335-10" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">test =</span> mm.test</span>
<span id="cb335-11"><a href="tree-based-methods.html#cb335-11" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">cl =</span> df.train<span class="sc">$</span>Purchase</span>
<span id="cb335-12"><a href="tree-based-methods.html#cb335-12" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">k =</span> <span class="dv">2</span>) <span class="co">#Tested a few, 2 appear to be best</span></span>
<span id="cb335-13"><a href="tree-based-methods.html#cb335-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb335-14"><a href="tree-based-methods.html#cb335-14" aria-hidden="true" tabindex="-1"></a><span class="co">#Confusion matrix</span></span>
<span id="cb335-15"><a href="tree-based-methods.html#cb335-15" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> <span class="fu">as.factor</span>(knn.pred)</span>
<span id="cb335-16"><a href="tree-based-methods.html#cb335-16" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">reference =</span> <span class="fu">as.factor</span>(df.test<span class="sc">$</span>Purchase)</span>
<span id="cb335-17"><a href="tree-based-methods.html#cb335-17" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">positive =</span> <span class="st">&quot;1&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 4275  266
##          1  258   23
##                                        
##                Accuracy : 0.8913       
##                  95% CI : (0.8822, 0.9)
##     No Information Rate : 0.9401       
##     P-Value [Acc &gt; NIR] : 1.0000       
##                                        
##                   Kappa : 0.023        
##                                        
##  Mcnemar&#39;s Test P-Value : 0.7598       
##                                        
##             Sensitivity : 0.07958      
##             Specificity : 0.94308      
##          Pos Pred Value : 0.08185      
##          Neg Pred Value : 0.94142      
##              Prevalence : 0.05993      
##          Detection Rate : 0.00477      
##    Detection Prevalence : 0.05827      
##       Balanced Accuracy : 0.51133      
##                                        
##        &#39;Positive&#39; Class : 1            
## </code></pre>
<p>We see that the sensitivity is higher for the KNN model.</p>
</div>
<div id="exercise-12" class="section level3" number="3.6.6">
<h3><span class="header-section-number">3.6.6</span> Exercise 12</h3>
<p><strong>Description</strong></p>
<p>Apply boosting, bagging, and random forests to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?</p>
<p><strong>Not done</strong></p>
</div>
</div>
<div id="casestudy" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Casestudy</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moving-beyond-linearity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="support-vector-machines.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
