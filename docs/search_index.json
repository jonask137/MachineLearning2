[["index.html", "Machine Learning for Business Intelligence 2 setup", " Machine Learning for Business Intelligence 2 setup All the output formatting is done in the _output.yml file while all bookdown rendering options is done in the _bookdown.yml file.§ library(bookdown) Tips and tricks Referencing This is making a figure with a caption and where we center the figure and set the size. Notice, that the figure is automatically numbered according to the chapter number. plot(pressure,type = &#39;b&#39;,pch = 19) Figure 0.1: A fig Now we are able to make a reference to the chunk. Refer to a chunk: 0.1 Options RMarkdown Reference Guide "],["introduction.html", "1 Introduction", " 1 Introduction library(dplyr) "],["moving-beyond-linearity.html", "2 Moving Beyond Linearity", " 2 Moving Beyond Linearity Literature: Moving Beyond Linearity (ISL CH7) Recall that complexity = also means lower interpretibility. This subject extents the linear models with the following: Polunomial Regression - where polynomials of the variables are added. Step Functions - where the x range is cut into k distinct regions to produce a qualitative variable. Hence also the name, piecewise constant function. Regression Splines - a combination / extensions of number one and two. Where polynomials functions are applied in specified regions of an X range. Smoothing Splines - Similar to the one above, but slightly different in the fitting process. Local Regression - Similar to regression splines, but these are able to overlap. Generalized Additive Models - allows to extent the model with several predictors. "],["models-beyond-linearity.html", "2.1 Models Beyond Linearity", " 2.1 Models Beyond Linearity Notice that all approaches despite GAM are extensions of simple linear regression, as it only takes on one predictor variable. 2.1.1 Polynomial Regression Can be defined by the following \\[\\begin{equation} y_{i\\ }=\\ \\beta_0+\\beta_1x_i+\\beta_2x_i^2+...+\\ \\beta_dx_i^d\\ +\\ \\epsilon_i \\tag{2.1} \\end{equation}\\] Rules of thumb: We don’t take on more than 3 or 4 degrees of d, as that yields strange lines Note that we can still use standard errors for coefficient estimates. 2.1.1.1 Beta coefficients and variance Each beta coefficient has its own variance (just as in linear regression). It can be defined by a matrix of j dimensions, e.g., if you have 5 betas (including beta 0) we can construct the correlation matrix. Covariance matrix can be identified by \\(\\hat{C}\\). Generally we get point estimates, but it is also interesting to show the confidence intervals (using 2 standard errors). Notice, that we cant really interprete beta coefficients as we do with linear regression, hence we dont have the same ability to do inference as the coefficients are missleading. 2.1.1.2 Application procedure Use lm() or glm() Use DV~poly(IV,degree) Perform CV with cv.glment() / aonva F-test to select degree This is basically either visually selecting the degrees that are the best using CV or using an ANOVA to assess if the MSE are significantly different from each other, hence an ANOVA test. Regarding ANOVA, if there stop being significance, then significant changes, e.g., in a poly 8, where the previous polynomials was not significant, then we can also disregard the 8’th polynomial. Fit the selected model Look at coef(summary(fit)) Plot data and predictions with predict() Check residuals Interpret The lecture shows exercise number 6 in chapter 7. 2.1.2 Step Functions This is literally just fitting a constant in different bins, see example on page 269. This is also called discretizing x. It is often applied when we see, e.g., five year age bins, e.g., a 20-25 year old is expected to earn so and so much etc. But we can only really use it, when there are natural cuts, hence one must be considerate using the model. Major disadvantage: If there are no natural breakpoints, then the model is likely to miss variance and also generalize too much. Remember, that the steps reflect the average increase in Y for each step. Hence the first bin (range) is defined by \\(\\beta_0\\), and can be regarded as the average of that range of x. Thus each coefficient of the ranges of x is to be understood as the average increase of response. In other words, \\(\\beta_0\\) is the reference level, where the following cuts reflect the average increase or decrease hereon. Note that we can still use standard errors for coefficient estimates. 2.1.3 Regression Splines 2.1.3.1 Piecewise Polynomials This is basically polynomial regression, where the coefficients apply to specified ranges of X. The points where the coefficients changes are called knots. Hence a cubic function will look like the following. Notice, that a piecewise polynomial function with no knots, is merely a standard polynomial function. \\[\\begin{equation} y_{i\\ }=\\ \\beta_0+\\beta_1x_i+\\beta_2x_i^2+\\ \\beta_3x_i^3\\ +\\ \\epsilon_i \\tag{2.2} \\end{equation}\\] Where it can be extended to be written for each range. The coefficients can then be written with: \\(\\beta_{01},\\beta_{11},\\beta_{21}\\) etc. and for the second set: \\(\\beta_{02},\\beta_{12},\\beta_{22}\\) And for each of the betas in all of the cuts, you add one degree of freedom. Rule of thumb: The more knots, the more complex = the more variance capture and noise trailed, hence low model bias but large model variance. Figure 2.1: Piecewise Polynomials 2.1.3.2 Constraints and Splines Figure 7.3 show how the splines look, as the top left window tell, the jump is rather odd. Hence, we can force the fit to be continuous, by imposing a constraint. Notice, that it is a piecewise polynomial regression, when you merely fit polynomials onto bins of data. If you want to have the ends tied together, you impose contraints and thus created a spline. We can further constrain the model, with adding derivatives of the functions, hence the first derivative and the second derivative (where it in this instance created linear splines, that is because we have added \\(d-1\\) i.e. 2 derivatives, if the function was to the power of 4, then we should have imposed 3 derivatives to achieve linearity in the splines.) Hence, the linear spline can be defined by: It is piecewise splines of degree-d polynomials, with continuity in derivatives up to degree \\(d-1\\) at each knot. Hence we have the following constraints: Continuity Derivatives 2.1.3.3 Choosing the number and location of the Knots Choosing amount of knots? One may ask themself, how many degrees of freedom do you want to include in the model? Amount of knots is therefore corresponding to amount of degrees of freedom. We can let software estimate the best amount and the best locations. Here one can use: In-sample performance Out-of-sample performance, e.g. with CV, perhaps extent to K folds with K tests, to ensure, that each variable has been held out once. This can be followed by visualizing the MSE for the different simulations with different amount of knots. 2.1.3.4 Comparison with Polynomial Regression With regression splines we are able to introduce knots that account for variance as it slightly resets the model in each section, hence we can fit the model to the data without having to impose as much complexity as we would in normal polynomial regression. Hence one often observes that regression splines have more stability than polynomial regression. 2.1.4 Smoothing Splines In prediction this is slightly better than basic functions and natural splines. This is basically attempting to find a model that captures the variance by a smoothing line. Doing so, we fit a very flexible model and impose restrictions upon this, to achieve a shrunken model, just as with Lasso and Ridge Regression. Thus, the smoothing (imposing restrictions) deals with overfitting. Also as we havee discovered previously, that degrees of freedom is equivilant with amount of knots, e.g., in polynomial splines, then three knots in a cubic function leads to 6 degrees of freedom, hence an smooth spline with df = 6.8 can be said to approximately have 3 knots, but we will never really know. This can be defined as a cubic spline with knot at every unique value of \\(x_i\\) Hence we have the following model: \\[\\begin{equation} RSS=\\sum_{i=1}^n\\left(y_i-g\\left(x_i\\right)\\right)^{^2}+\\lambda\\int g&#39;&#39;\\left(t\\right)^{^2}dt \\tag{2.3} \\end{equation}\\] i.e. Loss + Penalty Where: We define model g(x) \\((y_i-g(x_i))^{^2}\\) = the loss, meaning the difference between the fitted model and the actual y’s \\(\\lambda\\) = the tuning parameter, hence the restriction that we want to impose. If lambda is low, then much flexibility, if lambda is high, then low flexibility. Hence, controls the bias variance tradeoff. \\(\\int g&#39;&#39;\\left(t\\right)^{^2}dt\\) = a measure of how much \\(g&#39;(x)\\) changes oer time. Hence, the higher we set \\(\\lambda\\) the more imposed restrictions, meaning the smoother the model, as lambda gets closer to infinity, the model becomes linear. 2.1.4.1 Choosing optimal tuning parameter The analytic LOOCV can be calculated, the procedure appears to be the same as for lasso and ridge regression. The book (page 279) describes this a bit in details. However it says that software is able to do this. Basically what is done, is LOOCV and simulating different tuning parameters to assess what model that performs the best. 2.1.5 Local Regression This i basically fitting a linear regression to each x, where s observations are included in the fitting procedure. Thus, one creates several fits, that are based on the observations weighted, where observations close to \\(x_0\\) (the center of the regression) are given the highest weight and then the weight is gradually decreasing. This is often really good when you have outliers, as you define how big a neighborhood you want to evaluate (also called the span, e.g. span of 0.5 = 50% of the observations). This can be visualized with: Figure 2.2: Local regression Doing local regression has the following procedure (algorithm) Gather the fraction \\(s = k/n\\) of training points whose \\(x_i\\) are closest to \\(x_0\\). Assign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point in this neighborhood, so that the point furthest from x0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero. Fit a weighted least squares regression of the \\(y_i\\) on the \\(x_i\\) using the aforementioned weights, by finding \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize \\[\\begin{equation} \\sum_{i=1}^nK_{i0}\\left(y_i-\\beta_0-\\beta_1x_i\\right)^{^2} \\tag{2.4} \\end{equation}\\] The fitted value at \\(x_0\\) is given by \\(\\hat{f}(x_0)=\\hat\\beta_0+\\hat\\beta_1x_0\\) Where we see how the model is 2.1.6 Generalized Additive Models This can naturally both be applied in regression and classification problems, futher elaborated in the following. It is called generalized, as the dependent variable can be both continuous (e.g. Gaussian) and categorical (e.g., binomial, Poisson, or other distributions) distributed Additive = the model is adding different polynomials of the IDV toghether. Notice, as the model is additive, it does not account for interactions, then you have to specify the interactions. Thus GAM is merely an approach to make a model, where we include the posibility of having non linear components. Hence we include more complex model (with the ability to trail the observations more than linear models). But the advantage of linear regressions, are that we are able to quickly deduct the effects the variables. Although we dont always have a linear relationship, hence you can be forced to choose a more complex model. (see the R file “GAMs with discussion R”). We have previously worked with non parametric models (e.g., KNN regression). GAM is in between linear regression and non parametric models. That is the beuty of GAMs, as we preserve the ability of having transparancy in the model, despite it coming at a cost of worse prediction power than neural networks, but at such complex models, you are not able to deduct how the variables are interrelated, you can only say which are important and which are not. 2.1.6.1 GAM for regression problems Now we move beyond being constrained to only one predictor variable, hence GAM can be seen more as an extension of multiple linear regression. Hence GAM is a combination of different functions, where they are each fitted while holding the other variables fixed. GAM can consist of any different non-linear model, e.g., we can just as well use local regression, polynomial regression, or any combination of the approaches seen above in this subsection. See section @ref(fig:GAMPlotLab7.8.3) for explanation of interpretation of the plots. Disadvantages of GAM: The fitting procedure holds the other variables fixed, hence it does not count for interactions. Therefore, one may manually construct interaction variables to account for this, just like in mulitple linear regression. Prediction wise it is not competitive with Neural Networks and Support Vector Machines. Advantages of GAM: Allowing to fit non linear function for j variables (\\(f_j\\)) Has potential of making more accurate predictions As the model is additive (meaning that each function is fitted holding the other variables fixed) we are still able to make inference, e.g., assessing how one variable affects the y variable. Smoothness of function \\(f_j\\) can be summarized with degrees of freedom. Often applied when aiming for explanatory analysis (instead of prediction) 2.1.6.2 GAM for classification problems When y is qualitative (categorical), GAM can also be applied in the logistical form. As discovered in the classification section, we can apply logits (log of odds) and odds, see material from first semester. The same advantages and disadvantages as in the prior section applies. "],["lecture-notes.html", "2.2 Lecture notes", " 2.2 Lecture notes Talking about polynomials and what they are, e.g., can be parabula, etc. With splines we set polynomials in each of the X regions. Where just polynomial regression is fitted to the whole dataset, and not just in regions. By default poly() will make orthogonal polynomials. Meaning that it tries to create orthogonal terms, where the polynomials are orthogonal (not related to each other). As that is default, then we have to define, that we want to use raw data, hence raw = TRUE, hence we will get the regular polynomials of the dataa. See notes in her R file. General Wrap-Up The approaches are similar Must be aware of why the models are used Try them out Assess how they look "],["lab-section.html", "2.3 Lab section", " 2.3 Lab section Loading the data that will be used throughout the lab section. library(ISLR) attach(Wage) df &lt;- Wage 2.3.1 Polynomial Regression and Step Functions 2.3.1.1 Continous model Fitting the model: fit &lt;- lm(wage ~ poly(age,4) #Orthogonal polynomials ,data = df) fit2 &lt;- lm(wage ~ poly(age,4,raw = TRUE) #Orthogonal polynomials ,data = df) Note: poly() returns orthogonal polynomials, which is some linear combination of the variables to the d power. See the following two examples when using orthogonal and normal polynomials: { print(&quot;Orthogonal&quot;) cbind(df$age,poly(x = df$age,degree = 4))[1:5,] %&gt;% print() print(&quot;Regular&quot;) cbind(df$age,poly(x = df$age,degree = 4,raw = TRUE))[1:5,] %&gt;% print() } ## [1] &quot;Orthogonal&quot; ## 1 2 3 4 ## [1,] 18 -0.0386247992 0.055908727 -0.0717405794 0.08672985 ## [2,] 24 -0.0291326034 0.026298066 -0.0145499511 -0.00259928 ## [3,] 45 0.0040900817 -0.014506548 -0.0001331835 0.01448009 ## [4,] 43 0.0009260164 -0.014831404 0.0045136682 0.01265751 ## [5,] 50 0.0120002448 -0.009815846 -0.0111366263 0.01021146 ## [1] &quot;Regular&quot; ## 1 2 3 4 ## [1,] 18 18 324 5832 104976 ## [2,] 24 24 576 13824 331776 ## [3,] 45 45 2025 91125 4100625 ## [4,] 43 43 1849 79507 3418801 ## [5,] 50 50 2500 125000 6250000 In the end, it does not have a noticeable effect. options(scipen = 5) { coef(summary(fit)) %&gt;% print() coef(summary(fit2)) %&gt;% print() } ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.70361 0.7287409 153.283015 0.000000e+00 ## poly(age, 4)1 447.06785 39.9147851 11.200558 1.484604e-28 ## poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32 ## poly(age, 4)3 125.52169 39.9147851 3.144742 1.678622e-03 ## poly(age, 4)4 -77.91118 39.9147851 -1.951938 5.103865e-02 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -184.1541797743 60.04037718327 -3.067172 0.0021802539 ## poly(age, 4, raw = TRUE)1 21.2455205321 5.88674824448 3.609042 0.0003123618 ## poly(age, 4, raw = TRUE)2 -0.5638593126 0.20610825640 -2.735743 0.0062606446 ## poly(age, 4, raw = TRUE)3 0.0068106877 0.00306593115 2.221409 0.0263977518 ## poly(age, 4, raw = TRUE)4 -0.0000320383 0.00001641359 -1.951938 0.0510386498 Even though the coefficients are different and the p-values hereof, the fitted values will be indistinguishable (Hastie et al. 2013, 288). This is also shown later. Alternatives to using poly()?? We have two alternatives: Using I() Using cbind() Using I() fit2a &lt;- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4) #Note that &#39;I()&#39; is added ,data = df) coef(fit2a) ## (Intercept) age I(age^2) I(age^3) I(age^4) ## -184.1541797743 21.2455205321 -0.5638593126 0.0068106877 -0.0000320383 Notice I() as ‘^’ has another special meaning in formulas Hence we see that the coefficients are the same. Using cbind() fit2b &lt;- lm(wage ~ cbind(age,age^2,age^3,age^4) ,data = df) coef(fit2b) ## (Intercept) cbind(age, age^2, age^3, age^4)age ## -184.1541797743 21.2455205321 ## cbind(age, age^2, age^3, age^4) cbind(age, age^2, age^3, age^4) ## -0.5638593126 0.0068106877 ## cbind(age, age^2, age^3, age^4) ## -0.0000320383 We see that we are now able to use ‘^’ within the cbind(). proceeding with the lab sections. We can now present a grid of values for age, at which we want predictions and then call the predict() and also plot the standard errors. agelims &lt;- range(df$age) #The min and max age.grid &lt;- seq(from = agelims[1],to = agelims[2]) #Creating a counter within the range preds &lt;- predict(object = fit ,newdata = list(age = age.grid) #Creating a list with the counter named age, so it fits the IV naming ,se.fit = TRUE) se.bands &lt;- cbind(preds$fit + 2*preds$se.fit #Upper band ,preds$fit-2*preds$se.fit) #Lower band Notice that 2 SE (2 sd) = 95%, hence we expect to contain 95% of the data within confidence levels. Now we can plot the data plot(x = df$age,y = df$wage ,xlim = agelims ,cex = 0.5 #Size of dots ,col = &quot;darkgrey&quot;) title(&quot;Degree-4 Polynomial&quot;,outer = TRUE) lines(x = age.grid,y = preds$fit ,lwd = 2 ,col = &quot;blue&quot;) Comparison between raw polynomials and orthogonal polynomials With the following we see that the difference of the fitted values are practically 0. preds2 &lt;- predict(object = fit2 ,newdata = list(age = age.grid) ,se.fit = TRUE) max(abs(preds$fit-preds2$fit)) ## [1] 7.81597e-11 In terms of predictions, the two approaches are more or less the same, although the orthogonal polynomials removes some effect of collinearity. Assessing what polynomial to include Now we can compare models with different orthogonal polynomials. Using ANOVA, which compare the RSS to see if the decrease in RSS is significant. fit.1 &lt;- lm(wage~age,data=df) fit.2 &lt;- lm(wage~poly(df$age,2),data=df) fit.3 &lt;- lm(wage~poly(df$age,3),data=df) fit.4 &lt;- lm(wage~poly(df$age,4),data=df) fit.5 &lt;- lm(wage~poly(df$age,5),data=df) anova(fit.1,fit.2,fit.3,fit.4,fit.5) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 2998 5022216 NA NA NA NA 2997 4793430 1 228786.010 143.5931074 0.0000000 2996 4777674 1 15755.694 9.8887559 0.0016792 2995 4771604 1 6070.152 3.8098134 0.0510462 2994 4770322 1 1282.563 0.8049758 0.3696820 Note, the anova compares the sum of resduals squared. the anova follows an F distribution, hence we could apple the critical values based on the anova we see that the errors change significantly until the 5th degree, hence the decision should be to take the model with order 4 of polynomials. Notice, that the model will never become worse in sample when complexity is added, as we fit the model more to the data. Alternative We could also have obtained the same output using coef() instead of the anove, where we see that teh p-values are the same, also the squared value of t (\\(t^2=F\\)). coef(summary(fit.5)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.70361 0.7287647 153.2780243 0.000000e+00 ## poly(df$age, 5)1 447.06785 39.9160847 11.2001930 1.491111e-28 ## poly(df$age, 5)2 -478.31581 39.9160847 -11.9830341 2.367734e-32 ## poly(df$age, 5)3 125.52169 39.9160847 3.1446392 1.679213e-03 ## poly(df$age, 5)4 -77.91118 39.9160847 -1.9518743 5.104623e-02 ## poly(df$age, 5)5 -35.81289 39.9160847 -0.8972045 3.696820e-01 Notice: this is only an alternative when we exclusively have polynomials in the model! Using ANOVA to assess for best models The following is another example of using ANOVA where different variables are used: And recall, that we should never apply p-values of variables in a model, to decide which that should be included. NOTE; this approach only works when the models are nested, meanin that the overall variables are the same, hence M2 could not have regian for instance, they all need to have the same overall variable fit.1 = lm(wage~education +age ,data=df) fit.2 = lm(wage~education +poly(age ,2) ,data=df) fit.3 = lm(wage~education +poly(age ,3) ,data=df) anova(fit.1,fit.2,fit.3) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 2994 3867992 NA NA NA NA 2993 3725395 1 142597.10 114.696898 0.0000000 2992 3719809 1 5586.66 4.493588 0.0341043 What are we looking for? What model lowers the RSS significantly. Using CV We could also have chosen the order of polynomials using cross validation. library(boot) set.seed(19) cv.error = rep (0, 5) for (i in 1:5) { fit.i=glm(wage~poly(age,i),data=Wage) # notice glm here in conjunction with cv.glm function cv.error[i]=cv.glm(Wage, fit.i, K=10)$delta[1] #K fold CV } cv.error # the CV errors of the five polynomials models ## [1] 1674.979 1600.176 1595.913 1594.003 1596.304 Concl: A 5 order model is not justified as the it starts increasing. Also we see that the 4th order d, is the best, which corresponds with previous findings. 2.3.1.2 Logarithmic model The procedure is per se the same, but now we are working with a probabilistic model instead of. Hence the outcome must be binary. Thus, it is decided to predict whether a persons wage is higher or lower than 250. fit &lt;- glm(I(wage &gt; 250) ~ poly(age,4) #Note the use of I() ,data = df ,family = binomial) Note, that again I() is used, where the expression is evaluated on the fly, one could naturally also had made a vector of the classes. Note, by default glm() will transform TRUE and FALSE to respectively 1 and 0. Now we can make predictions, which are logits, these can be used for much, hence later we will transform them into probabilities. preds = predict(fit ,newdata = list(age=age.grid) ,se.fit = TRUE) # We could have added type = response to get probabilities preds$fit[1:10]#First 10 logits ## 1 2 3 4 5 6 7 ## -18.438190 -16.395452 -14.560646 -12.919746 -11.459196 -10.165904 -9.027249 ## 8 9 10 ## -8.031077 -7.165700 -6.419899 To make confidence intevals for Pr(Y = 1|X), i.e. \\[\\begin{equation} Pr(Y=1|X)= \\frac{exp(X\\beta)}{1+exp(X\\beta)} \\tag{2.5} \\end{equation}\\] Where \\(X\\beta\\) can be explained by: \\[\\begin{equation} log(\\frac{Pr(Y=1|X)}{1-Pr(Y=1|X)})=X\\beta \\tag{2.6} \\end{equation}\\] Hence we must first calculate \\(X\\beta\\) to find Pr(Y=1|X). #Making Prbabilities pfit = exp(preds$fit)/(1+exp(preds$fit)) #See equation above #X beta se.bands.logit = cbind(preds$fit+2*preds$se.fit #Upper level ,preds$fit-2*preds$se.fit) #Lower level #Pr(Y = 1|X) se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit)) Remember that 2 SE = 95%, thus with the confidence levels we expect to contain 95% of the data. Notice, that the posterior probabilities could also have been found by using predict(), see the following: preds = predict (fit ,newdata = list(age = age.grid) ,type = &quot;response&quot; #Getting probabilities instead of logits ,se.fit = TRUE) NOTICE: for some reason this will lead to wrong confidence intervals (Hastie et al. 2013, 292), thus we prefer the regular approach, as shown before Now we can make the right hand plot, so we can compare with continous result. par(mfrow = c(1,2) ,mar = c(3,4.5,4.5,1.1) #Controls the margins ,oma = c(0,0,4,0)) #Controls the margins #Copy from earlier to combine plots fit &lt;- lm(wage ~ poly(age,4) #Orthogonal polynomials ,data = df) preds &lt;- predict(object = fit ,newdata = list(age = age.grid) ,se.fit = TRUE) plot(x = df$age,y = df$wage ,xlim = agelims ,cex = 0.5 #Size of dots ,col = &quot;darkgrey&quot;) title(&quot;Degree-4 Polynomial&quot;,outer = TRUE) lines(x = age.grid,y = preds$fit ,lwd = 2 ,col = &quot;blue&quot;) #The new plot plot(x = age,y = I(wage &gt;250) ,xlim = agelims ,type =&quot;n&quot; ,ylim = c(0,.2)) points(jitter(age) ,I((wage&gt;250)/5) ,cex = .5 ,pch = &quot;|&quot; ,col = &quot;darkgrey&quot;) lines(x = age.grid,y = pfit ,lwd = 2 ,col= &quot;blue&quot;) matlines(x = age.grid ,y = se.bands ,lwd = 1 ,col = &quot;blue&quot; ,lty = 3) We see on the right hand panel that the all the observations that have a wage above 250 is in the top and all those below hare in the bottom of the visualization. Although at the tail, we aren’t able to conclude much, as confidence interval is really high, hence it can both be high and low earners. jitter() is merely an approach to avoid observations to overlap each other. 2.3.1.3 Step function To fit the step function we must do: Define the cuts, cut() is able to automatically pick cutpoints. One could also use break() to define where the cuts should be. Train the model. Notice, that lm() will automatically create dummy variables for the ranges. {table(cut(df$age,4)) %&gt;% print() fit &lt;- lm(wage ~ cut(df$age,4) ,data = df) coef(summary(fit)) %&gt;% print()} ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 94.158392 1.476069 63.789970 0.000000e+00 ## cut(df$age, 4)(33.5,49] 24.053491 1.829431 13.148074 1.982315e-38 ## cut(df$age, 4)(49,64.5] 23.664559 2.067958 11.443444 1.040750e-29 ## cut(df$age, 4)(64.5,80.1] 7.640592 4.987424 1.531972 1.256350e-01 We see that the p value of the cuts are significant, not that we can use the p-values for much. Notice, that the first range is the base level, thus it is also left out. We can then use the intercept as the average wage for all in the range of up to 33.5 years. Hence for a 40 year old person, the model will say that he has an wage of 94 + 24 = 118 rm(list = ls()) 2.3.2 Splines The different approaches to splines are presented in the following. 2.3.2.1 Basis Function Splines library(ISLR) df &lt;- Wage library(splines) agelims &lt;- range(df$age) #The min and max age.grid &lt;- seq(from = agelims[1],to = agelims[2]) #Creating a counter within the range The splines library contain what we need. We introduce the following functions: bs(): Basis functions for splines. Generates entire matrix of basis functions for splines with the specified set of knots. ns(): Natural splines. smooth.spline(): Used when fitting smoothing splines. loess(): When fitting local regression. Note, that by default the splines will be choosen to be 3, this can also be found in the function documentation. par(mfrow = c(1,1),oma = c(0,0,0,0)) fit.bs &lt;- lm(wage ~ bs(age,knots = c(25,40,60)) #we just chose the knots randomly ,data = df) pred.bs &lt;- predict(fit.bs ,newdata = list(age = age.grid) ,se.fit = TRUE) plot(df$age ,df$wage ,col = &quot;gray&quot;) lines(age.grid ,pred.bs$fit ,lwd = 2) lines(age.grid ,pred.bs$fit+2*pred.bs$se ,lty = &quot;dashed&quot;) lines(age.grid ,pred.bs$fit-2*pred.bs$se ,lty = &quot;dashed&quot;) title(&quot;Splines - Basis Functions&quot;) We see that the splines have been fitted to the data and notice that the tails have wider confidence intervals. We can get the amount of degrees of freedom by calling the dim()function. { #Specifying the knots dim(bs(age,knots = c(25,40,60))) %&gt;% print() #df can be specified instead of knots dim(bs(age,df = 6)) %&gt;% print() } ## [1] 3000 6 ## [1] 3000 6 We see that the two alternatives produce the same results. Notice, that there are packages that will optimize the amount of knots. We can assess where the bs() placed the knots, by calling the attr(). attr(bs(age,df=6),&quot;knots&quot;) ## 25% 50% 75% ## 33.75 42.00 51.00 In this case, R chose the 25%, 50% and 75% quantiles. 2.3.2.2 Natural Splines It similar to bs(), but it has an additional condition. I did not really get it. The fitting procedure is the same, but now we just use ns() instead of bs(). fit.ns = lm(wage ~ ns(age ,df = 4 #Note, as with bs() we could have specified the knots instead of. ) ,data = df) pred.ns = predict(fit.ns ,newdata = list(age=age.grid) ,se.fit = TRUE) #Copy of old plot plot(df$age ,df$wage ,col = &quot;gray&quot;) lines(age.grid ,pred.bs$fit ,lwd = 2) lines(age.grid ,pred.bs$fit+2*pred.bs$se ,lty = &quot;dashed&quot;) lines(age.grid ,pred.bs$fit-2*pred.bs$se ,lty = &quot;dashed&quot;) #Adding natural splines lines(age.grid ,pred.ns$fit ,col =&quot;red&quot; ,lwd =2) title(&quot;Splines - Basis Functions + Natural Splines&quot;) legend(&quot;topright&quot;,c(&quot;Basis&quot;,&quot;Natural&quot;),lty = 1,col = c(&quot;Black&quot;,&quot;Red&quot;),cex = 0.6) 2.3.2.3 Smooth Splines As we discovered in the first part of the chapter, it sets a knot at each observation, and then we will penalize the function with a lamda (\\(\\lambda\\)), to avoid overfitting. The code show the procedure. #Hardcoding degrees of freedom fit.ss &lt;- smooth.spline(x = df$age,y = df$wage ,df = 16) #Remember that we must impose constraints #Choosing smoothing param with CV fit.ss2 &lt;- smooth.spline (df$age ,df$wage ,cv = TRUE) #we choose cv instead of fixed amount of df fit.ss2$df ## [1] 6.794596 We get sparsity hence we have degrees of freedom of 6.8. That is due to the tuning parameter which was found by the cross validation proces. We can find the specific lambda value with the following: fit.ss2$lambda ## [1] 0.02792303 plot(age,wage ,xlim = agelims ,cex = .5 ,col = &quot;darkgrey&quot;) title(&quot;Smoothing Spline&quot;) lines(fit.ss,col = &quot;red&quot;,lwd = 2) lines(fit.ss2,col = &quot;blue&quot;,lwd =2) legend(&quot;topright&quot;,legend = c(&quot;16 DF&quot;,&quot;6.8 DF&quot;) ,col = c(&quot;red&quot;,&quot;blue&quot;) ,lty = 1 ,lwd = 2 ,cex = .8) As expected, we see that the more complex model (highest amount of df) is the more flexible model. Note: tuning parameter = \\(\\lambda\\), where the CV seeks to choose the parameter that leads to the lowest error and return the df that leads to this level. 2.3.2.4 Local Regression Recall that local regression makes a linear regression for the observations that are close to the observation under evaluation (\\(x_0\\)). Thus we have to specify the span, the larger the span the smoother the fit, as we will include more observations. NB: locfit library can also be used for fitting local regress plot(x = df$age,y = df$wage ,xlim = agelims ,cex = .5 ,col = &quot;darkgrey&quot;) title (&quot;Local Regression&quot;) fit.lr &lt;- loess(wage ~ age ,span = .2 #Degree of smoothing / neighborhood to be included ,data = df) fit.lr2 &lt;- loess(wage ~ age ,span = .5 #Degree of smoothing / neighborhood to be included ,data = df) lines(x = age.grid,y = predict(object = fit.lr,newdata = data.frame(age=age.grid)) ,col = &quot;red&quot; ,lwd = 2) lines(x = age.grid,y = predict(object = fit.lr2,newdata = data.frame(age=age.grid)) ,col =&quot; blue&quot; ,lwd = 2) legend(x = &quot;topright&quot; ,legend = c(&quot;Span = 0.2&quot;,&quot;Span = 0.5&quot;) ,col=c(&quot;red&quot;,&quot;blue&quot;) ,lty = 1 ,lwd = 2 ,cex = .8) From the plot we also see that the model with the largest span has the smoothest fit. 2.3.3 GAMs We want to predict wage, where year, age and education (as categorical) as predictors. 2.3.3.1 With only natural splines According to the Hastie et al. (2013), 294, this is just a bunch of linear functions, hence we can merely apply lm(), see the following. gam.m1 &lt;- lm(wage ~ ns(year,df = 4) + ns(age,df = 5) + education #NOTICE, that we just use lm() ,data = df) summary(gam.m1) ## ## Call: ## lm(formula = wage ~ ns(year, df = 4) + ns(age, df = 5) + education, ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -120.513 -19.608 -3.583 14.112 214.535 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.949 4.704 9.980 &lt; 2e-16 *** ## ns(year, df = 4)1 8.625 3.466 2.488 0.01289 * ## ns(year, df = 4)2 3.762 2.959 1.271 0.20369 ## ns(year, df = 4)3 8.127 4.211 1.930 0.05375 . ## ns(year, df = 4)4 6.806 2.397 2.840 0.00455 ** ## ns(age, df = 5)1 45.170 4.193 10.771 &lt; 2e-16 *** ## ns(age, df = 5)2 38.450 5.076 7.575 4.78e-14 *** ## ns(age, df = 5)3 34.239 4.383 7.813 7.69e-15 *** ## ns(age, df = 5)4 48.678 10.572 4.605 4.31e-06 *** ## ns(age, df = 5)5 6.557 8.367 0.784 0.43328 ## education2. HS Grad 10.983 2.430 4.520 6.43e-06 *** ## education3. Some College 23.473 2.562 9.163 &lt; 2e-16 *** ## education4. College Grad 38.314 2.547 15.042 &lt; 2e-16 *** ## education5. Advanced Degree 62.554 2.761 22.654 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.16 on 2986 degrees of freedom ## Multiple R-squared: 0.293, Adjusted R-squared: 0.2899 ## F-statistic: 95.2 on 13 and 2986 DF, p-value: &lt; 2.2e-16 From the summary we see the variables that have been created and also the factor levels for education. Again, we don’t have to interprete the coefficients, we just need to look at the shape. 2.3.3.2 With different splines Now we have to apply the package gam. This is the best approach. library(gam) We can also construct a GAM model, that contains smoothing splines, that is done by calling s(). Where year and age will be included with up to 4 and 5 degrees of freedom. gam.m3 &lt;- gam(wage ~ s(year,df = 4) + s(age,df = 5) + education ,data = df) Remember, that GAM fits each variable while holding all other variables fixed. The actual fitting procedure is called backfitting, and fits variables by repeatedly updating the fit for each predictor (Hastie et al. 2013, 284–85). Hence, we create plots to interprete how. par(mfrow = c(1,3)) plot(gam.m3 #Note, automatically identifies the GAM object, hence plots for each variable ,se = TRUE ,col =&quot;blue&quot;) (#fig:GAMPlotLab7.8.3)GAM plot and intepretation Interpreting the plot: Recall that the plots assumes that we hold the other variables fixed, hence we see the following: Left: We see that holding education and age fixed, the wage tends to increase over the years, that is quite natural, e.g., because of inflation. Center: Holding year and education fixed, we see that the wage tends to be highest in the middle region around 40-45 years of age. That is also quite intuitive that the wage first increase and then decreasing as the person gets closer to the retirement age. Right: Holding year and age fixed, we see that the higher education you have, the higher will your wage be. par(mfrow = c(1,3)) plot.Gam(gam.m1 ,se = TRUE ,col = &quot;red&quot;) Figure 2.3: GAM of natural splines Notice, that this plot looks very similar to @(fig:GAMPlotLab7.8.3). This command could naturally also be used for the other GAM object, it is just that plot() does not automatically identify, that it is in fact intended to be interpretet as a GAM. 2.3.3.3 But what variables to include? It looks as is year is rather linear. To make this assessment, we can apply an ANOVA test of the different combinations. Hence: Note, the first model is nested in the second model (has the same variables), hence we can use ANOVA #Excluding year gam.m1 &lt;- gam(wage ~ s(age,df = 5) + education ,data = df) #Including year, but as a linear gam.m2 &lt;- gam(wage ~ year + s(age,df = 5) + education ,data = df) anova(gam.m1,gam.m2,gam.m3,test = &quot;F&quot;) Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) 2990 3711731 NA NA NA NA 2989 3693842 1.000000 17889.243 14.477130 0.0001447 2986 3689770 2.999989 4071.134 1.098212 0.3485661 We see that performance is significantly better going from model 1 to model 2, but on a five percent level, we are able to say, that we don’t gain anything with the third model, which is most complex model. Thus, the linear constellation of year, with polynomials on age + education as factors, appear to be the best performing model. With this in mind, it is interesting to assess the summary of the complex model: summary(gam.m3) ## ## Call: gam(formula = wage ~ s(year, df = 4) + s(age, df = 5) + education, ## data = df) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -119.43 -19.70 -3.33 14.17 213.48 ## ## (Dispersion Parameter for gaussian family taken to be 1235.69) ## ## Null Deviance: 5222086 on 2999 degrees of freedom ## Residual Deviance: 3689770 on 2986 degrees of freedom ## AIC: 29887.75 ## ## Number of Local Scoring Iterations: NA ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## s(year, df = 4) 1 27162 27162 21.981 0.000002877 *** ## s(age, df = 5) 1 195338 195338 158.081 &lt; 2.2e-16 *** ## education 4 1069726 267432 216.423 &lt; 2.2e-16 *** ## Residuals 2986 3689770 1236 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## s(year, df = 4) 3 1.086 0.3537 ## s(age, df = 5) 4 32.380 &lt;2e-16 *** ## education ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the section: “Anova for Nonparametric Effects,” we see that the smoothing spline on year, is not significant, hence it supports the conclusion from above, that we are better off, including the year as a linear variable. Now we can make predictions. #Predictions with linear year, non linear age and factors of education preds &lt;-predict(gam.m2 ,newdata = df) 2.3.3.4 GAM with local regression We are also able to make GAM on other building blocks, for instance local regression, that will be shown in the following For some reason the following cant be run. #GAM with local regression gam.lo &lt;- gam(wage ~ s(df$year,df = 4) + lo(df$age,span = 0.7) + education ,data = df) # plot.Gam(gam.lo #For some reason it cant be plotted # ,se = TRUE # ,col = &quot;green&quot;) Making interactions in the local regression: gam.lo.i &lt;- gam(wage ~ lo(year,age,span = 0.5) + education ,data = df) library(akima) plot(gam.lo.i) 2.3.3.5 Logistic Regression Plotting logistic regression GAM, here we can apply I() as previous used, to make the expression on the fly. gam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age,df = 5) + education ,family = binomial ,data = df) par(mfrow =c(1,3)) plot(gam.lr,se=T,col =&quot; green &quot;) One could interprete the plot and assess each window to see how the variable influence the decision wether the observation is above or below. Remember that the outcome can be seen as probabilities, these can also be plotted to be shown the spread: par(mfrow = c(1,1)) plot(gam.lr$fitted.values) From th plot, we see that there is a tendency that the lower the education the lower the wage, the following table show how the high earners are distributed. table(education,I(wage &gt; 250)) ## ## education FALSE TRUE ## 1. &lt; HS Grad 268 0 ## 2. HS Grad 966 5 ## 3. Some College 643 7 ## 4. College Grad 663 22 ## 5. Advanced Degree 381 45 We see that there are no people with less than high school degree that earns more than 250. To get more sensible result, we can remove the observations with a low degree, this will also show a more sensible result for the other degrees, see the following. gam.lr.s = gam(I(wage &gt; 250) ~ year + s(age,df = 5) + education ,family = binomial ,data = df ,subset = (education != &quot;1. &lt; HS Grad&quot;)) #removing people in the lowest group of education. par(mfrow = c(1,3)) plot(gam.lr.s ,se = TRUE #Standard errors ,col =&quot; green &quot;) Do we need a nonlinear term for year? Use anova for comparing the previous model with a model that includes a smooth spline of year with df=4 We can do an ANOVA, but please notice, we use Chi Square now. gam.y.s = gam(I(wage&gt;250) ~ s(year, 4) + s(age,5) + education,family=binomial,data = df,subset=(education!=&quot;1. &lt; HS Grad&quot;)) anova(gam.lr.s,gam.y.s, test=&quot;Chisq&quot;) # Chi-square test as Dep variable is categorical Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 2722 602.4588 NA NA NA 2719 601.5718 2.999982 0.8869514 0.8285731 We do not need a non-linear term for year. "],["exercises.html", "2.4 Exercises", " 2.4 Exercises 2.4.1 Exercise 6 library(ISLR) df &lt;- Wage 2.4.1.1 6.a Polynomial Regression We use orthogonal polynomials in the modeling process as we know that these are slightly better than raw polynomials due to the fact that this tend to avoid collinearity. Training the model library(boot) set.seed(1337) cv.error = rep (0,5) for (i in seq(from = 1,to = length(cv.error),by = 1)) { #Training fit.i &lt;- glm(wage ~ poly(age,i),data = df) # notice glm here in conjunction with cv.glm function #Performing cross validation cv.error[i] &lt;- cv.glm(data = df,glmfit = fit.i,K = 10)$delta[1] #K fold CV, delta = prediction error } #Printing the cv.error # the CV errors of the five polynomials models ## [1] 1675.056 1600.832 1594.505 1594.872 1594.608 The vector above are all of the prediction errors computed in the loop. which.min(cv.error) ## [1] 3 We see that the fifth prediction appear to yield the lowest MSE, but is it significantly different than e.g. forth or third order polynomial? fit.1 &lt;- glm(wage ~ poly(age,1),data = df) fit.2 &lt;- glm(wage ~ poly(age,2),data = df) fit.3 &lt;- glm(wage ~ poly(age,3),data = df) fit.4 &lt;- glm(wage ~ poly(age,4),data = df) fit.5 &lt;- glm(wage ~ poly(age,5),data = df) anova(fit.1,fit.2,fit.3,fit.4,fit.5,test = &quot;F&quot;) Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) 2998 5022216 NA NA NA NA 2997 4793430 1 228786.010 143.5931074 0.0000000 2996 4777674 1 15755.694 9.8887559 0.0016792 2995 4771604 1 6070.152 3.8098134 0.0510462 2994 4770322 1 1282.563 0.8049758 0.3696820 Using the F test, we see that on a five percent level the 4th ppolynomial is not justified, but close to. Thus we select a model with three polynomials. Plotting the errors, we also see that there does not happen much after the thrid polynomial plot(cv.error,type = &quot;b&quot;) Plotting the polynomial regression This is done with the following procedure: Make a grid counting IDV (Age) Make predictions Make a plot with the variables Fit a line onto the predictions Perhaps calculate confidence levels and plot these #Grid of X age.grid &lt;- seq(from = min(df$age),to = max(df$age),by = 1) #Predictions preds &lt;- predict(object = fit.3 ,newdata = list(age = age.grid) #Renaming age.grid to age ,se.fit = TRUE) #We want to produce confidence levels #Plotting plot(x = df$age,y = df$wage,col = &quot;darkgrey&quot;,cex = 0.8) grid() lines(x = age.grid #We need to define the grid, otherwise the fit will not be alligned with the data ,y = preds$fit ,col = &quot;red&quot;) title(&quot;Polynomial of 3rd order&quot;) 2.4.1.2 6.b Step function cuts &lt;- 4 #Cutting the x variable table(cut(df$age ,breaks = cuts)) ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 #&#39; Note, this only shows where the cuts lie and how many there are in each #Fitting the step function fit.step &lt;- lm(wage ~ cut(df$age,4) ,data = df) coef(summary(fit.step)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 94.158392 1.476069 63.789970 0.000000e+00 ## cut(df$age, 4)(33.5,49] 24.053491 1.829431 13.148074 1.982315e-38 ## cut(df$age, 4)(49,64.5] 23.664559 2.067958 11.443444 1.040750e-29 ## cut(df$age, 4)(64.5,80.1] 7.640592 4.987424 1.531972 1.256350e-01 We see that the the first cut (bin with people up to 33,5) have been left out. That is because they are contained in the intercept. Now we can fit the step function library(stats) #Predictions preds &lt;- predict(object = fit.step ,newdata = list(age = age.grid)) #Renaming age.grid to age #Plotting # plot(x = df$age,y = df$wage,col = &quot;darkgrey&quot;,cex = 0.8) # grid() # lines(age.grid # ,preds # ,col = &quot;red&quot;) # title(&quot;Step function of 3rd order&quot;) I need to check what she is doing, one could perhaps manually order the 2.4.2 Exercise 7 df &lt;- Wage Evaluating features other features to see how age respond hereon. We can plot the variables agains each other, to see how they interact. for (i in 1:10) { plot(y = df$wage,x = df[,i],xlab = names(df)[i],ylab = &quot;Wage&quot;) grid() names(df)[i] %&gt;% title() } Looking at race, it appears as if there is some relationship between race and wage the same with maritial status. Region only has values in one category, jobclass appear to visually have different means. The same goes for health and health insurance. Naturall log of wage has a non linear relationship with wage. Although the variable is the same, thus it cant be used for much to predict wage levels. Since all the variables of interest, and we haven’t worked with are all categorical, then we can’t really do any polynomial regression with the data, as they are all factors. What one could do is a mutlivariate linear model with different factors, or step functions or perhaps GAM where a continous varaible with polynomials are included. Therefore, I will not elaborate much more on this. 2.4.3 Exercise 8 df &lt;- Auto Are we able to predict how old a car is based on the variables at hand? Hence year = DV Name contains a lot of value, let us only use the first word, as that appear to be the brand. Therefore a loop is created to correct all the misspelled names. brand &lt;- strsplit(x = as.character(df$name),split = &quot; &quot;) brand.name &lt;- as.vector(rep(0,length(brand))) for (i in c(1:length(brand))) { brand.name[i] &lt;- brand[[i]][1] } table(brand.name) ## brand.name ## amc audi bmw buick cadillac ## 27 7 2 17 2 ## capri chevroelt chevrolet chevy chrysler ## 1 1 43 3 6 ## datsun dodge fiat ford hi ## 23 28 8 48 1 ## honda maxda mazda mercedes mercedes-benz ## 13 2 10 1 2 ## mercury nissan oldsmobile opel peugeot ## 11 1 10 4 8 ## plymouth pontiac renault saab subaru ## 31 16 3 4 4 ## toyota toyouta triumph vokswagen volkswagen ## 25 1 1 1 15 ## volvo vw ## 6 6 misspelled &lt;- matrix(byrow = TRUE,ncol = 2 ,data = c(&quot;mercedes&quot;,&quot;mercedes-benz&quot; ,&quot;toyouta&quot;,&quot;toyota&quot; ,&quot;chevroelt&quot;,&quot;chevrolet&quot; ,&quot;maxda&quot;,&quot;mazda&quot; ,&quot;vokswagen&quot;,&quot;volkswagen&quot; ,&quot;vw&quot;,&quot;volkswagen&quot;)) index &lt;- as.vector(&quot;&quot;) n &lt;- 0 bn.list &lt;- as.list(0) brand.name.recent &lt;- brand.name for (i in c(misspelled[,1])) { n &lt;- n + 1 index &lt;- rep(FALSE,length(brand.name)) index[brand.name == i] &lt;- TRUE bn.list[[n]] &lt;- replace(x = brand.name.recent,list = index,values = misspelled[n,2]) brand.name.recent &lt;- replace(x = brand.name.recent,list = index,values = misspelled[n,2]) } df &lt;- cbind(df[,-9],as.factor(bn.list[[6]])) names(df)[names(df) == &#39;bn.list[[6]]&#39;] &lt;- &quot;brand.name&quot; Also we must convert origin to a factor. df$origin &lt;- as.factor(df$origin) Checking correlations. The following can be run to see all the combinations # par(mfrow = c(1,1)) # for (i in 1:dim(mm)[2]) { # plot(y = df$year,x = mm[,i],xlab = names(mm)[i],ylab = &quot;Year&quot;) # grid() # colnames(mm)[i] %&gt;% title() # } Before training the model, we can partition the data to test the model out of sample set.seed(1337) train.size &lt;- round(x = nrow(df)*0.8,digits = 0) #Setting the training size train.index &lt;- sample(x = c(1:nrow(df)),size = train.size) #setting seed and creating vector for index mm &lt;- model.matrix(year ~ .,data = df)[,-1] #tried to make it mm first, to get rid of having variables that were in one partition but not the other. year &lt;- df$year train.df &lt;- as.data.frame(cbind(year,mm[train.index,])) #crating the training set test.df &lt;- as.data.frame(cbind(year,mm[-train.index,])) #creating the testing set library(gam) gam.m1 &lt;- gam(year ~ s(train.df$mpg,df = 5) + s(train.df$cylinders,df = 5) + s(train.df$displacement,df = 5) + s(train.df$horsepower,df = 5) + s(train.df$weight,df = 5) + s(train.df$acceleration,df = 5) + . ,data = train.df) summary(gam.m1) ## ## Call: gam(formula = year ~ s(train.df$mpg, df = 5) + s(train.df$cylinders, ## df = 5) + s(train.df$displacement, df = 5) + s(train.df$horsepower, ## df = 5) + s(train.df$weight, df = 5) + s(train.df$acceleration, ## df = 5) + ., data = train.df) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.5818 -2.0705 -0.2068 2.0660 6.0843 ## ## (Dispersion Parameter for gaussian family taken to be 8.889) ## ## Null Deviance: 2675.86 on 313 degrees of freedom ## Residual Deviance: 2266.693 on 255.0004 degrees of freedom ## AIC: 1631.771 ## ## Number of Local Scoring Iterations: NA ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value ## s(train.df$mpg, df = 5) 1 10.71 10.707 1.2045 ## s(train.df$cylinders, df = 5) 1 0.00 0.002 0.0003 ## s(train.df$displacement, df = 5) 1 2.44 2.441 0.2747 ## s(train.df$horsepower, df = 5) 1 0.13 0.134 0.0151 ## s(train.df$weight, df = 5) 1 5.93 5.926 0.6667 ## s(train.df$acceleration, df = 5) 1 1.52 1.517 0.1706 ## origin2 1 16.05 16.050 1.8056 ## origin3 1 11.10 11.097 1.2484 ## `\\\\`as.factor(bn.list[[6]])\\\\`audi` 1 6.53 6.526 0.7342 ## `\\\\`as.factor(bn.list[[6]])\\\\`bmw` 1 32.67 32.668 3.6751 ## `\\\\`as.factor(bn.list[[6]])\\\\`buick` 1 8.31 8.307 0.9345 ## `\\\\`as.factor(bn.list[[6]])\\\\`cadillac` 1 0.85 0.854 0.0960 ## `\\\\`as.factor(bn.list[[6]])\\\\`capri` 1 0.00 0.003 0.0004 ## `\\\\`as.factor(bn.list[[6]])\\\\`chevrolet` 1 0.23 0.230 0.0258 ## `\\\\`as.factor(bn.list[[6]])\\\\`chevy` 1 0.00 0.003 0.0004 ## `\\\\`as.factor(bn.list[[6]])\\\\`chrysler` 1 46.30 46.301 5.2088 ## `\\\\`as.factor(bn.list[[6]])\\\\`datsun` 1 18.32 18.321 2.0611 ## `\\\\`as.factor(bn.list[[6]])\\\\`dodge` 1 2.70 2.702 0.3040 ## `\\\\`as.factor(bn.list[[6]])\\\\`fiat` 1 1.89 1.895 0.2132 ## `\\\\`as.factor(bn.list[[6]])\\\\`ford` 1 25.85 25.847 2.9077 ## `\\\\`as.factor(bn.list[[6]])\\\\`hi` 1 0.34 0.337 0.0379 ## `\\\\`as.factor(bn.list[[6]])\\\\`honda` 1 3.40 3.401 0.3826 ## `\\\\`as.factor(bn.list[[6]])\\\\`mazda` 1 4.99 4.986 0.5609 ## `\\\\`as.factor(bn.list[[6]])\\\\`mercedes-benz` 1 0.01 0.014 0.0016 ## `\\\\`as.factor(bn.list[[6]])\\\\`mercury` 1 0.31 0.313 0.0352 ## `\\\\`as.factor(bn.list[[6]])\\\\`nissan` 1 3.68 3.684 0.4145 ## `\\\\`as.factor(bn.list[[6]])\\\\`oldsmobile` 1 18.12 18.124 2.0389 ## `\\\\`as.factor(bn.list[[6]])\\\\`opel` 1 0.30 0.303 0.0341 ## `\\\\`as.factor(bn.list[[6]])\\\\`peugeot` 1 1.79 1.794 0.2018 ## `\\\\`as.factor(bn.list[[6]])\\\\`plymouth` 1 0.53 0.527 0.0593 ## `\\\\`as.factor(bn.list[[6]])\\\\`pontiac` 1 4.37 4.374 0.4921 ## `\\\\`as.factor(bn.list[[6]])\\\\`renault` 1 16.53 16.533 1.8600 ## `\\\\`as.factor(bn.list[[6]])\\\\`saab` 1 0.10 0.103 0.0115 ## `\\\\`as.factor(bn.list[[6]])\\\\`subaru` 1 0.00 0.001 0.0001 ## `\\\\`as.factor(bn.list[[6]])\\\\`volkswagen` 1 0.63 0.634 0.0713 ## Residuals 255 2266.69 8.889 ## Pr(&gt;F) ## s(train.df$mpg, df = 5) 0.27346 ## s(train.df$cylinders, df = 5) 0.98665 ## s(train.df$displacement, df = 5) 0.60068 ## s(train.df$horsepower, df = 5) 0.90226 ## s(train.df$weight, df = 5) 0.41496 ## s(train.df$acceleration, df = 5) 0.67989 ## origin2 0.18023 ## origin3 0.26491 ## `\\\\`as.factor(bn.list[[6]])\\\\`audi` 0.39233 ## `\\\\`as.factor(bn.list[[6]])\\\\`bmw` 0.05635 . ## `\\\\`as.factor(bn.list[[6]])\\\\`buick` 0.33460 ## `\\\\`as.factor(bn.list[[6]])\\\\`cadillac` 0.75690 ## `\\\\`as.factor(bn.list[[6]])\\\\`capri` 0.98494 ## `\\\\`as.factor(bn.list[[6]])\\\\`chevrolet` 0.87244 ## `\\\\`as.factor(bn.list[[6]])\\\\`chevy` 0.98454 ## `\\\\`as.factor(bn.list[[6]])\\\\`chrysler` 0.02330 * ## `\\\\`as.factor(bn.list[[6]])\\\\`datsun` 0.15233 ## `\\\\`as.factor(bn.list[[6]])\\\\`dodge` 0.58186 ## `\\\\`as.factor(bn.list[[6]])\\\\`fiat` 0.64469 ## `\\\\`as.factor(bn.list[[6]])\\\\`ford` 0.08937 . ## `\\\\`as.factor(bn.list[[6]])\\\\`hi` 0.84586 ## `\\\\`as.factor(bn.list[[6]])\\\\`honda` 0.53679 ## `\\\\`as.factor(bn.list[[6]])\\\\`mazda` 0.45459 ## `\\\\`as.factor(bn.list[[6]])\\\\`mercedes-benz` 0.96840 ## `\\\\`as.factor(bn.list[[6]])\\\\`mercury` 0.85133 ## `\\\\`as.factor(bn.list[[6]])\\\\`nissan` 0.52028 ## `\\\\`as.factor(bn.list[[6]])\\\\`oldsmobile` 0.15454 ## `\\\\`as.factor(bn.list[[6]])\\\\`opel` 0.85372 ## `\\\\`as.factor(bn.list[[6]])\\\\`peugeot` 0.65362 ## `\\\\`as.factor(bn.list[[6]])\\\\`plymouth` 0.80775 ## `\\\\`as.factor(bn.list[[6]])\\\\`pontiac` 0.48365 ## `\\\\`as.factor(bn.list[[6]])\\\\`renault` 0.17383 ## `\\\\`as.factor(bn.list[[6]])\\\\`saab` 0.91452 ## `\\\\`as.factor(bn.list[[6]])\\\\`subaru` 0.99320 ## `\\\\`as.factor(bn.list[[6]])\\\\`volkswagen` 0.78966 ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## s(train.df$mpg, df = 5) 4 0.97458 0.4219 ## s(train.df$cylinders, df = 5) 3 0.63467 0.5933 ## s(train.df$displacement, df = 5) 4 1.72358 0.1452 ## s(train.df$horsepower, df = 5) 4 1.13935 0.3384 ## s(train.df$weight, df = 5) 4 0.69772 0.5941 ## s(train.df$acceleration, df = 5) 4 0.93342 0.4451 ## mpg ## cylinders ## displacement ## horsepower ## weight ## acceleration ## origin2 ## origin3 ## `\\\\`as.factor(bn.list[[6]])\\\\`audi` ## `\\\\`as.factor(bn.list[[6]])\\\\`bmw` ## `\\\\`as.factor(bn.list[[6]])\\\\`buick` ## `\\\\`as.factor(bn.list[[6]])\\\\`cadillac` ## `\\\\`as.factor(bn.list[[6]])\\\\`capri` ## `\\\\`as.factor(bn.list[[6]])\\\\`chevrolet` ## `\\\\`as.factor(bn.list[[6]])\\\\`chevy` ## `\\\\`as.factor(bn.list[[6]])\\\\`chrysler` ## `\\\\`as.factor(bn.list[[6]])\\\\`datsun` ## `\\\\`as.factor(bn.list[[6]])\\\\`dodge` ## `\\\\`as.factor(bn.list[[6]])\\\\`fiat` ## `\\\\`as.factor(bn.list[[6]])\\\\`ford` ## `\\\\`as.factor(bn.list[[6]])\\\\`hi` ## `\\\\`as.factor(bn.list[[6]])\\\\`honda` ## `\\\\`as.factor(bn.list[[6]])\\\\`mazda` ## `\\\\`as.factor(bn.list[[6]])\\\\`mercedes-benz` ## `\\\\`as.factor(bn.list[[6]])\\\\`mercury` ## `\\\\`as.factor(bn.list[[6]])\\\\`nissan` ## `\\\\`as.factor(bn.list[[6]])\\\\`oldsmobile` ## `\\\\`as.factor(bn.list[[6]])\\\\`opel` ## `\\\\`as.factor(bn.list[[6]])\\\\`peugeot` ## `\\\\`as.factor(bn.list[[6]])\\\\`plymouth` ## `\\\\`as.factor(bn.list[[6]])\\\\`pontiac` ## `\\\\`as.factor(bn.list[[6]])\\\\`renault` ## `\\\\`as.factor(bn.list[[6]])\\\\`saab` ## `\\\\`as.factor(bn.list[[6]])\\\\`subaru` ## `\\\\`as.factor(bn.list[[6]])\\\\`toyota` ## `\\\\`as.factor(bn.list[[6]])\\\\`triumph` ## `\\\\`as.factor(bn.list[[6]])\\\\`volkswagen` ## `\\\\`as.factor(bn.list[[6]])\\\\`volvo` It appears as if non of the parameters are good predictors. Then one could try out other models, or perhaps it is just very difficult with the data at hand to predict the year of the car. 2.4.4 Exercise 9 library(MASS) df &lt;- Boston df &lt;- as.data.frame(cbind(df$nox,df$dis)) colnames(df) &lt;- c(&quot;nox&quot;,&quot;dis&quot;) 2.4.4.1 (a) using poly function to fit cubic polynomial regression fit.poly &lt;- lm(nox ~ poly(dis,3),data = df) summary(fit.poly) ## ## Call: ## lm(formula = nox ~ poly(dis, 3), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.121130 -0.040619 -0.009738 0.023385 0.194904 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.554695 0.002759 201.021 &lt; 2e-16 *** ## poly(dis, 3)1 -2.003096 0.062071 -32.271 &lt; 2e-16 *** ## poly(dis, 3)2 0.856330 0.062071 13.796 &lt; 2e-16 *** ## poly(dis, 3)3 -0.318049 0.062071 -5.124 0.000000427 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06207 on 502 degrees of freedom ## Multiple R-squared: 0.7148, Adjusted R-squared: 0.7131 ## F-statistic: 419.3 on 3 and 502 DF, p-value: &lt; 2.2e-16 Remember that we are not interested in the coefficients as they are misleading, thus we want to look at the shape. The table above is mostly presented for explanatory reasons. As we are interested in the curve, we can fit that. #Defining range dislims &lt;- range(df$dis) n &lt;- (dislims[2]-dislims[1])/nrow(df) dis.grid &lt;- seq(from = dislims[1],to = dislims[2],by = n) #Predictions for the plot preds &lt;- predict(object = fit.poly,newdata = list(dis = dis.grid)) #Plotting plot(nox ~ dis, data = df, col = &quot;darkgrey&quot;) grid() lines(x = dis.grid,y = preds, col = &quot;blue&quot;,lwd = 2) title(&quot;Cubic polynomial&quot;) 2.4.4.2 (b) Plotting polynomial fits for a range of polynomials models &lt;- list() RSS &lt;- 0 for (d in 1:10) { models[[d]] &lt;- lm(nox ~ poly(dis,d),data = df) RSS[d] &lt;- sum(residuals(models[[d]])^2) } plot(RSS,type = &quot;b&quot;) points(x = which.min(RSS),y = RSS[which.min(RSS)],col = &quot;red&quot;,pch = 19) grid() abline(h = min(RSS),col = &quot;blue&quot;,lty = 2) title(&quot;In-sample error&quot;) We see that the RSS decrease with complexity, that it as expected, as we fit to the in sample data. We could do this with a partition of the data to see out of performance instead. 2.4.4.3 (c) Using CV to select best degree of d Here we run a loop with cross validation to see how the different order of d performs. As the partitions are randomly selected, we preduce 10 simulations to see which orders that tend to occur most often. models &lt;- list() RSS &lt;- 0 CV.RSS &lt;- 0 CV.RSS.sim &lt;- 0 for (i in 1:20) { for (d in 1:10) { models[[d]] &lt;- glm(nox ~ poly(dis,d),data = df) RSS[d] &lt;- sum(residuals(models[[d]])^2) CV.RSS[d] &lt;- cv.glm(data = df,glmfit = models[[d]],K = 10)$delta[2] #Delta = prediction error (adjusted) } CV.RSS.sim[i] &lt;- which.min(CV.RSS) } #Plotting prediction error plot(CV.RSS,type = &quot;b&quot;) points(x = which.min(CV.RSS),y = CV.RSS[which.min(CV.RSS)],col = &quot;red&quot;,pch = 19) grid() abline(h = min(CV.RSS),col = &quot;blue&quot;,lty = 2) title(&quot;CV K = 10 prediction error&quot;) #Plotting simulations barplot(table(CV.RSS.sim),xlab = &quot;Degree of d&quot;,ylab = &quot;Frequency&quot;) abline(h = 0) title(&quot;CV K = 10, Iterations = 20&quot;) In these simulations we see that the best fit is likely to be with using . It is actually quite interesting that a model with 10 degrees of d is as competitive as 4 in this example, although the cubic model is far superior than the other models. 2.4.4.4 (d) Use bs() to fit a regression spline par(mfrow = c(1,1)) fit.bs &lt;- lm(nox ~ bs(dis,df = 4),data = df) #Note as degree is not defined, default = 3 preds &lt;- predict(object = fit.bs,newdata = list(dis = dis.grid)) plot(x = df$dis,y = df$nox,col = &quot;darkgrey&quot;,pch = 20,ylab = &quot;nox&quot;,xlab = &quot;dis&quot;) lines(x = dis.grid,y = preds,col = &quot;blue&quot;,lwd = 2) grid() title(&quot;Regression Spline df = 4&quot;) abline(v = 3.20745,col = &quot;red&quot;,lty = 2) #This is the cut, found in next chunk legend(x = &quot;topright&quot;,legend = c(&quot;fit&quot;,&quot;cut&quot;),lty = 1:2,col = c(&quot;blue&quot;,&quot;red&quot;)) Notice that we merely specified the amount of df that we wanted. The function merely specified them automatically. We can interpret these, by using dim() and attr(). print(dim(bs(df$dis,df = 4))) ## [1] 506 4 attr(bs(df$dis,df = 4),&quot;knots&quot;) ## 50% ## 3.20745 We see that a model with 4 degrees of freedom yields one cut. Where the model put this at 50%, hence the first half (up to 3.20745). For simplicity, this cut has been added to the plot above, to show where the spline is split. 2.4.4.5 (e) Now fit a regression spline par(mfrow = c(2,2)) for (d in 4:7) { #The fit + preds fit.bs &lt;- lm(nox ~ bs(dis,df = d,degree = 3),data = df) preds &lt;- predict(object = fit.bs,newdata = list(dis = dis.grid)) #Cut cut &lt;- attr(bs(df$dis,df = d),&quot;knots&quot;) #Plot plot(x = df$dis,y = df$nox,col = &quot;darkgrey&quot;,pch = 20,ylab = &quot;nox&quot;,xlab = &quot;dis&quot;) lines(x = dis.grid,y = preds,col = &quot;blue&quot;,lwd = 2) grid() title(paste(&quot;Cubic Regression Spline, df =&quot;,d)) abline(v = cut,col = &quot;red&quot;,lty = 2) #This is the cut, found in next chunk legend(x = &quot;topright&quot;,legend = c(&quot;fit&quot;,&quot;cut&quot;),lty = 1:2,col = c(&quot;blue&quot;,&quot;red&quot;)) } We start at four degrees of freedom as a model with only three degrees of freedom, hence cubic regression (three orders of polynomials) = three degrees of freedom (this has to be fact checked). As we add complexity with knots we also adds degrees of freedom, where we add one degree of freedom for each cut, hence for the cubic spline with 7 degrees of freedom, four cuts and three polynomials (this has to be fact checked). 2.4.4.6 (f) models &lt;- list() RSS &lt;- 0 CV.RSS.sim &lt;- 0 for (i in 1:20) { for (d in 4:15) { models[[d]] &lt;- glm(nox ~ bs(dis,df = d,degree = 3),data = df) RSS[d] &lt;- sum(residuals(models[[d]])^2) CV.RSS[d] &lt;- cv.glm(data = df,glmfit = models[[d]],K = 10)$delta[2] #Delta = prediction error (adjusted) } CV.RSS.sim[i] &lt;- which.min(CV.RSS) } par(mfrow = c(1,1),mar = c(5,4.5,4.5,2.1),oma = c(0,0,0,0)) #Plotting prediction error plot(CV.RSS,type = &quot;b&quot;) points(x = which.min(CV.RSS),y = CV.RSS[which.min(CV.RSS)],col = &quot;red&quot;,pch = 19) grid() abline(h = min(CV.RSS),col = &quot;blue&quot;,lty = 2) title(&quot;CV K = 10 prediction error&quot;) #Plotting simulations barplot(table(CV.RSS.sim),xlab = &quot;Degree of d&quot;,ylab = &quot;Frequency&quot;) abline(h = 0) title(&quot;CV K = 10, Iterations = 20&quot;) First we see the last iteration and the prediction error hereof. Overall we see that it tend to be the rather complex models tend to be 2.4.5 Exercise 10 2.4.5.1 (a) Partitioning the data #Loading df &lt;- College #Partitioning set.seed(1337) train.size &lt;- round(x = nrow(df)*0.8,digits = 0) #Setting the training size train.index &lt;- sample(x = c(1:nrow(df)),size = train.size) #setting seed and creating vector for index train.df &lt;- df[train.index,] #crating the training set test.df &lt;- df[-train.index,] #creating the testing set rm(train.size) rm(train.index) Finding the best subset using forward selection reg_null &lt;- lm(Outstate ~ 1,data = train.df) #The null models reg_full &lt;- lm(Outstate ~ .,data = train.df) #The full model step.for &lt;- stepAIC(direction = &quot;forward&quot;,object = reg_null,trace = TRUE,scope = list(upper = reg_full,lower = reg_null)) ## Start: AIC=10345.68 ## Outstate ~ 1 ## ## Df Sum of Sq RSS AIC ## + Expend 1 4629213116 5745572675 9980.1 ## + Room.Board 1 4590249421 5784536370 9984.3 ## + Grad.Rate 1 3627082940 6747702852 10080.1 ## + Top10perc 1 3457381465 6917404327 10095.6 ## + perc.alumni 1 3450011987 6924773804 10096.2 ## + S.F.Ratio 1 3359826734 7014959058 10104.3 ## + Private 1 3075173999 7299611793 10129.0 ## + Top25perc 1 2644625673 7730160118 10164.7 ## + Terminal 1 2115825269 8258960523 10205.8 ## + PhD 1 1886906305 8487879487 10222.8 ## + Personal 1 735337851 9639447941 10301.9 ## + P.Undergrad 1 571871111 9802914681 10312.4 ## + F.Undergrad 1 404016737 9970769054 10323.0 ## + Enroll 1 180722874 10194062918 10336.7 ## + Apps 1 52841694 10321944097 10344.5 ## &lt;none&gt; 10374785791 10345.7 ## + Books 1 14584993 10360200799 10346.8 ## + Accept 1 90338 10374695453 10347.7 ## ## Step: AIC=9980.11 ## Outstate ~ Expend ## ## Df Sum of Sq RSS AIC ## + Private 1 1549696145 4195876531 9786.6 ## + Room.Board 1 1512070540 4233502135 9792.1 ## + Grad.Rate 1 1328830834 4416741842 9818.5 ## + perc.alumni 1 1089768014 4655804661 9851.3 ## + Personal 1 501146318 5244426358 9925.3 ## + S.F.Ratio 1 464809224 5280763451 9929.6 ## + F.Undergrad 1 454756490 5290816185 9930.8 ## + P.Undergrad 1 349207380 5396365295 9943.1 ## + Top10perc 1 346779721 5398792955 9943.4 ## + Top25perc 1 345553704 5400018971 9943.5 ## + Enroll 1 331862326 5413710349 9945.1 ## + Terminal 1 269786468 5475786207 9952.2 ## + PhD 1 210830779 5534741896 9958.9 ## + Apps 1 110876800 5634695875 9970.0 ## + Accept 1 85558833 5660013842 9972.8 ## + Books 1 18850616 5726722060 9980.1 ## &lt;none&gt; 5745572675 9980.1 ## ## Step: AIC=9786.59 ## Outstate ~ Expend + Private ## ## Df Sum of Sq RSS AIC ## + Room.Board 1 876836109 3319040422 9642.8 ## + Terminal 1 743790341 3452086190 9667.2 ## + Grad.Rate 1 703122598 3492753933 9674.5 ## + PhD 1 693220664 3502655866 9676.3 ## + perc.alumni 1 408270963 3787605568 9724.9 ## + Top25perc 1 401010315 3794866215 9726.1 ## + Top10perc 1 319956878 3875919653 9739.3 ## + Accept 1 152840390 4043036140 9765.5 ## + Personal 1 128047157 4067829374 9769.3 ## + Apps 1 118364448 4077512083 9770.8 ## + Enroll 1 37847497 4158029034 9783.0 ## + S.F.Ratio 1 28944091 4166932439 9784.3 ## + F.Undergrad 1 16848576 4179027955 9786.1 ## &lt;none&gt; 4195876531 9786.6 ## + Books 1 5437161 4190439370 9787.8 ## + P.Undergrad 1 3721562 4192154968 9788.0 ## ## Step: AIC=9642.78 ## Outstate ~ Expend + Private + Room.Board ## ## Df Sum of Sq RSS AIC ## + perc.alumni 1 419740777 2899299645 9560.7 ## + Grad.Rate 1 412477957 2906562465 9562.2 ## + PhD 1 379305385 2939735036 9569.3 ## + Terminal 1 369656216 2949384206 9571.3 ## + Top25perc 1 311299881 3007740541 9583.5 ## + Top10perc 1 280108774 3038931648 9589.9 ## + Personal 1 84780679 3234259743 9628.7 ## + Accept 1 42471882 3276568540 9636.8 ## + Books 1 35245677 3283794745 9638.1 ## + P.Undergrad 1 29148614 3289891808 9639.3 ## + S.F.Ratio 1 27660641 3291379781 9639.6 ## + Apps 1 24109291 3294931131 9640.2 ## + Enroll 1 12561816 3306478606 9642.4 ## &lt;none&gt; 3319040422 9642.8 ## + F.Undergrad 1 2021439 3317018983 9644.4 ## ## Step: AIC=9560.68 ## Outstate ~ Expend + Private + Room.Board + perc.alumni ## ## Df Sum of Sq RSS AIC ## + PhD 1 248319695 2650979950 9507.0 ## + Terminal 1 241588269 2657711376 9508.6 ## + Grad.Rate 1 194250957 2705048688 9519.5 ## + Top25perc 1 146504265 2752795380 9530.4 ## + Top10perc 1 124783671 2774515974 9535.3 ## + Accept 1 68407796 2830891849 9547.8 ## + Apps 1 37351605 2861948040 9554.6 ## + Personal 1 22021761 2877277884 9557.9 ## + Enroll 1 21250394 2878049251 9558.1 ## + Books 1 16826748 2882472897 9559.1 ## + S.F.Ratio 1 12022816 2887276829 9560.1 ## &lt;none&gt; 2899299645 9560.7 ## + F.Undergrad 1 9250654 2890048991 9560.7 ## + P.Undergrad 1 5862353 2893437292 9561.4 ## ## Step: AIC=9506.99 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD ## ## Df Sum of Sq RSS AIC ## + Grad.Rate 1 138096695 2512883255 9475.7 ## + Top25perc 1 42169169 2608810781 9499.0 ## + Top10perc 1 35379421 2615600528 9500.6 ## + Terminal 1 26636450 2624343499 9502.7 ## + Personal 1 26014281 2624965669 9502.9 ## + Accept 1 25856057 2625123892 9502.9 ## + S.F.Ratio 1 20504248 2630475702 9504.2 ## + P.Undergrad 1 15565029 2635414921 9505.3 ## + Books 1 14472738 2636507212 9505.6 ## + Apps 1 10845099 2640134851 9506.4 ## &lt;none&gt; 2650979950 9507.0 ## + Enroll 1 1715151 2649264798 9508.6 ## + F.Undergrad 1 235072 2650744878 9508.9 ## ## Step: AIC=9475.71 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate ## ## Df Sum of Sq RSS AIC ## + Terminal 1 29069264 2483813991 9470.5 ## + S.F.Ratio 1 23980164 2488903091 9471.7 ## + Personal 1 15282065 2497601190 9473.9 ## + Top25perc 1 14080081 2498803174 9474.2 ## + Books 1 13787420 2499095834 9474.3 ## + Top10perc 1 10584708 2502298546 9475.1 ## + Accept 1 10459352 2502423903 9475.1 ## &lt;none&gt; 2512883255 9475.7 ## + P.Undergrad 1 5510297 2507372958 9476.3 ## + F.Undergrad 1 1533109 2511350146 9477.3 ## + Apps 1 974955 2511908300 9477.5 ## + Enroll 1 16562 2512866693 9477.7 ## ## Step: AIC=9470.48 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal ## ## Df Sum of Sq RSS AIC ## + S.F.Ratio 1 21234904 2462579087 9467.1 ## + Books 1 18033724 2465780267 9467.9 ## + Personal 1 17438422 2466375568 9468.1 ## + Top25perc 1 11061012 2472752979 9469.7 ## + Top10perc 1 10503050 2473310941 9469.8 ## + Accept 1 8692072 2475121919 9470.3 ## &lt;none&gt; 2483813991 9470.5 ## + P.Undergrad 1 6212554 2477601437 9470.9 ## + F.Undergrad 1 2755569 2481058422 9471.8 ## + Apps 1 617630 2483196361 9472.3 ## + Enroll 1 29501 2483784490 9472.5 ## ## Step: AIC=9467.13 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio ## ## Df Sum of Sq RSS AIC ## + Personal 1 19556149 2443022938 9464.2 ## + Books 1 17642529 2444936558 9464.7 ## + Accept 1 12220653 2450358434 9466.0 ## + Top25perc 1 10915780 2451663307 9466.4 ## + Top10perc 1 9884752 2452694335 9466.6 ## &lt;none&gt; 2462579087 9467.1 ## + P.Undergrad 1 5425216 2457153871 9467.8 ## + Apps 1 1708616 2460870471 9468.7 ## + F.Undergrad 1 1057807 2461521279 9468.9 ## + Enroll 1 169448 2462409639 9469.1 ## ## Step: AIC=9464.18 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio + Personal ## ## Df Sum of Sq RSS AIC ## + Accept 1 16268824 2426754113 9462.0 ## + Books 1 12585987 2430436951 9463.0 ## + Top25perc 1 11662715 2431360223 9463.2 ## + Top10perc 1 10776478 2432246460 9463.4 ## &lt;none&gt; 2443022938 9464.2 ## + Apps 1 3319537 2439703401 9465.3 ## + P.Undergrad 1 2234491 2440788447 9465.6 ## + Enroll 1 1504102 2441518836 9465.8 ## + F.Undergrad 1 17762 2443005175 9466.2 ## ## Step: AIC=9462.02 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio + Personal + Accept ## ## Df Sum of Sq RSS AIC ## + F.Undergrad 1 34267884 2392486229 9455.2 ## + Apps 1 28349962 2398404152 9456.7 ## + Enroll 1 21722919 2405031194 9458.4 ## + Books 1 13764794 2412989319 9460.5 ## + Top10perc 1 9007820 2417746293 9461.7 ## + Top25perc 1 8657418 2418096695 9461.8 ## + P.Undergrad 1 7813774 2418940340 9462.0 ## &lt;none&gt; 2426754113 9462.0 ## ## Step: AIC=9455.17 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad ## ## Df Sum of Sq RSS AIC ## + Apps 1 33237860 2359248369 9448.5 ## + Top10perc 1 14141658 2378344572 9453.5 ## + Top25perc 1 13224939 2379261290 9453.7 ## + Books 1 11896812 2380589418 9454.1 ## &lt;none&gt; 2392486229 9455.2 ## + P.Undergrad 1 793001 2391693228 9457.0 ## + Enroll 1 19024 2392467206 9457.2 ## ## Step: AIC=9448.47 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad + ## Apps ## ## Df Sum of Sq RSS AIC ## + Top10perc 1 34199271 2325049099 9441.4 ## + Top25perc 1 24042385 2335205985 9444.1 ## + Books 1 10661970 2348586399 9447.7 ## &lt;none&gt; 2359248369 9448.5 ## + Enroll 1 1072888 2358175482 9450.2 ## + P.Undergrad 1 755069 2358493300 9450.3 ## ## Step: AIC=9441.39 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad + ## Apps + Top10perc ## ## Df Sum of Sq RSS AIC ## + Books 1 15416176 2309632923 9439.3 ## &lt;none&gt; 2325049099 9441.4 ## + Enroll 1 3260513 2321788586 9442.5 ## + Top25perc 1 166941 2324882158 9443.3 ## + P.Undergrad 1 256 2325048842 9443.4 ## ## Step: AIC=9439.25 ## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + ## Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad + ## Apps + Top10perc + Books ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2309632923 9439.3 ## + Enroll 1 3152187 2306480736 9440.4 ## + Top25perc 1 252598 2309380324 9441.2 ## + P.Undergrad 1 53 2309632870 9441.3 summary(step.for) ## ## Call: ## lm(formula = Outstate ~ Expend + Private + Room.Board + perc.alumni + ## PhD + Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + ## F.Undergrad + Apps + Top10perc + Books, data = train.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6206 -1233 10 1325 5390 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2286.93077 865.13003 -2.643 0.008418 ** ## Expend 0.16360 0.02379 6.878 1.51e-11 *** ## PrivateYes 2242.59519 268.94651 8.338 5.06e-16 *** ## Room.Board 0.94499 0.09401 10.052 &lt; 2e-16 *** ## perc.alumni 42.13604 8.50315 4.955 9.38e-07 *** ## PhD 14.09387 10.16544 1.386 0.166118 ## Grad.Rate 28.68532 6.13106 4.679 3.56e-06 *** ## Terminal 32.16147 11.27001 2.854 0.004468 ** ## S.F.Ratio -61.88646 28.54270 -2.168 0.030531 * ## Personal -0.19357 0.13361 -1.449 0.147935 ## Accept 0.67907 0.13205 5.143 3.66e-07 *** ## F.Undergrad -0.14101 0.03952 -3.568 0.000388 *** ## Apps -0.28245 0.07533 -3.749 0.000194 *** ## Top10perc 23.14400 7.23340 3.200 0.001448 ** ## Books -1.00042 0.49701 -2.013 0.044572 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1951 on 607 degrees of freedom ## Multiple R-squared: 0.7774, Adjusted R-squared: 0.7722 ## F-statistic: 151.4 on 14 and 607 DF, p-value: &lt; 2.2e-16 We see that the forward selection decides on 14 variables to be included 2.4.5.2 (b) Fitting a GAM library(forecast) best.formula &lt;- formula(step.for) fit.gam &lt;- gam(formula = best.formula,data = train.df) preds &lt;- predict(object = fit.gam,newdata = test.df) plot(x = preds,y = preds-test.df$Outstate,ylab = &quot;Residuals&quot;,xlab = &quot;Predicted values&quot;) From the plot we see that in general we have a resdiausl around 0 withinn +/- 5000. Also the residuals do look rather normal. Although one could argue that the variance is a but smaller in the lower region of the predicted value, and it does in fact appear as if we are under estimating these result. We can interprete how Outstate responds in the following: par(mfrow = c(4,4)) plot(fit.gam #Note, automatically identifies the GAM object, hence plots for each variable ,se = TRUE ,col =&quot;blue&quot;) Interpreting the plot: Recall that the plots assumes that we hold the other variables fixed, hence we see the following: e.g., Expend: We see that holding the other variables fixed, the outstate tends to increase over the expenditure. e.g., Apps: I assume that this is applicants, we see that holder the other variables fixed, outstate students tend to decrease as amount of applicants decrease. We can also assess the overall accuracy accuracy(preds,x = test.df$Outstate) ## ME RMSE MAE MPE MAPE ## Test set 85.03267 2041.19 1587.582 -1.863903 16.86691 We see that the MAE is 1587, where the mean absolute percentage error is almost 17%, hence it appear to be rather high. 2.4.5.3 (c) Evaluating on the test set This is what was done above. It is expected that if we compared applying on the train and test set, we will observe that the model has a lot of optimism on the train data, thus we should also see that the MAPE is lower on this partition, has this is what the model was trained on. 2.4.5.4 (d) Which variables appear to have a non linear relationship? par(mfrow = c(3,3)) for (i in c(1:8,10:18)) { plot(y = df$Outstate,x = df[,i],xlab = names(df)[i],ylab = &quot;Outstate&quot;,pch = 20,col = &quot;darkgrey&quot;) grid() colnames(df)[i] %&gt;% title() } It appears as if expend has som non linear relationship with Outstate. Perhaps enroll, F.Undergrad, and P.Undergrad also have a non linear trend. To further decide if there is evidence for a non linear relationship, one could make, e.g., a smoothing model to assess the performance hereof. "],["tree-based-methods.html", "3 Tree Based Methods", " 3 Tree Based Methods "],["subject-1.html", "4 Subject 1", " 4 Subject 1 "],["subject-1-1.html", "5 Subject 1", " 5 Subject 1 "],["references.html", "6 References", " 6 References Hastie, Trevor, Gareth James, Daniela Witten, and Tibshirani Robert. 2013. An Introduction to Statistical Learning with Applications in r. 1st, 8th print. Boca Raton, Florida: Springer. "]]
