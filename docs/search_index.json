[["index.html", "Machine Learning for Business Intelligence 2 setup", " Machine Learning for Business Intelligence 2 setup All the output formatting is done in the _output.yml file while all bookdown rendering options is done in the _bookdown.yml file.§ library(bookdown) Tips and tricks Referencing This is making a figure with a caption and where we center the figure and set the size. Notice, that the figure is automatically numbered according to the chapter number. plot(pressure,type = &#39;b&#39;,pch = 19) Figure 0.1: A fig Now we are able to make a reference to the chunk. Refer to a chunk: 0.1 Options RMarkdown Reference Guide "],["introduction.html", "1 Introduction", " 1 Introduction "],["moving-beyond-linearity.html", "2 Moving Beyond Linearity", " 2 Moving Beyond Linearity Literature: Moving Beyond Linearity (ISL CH7) Recall that complexity = also means lower interpretibility. This subject extents the linear models with the following: Polunomial Regression - where polynomials of the variables are added. Step Functions - where the x range is cut into k distinct regions to produce a qualitative variable. Hence also the name, piecewise constant function. Regression Splines - a combination / extensions of number one and two. Where polynomials functions are applied in specified regions of an X range. Smoothing Splines - Similar to the one above, but slightly different in the fitting process. Local Regression - Similar to regression splines, but these are able to overlap. Generalized Additive Models - allows to extent the model with several predictors. "],["models-beyond-linearity.html", "2.1 Models Beyond Linearity", " 2.1 Models Beyond Linearity Notice that all approaches despite GAM are extensions of simple linear regression, as it only takes on one predictor variable. 2.1.1 Polynomial Regression Can be defined by the following \\[\\begin{equation} y_{i\\ }=\\ \\beta_0+\\beta_1x_i+\\beta_2x_i^2+...+\\ \\beta_dx_i^d\\ +\\ \\epsilon_i \\tag{2.1} \\end{equation}\\] Rules of thumb: We don’t take on more than 3 or 4 degrees of d, as that yields strange lines Note that we can still use standard errors for coefficient estimates. 2.1.1.1 Beta coefficients and variance Each beta coefficient has its own variance (just as in linear regression). It can be defined by a matrix of j dimensions, e.g., if you have 5 betas (including beta 0) we can construct the correlation matrix. Covariance matrix can be identified by \\(\\hat{C}\\). Generally we get point estimates, but it is also interesting to show the confidence intervals (using 2 standard errors). 2.1.1.2 Application procedure Use lm() or glm() Use DV~poly(IV,degree) Perform CV with cv.glment() / aonva F-test to select degree This is basically either visually selecting the degrees that are the best using CV or using an ANOVA to assess if the MSE are significantly different from each other, hence an ANOVA test. Regarding ANOVA, if there stop being significance, then significant changes, e.g., in a poly 8, where the previous polynomials was not significant, then we can also disregard the 8’th polynomial. Fit the selected model Look at coef(summary(fit)) Plot data and predictions with predict() Check residuals Interpret The lecture shows exercise number 6 in chapter 7. 2.1.2 Step Functions This is literally just fitting a constant in different bins, see example on page 269. This is also called discretizing x. It is often applied when we see, e.g., five year age bins, e.g., a 20-25 year old is expected to earn so and so much etc. Major disadvantage: If there are no natural breakpoints, then the model is likely to miss variance and also generalize too much. Remember, that the steps reflect the average increase in Y for each step. Hence the first bin (range) is defined by \\(\\beta_0\\), and can be regarded as the average of that range of x. Thus each coefficient of the ranges of x is to be understood as the average increase of response. In other words, \\(\\beta_0\\) is the reference level, where the following cuts reflect the average increase or decrease hereon. Note that we can still use standard errors for coefficient estimates. 2.1.3 Regression Splines 2.1.3.1 Piecewise Polynomials This is basically polynomial regression, where the coefficients apply to specified ranges of X. The points where the coefficients changes are called knots. Hence a cubic function will look like the following. Notice, that a piecewise polynomial function with no knots, is merely a standard polynomial function. \\[\\begin{equation} y_{i\\ }=\\ \\beta_0+\\beta_1x_i+\\beta_2x_i^2+\\ \\beta_3x_i^3\\ +\\ \\epsilon_i \\tag{2.2} \\end{equation}\\] Where it can be extended to be written for each range. The coefficients can then be written with: \\(\\beta_{01},\\beta_{11},\\beta_{21}\\) etc. and for the second set: \\(\\beta_{02},\\beta_{12},\\beta_{22}\\) And for each of the betas in all of the cuts, you add one degree of freedom. Rule of thumb: The more knots, the more complex = the more variance capture and noise trailed, hence low model bias but large model variance. Figure 2.1: Piecewise Polynomials 2.1.3.2 Constraints and Splines Figure 7.3 show how the splines look, as the top left window tell, the jump is rather odd. Hence, we can force the fit to be continuous, by imposing a constraint. We can further constrain the model, with adding derivatives of the functions, hence the first derivative and the second derivative (where it in this instance created linear splines, that is because we have added \\(d-1\\) i.e. 2 derivatives, if the function was to the power of 4, then we should have imposed 3 derivatives to achieve linearity in the splines.) Hence, the linear spline can be defined by: It is piecewise splines of degree-d polynomials, with continuity in derivatives up to degree \\(d-1\\) at each knot. Hence we have the following constraints: Continuity Derivatives 2.1.3.3 Choosing the number and location of the Knots Choosing amount of knots? One may ask themself, how many degrees of freedom do you want to include in the model? Amount of knots is therefore corresponding to amount of degrees of freedom. We can let software estimate the best amount and the best locations. Here one can use: In-sample performance Out-of-sample performance, e.g. with CV, perhaps extent to K folds with K tests, to ensure, that each variable has been held out once. This can be followed by visualizing the MSE for the different simulations with different amount of knots. 2.1.3.4 Comparison with Polynomial Regression With regression splines we are able to introduce knots that account for variance as it slightly resets the model in each section, hence we can fit the model to the data without having to impose as much complexity as we would in normal polynomial regression. Hence one often observes that regression splines have more stability than polynomial regression. 2.1.4 Smoothing Splines This is basically attempting to find a model that captures the variance by a smoothing line. Doing so, we fit a very flexible model and impose restrictions upon this, to achieve a shrunken model, just as with Lasso and Ridge Regression. Thus, the smoothing (imposing restrictions) deals with overfitting. This can be defined as a cubic spline with knot at every unique value of \\(x_i\\) Hence we have the following model: \\[\\begin{equation} RSS=\\sum_{i=1}^n\\left(y_i-g\\left(x_i\\right)\\right)^{^2}+\\lambda\\int g&#39;&#39;\\left(t\\right)^{^2}dt \\tag{2.3} \\end{equation}\\] i.e. Loss + Penalty Where: We define model g(x) \\((y_i-g(x_i))^{^2}\\) = the loss, meaning the difference between the fitted model and the actual y’s \\(\\lambda\\) = the tuning parameter, hence the restriction that we want to impose. If lambda is low, then much flexibility, if lambda is high, then low flexibility. Hence, controls the bias variance tradeoff. \\(\\int g&#39;&#39;\\left(t\\right)^{^2}dt\\) = a measure of how much \\(g&#39;(x)\\) changes oer time. Hence, the higher we set \\(\\lambda\\) the more imposed restrictions, meaning the smoother the model, as lambda gets closer to infinity, the model becomes linear. 2.1.4.1 Choosing optimal tuning parameter The analytic LOOCV can be calculated, the procedure appears to be the same as for lasso and ridge regression. The book (page 279) describes this a bit in details. However it says that software is able to do this. Basically what is done, is LOOCV and simulating different tuning parameters to assess what model that performs the best. 2.1.5 Local Regression This i basically fitting a linear regression to each x, where s observations are included in the fitting procedure. Thus, one creates several fits, that are based on the observations weighted, where observations close to \\(x_0\\) (the center of the regression) are given the highest weight and then the weight is gradually decreasing. This can be visualized with: Figure 2.2: Local regression Doing local regression has the following procedure (algorithm) Gather the fraction \\(s = k/n\\) of training points whose \\(x_i\\) are closest to \\(x_0\\). Assign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point in this neighborhood, so that the point furthest from x0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero. Fit a weighted least squares regression of the \\(y_i\\) on the \\(x_i\\) using the aforementioned weights, by finding \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) that minimize \\[\\begin{equation} \\sum_{i=1}^nK_{i0}\\left(y_i-\\beta_0-\\beta_1x_i\\right)^{^2} \\tag{2.4} \\end{equation}\\] The fitted value at \\(x_0\\) is given by \\(\\hat{f}(x_0)=\\hat\\beta_0+\\hat\\beta_1x_0\\) Where we see how the model is 2.1.6 Generalized Additive Models This can naturally both be applied in regression and classification problems, futher elaborated in the following. 2.1.6.1 GAM for regression problems Now we move beyond being constrained to only one predictor variable, hence GAM can be seen more as an extension of multiple linear regression. Hence GAM is a combination of different functions, where they are each fitted while holding the other variables fixed. GAM can consist of any different non-linear model, e.g., we can just as well use local regression, polynomial regression, or any combination of the approaches seen above in this subsection. Disadvantages of GAM: The fitting procedure holds the other variables fixed, hence it does not count for interactions. Therefore, one may manually construct interaction variables to account for this, just like in mulitple linear regression. Prediction wise it is not competitive with Neural Networks and Support Vector Machines. Advantages of GAM: Allowing to fit non linear function for j variables (\\(f_j\\)) Has potential of making more accurate predictions As the model is additive (meaning that each function is fitted holding the other variables fixed) we are still able to make inference, e.g., assessing how one variable affects the y variable. Smoothness of function \\(f_j\\) can be summarized with degrees of freedom. Often applied when aiming for explanatory analysis (instead of prediction) 2.1.6.2 GAM for classification problems When y is qualitative (categorical), GAM can also be applied in the logistical form. As discovered in the classification section, we can apply logits (log of odds) and odds, see material from first semester. The same advantages and disadvantages as in the prior section applies. "],["lab-section.html", "2.2 Lab section", " 2.2 Lab section Loading the data that will be used throughout the lab section. library(ISLR) attach(Wage) df &lt;- Wage 2.2.1 Polynomial Regression and Step Functions 2.2.1.1 Continous model Fitting the model: fit &lt;- lm(wage ~ poly(age,4) #Orthogonal polynomials ,data = df) fit2 &lt;- lm(wage ~ poly(age,4,raw = TRUE) #Orthogonal polynomials ,data = df) Note: poly() returns orthogonal polynomials, which is some linear combination of the variables to the d power. See the following two examples when using orthogonal and normal polynomials: { print(&quot;Orthogonal&quot;) cbind(df$age,poly(x = df$age,degree = 4))[1:5,] %&gt;% print() print(&quot;Regular&quot;) cbind(df$age,poly(x = df$age,degree = 4,raw = TRUE))[1:5,] %&gt;% print() } ## [1] &quot;Orthogonal&quot; ## 1 2 3 4 ## [1,] 18 -0.0386247992 0.055908727 -0.0717405794 0.08672985 ## [2,] 24 -0.0291326034 0.026298066 -0.0145499511 -0.00259928 ## [3,] 45 0.0040900817 -0.014506548 -0.0001331835 0.01448009 ## [4,] 43 0.0009260164 -0.014831404 0.0045136682 0.01265751 ## [5,] 50 0.0120002448 -0.009815846 -0.0111366263 0.01021146 ## [1] &quot;Regular&quot; ## 1 2 3 4 ## [1,] 18 18 324 5832 104976 ## [2,] 24 24 576 13824 331776 ## [3,] 45 45 2025 91125 4100625 ## [4,] 43 43 1849 79507 3418801 ## [5,] 50 50 2500 125000 6250000 In the end, it does not have a noticeable effect. options(scipen = 5) { coef(summary(fit)) %&gt;% print() coef(summary(fit2)) %&gt;% print() } ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.70361 0.7287409 153.283015 0.000000e+00 ## poly(age, 4)1 447.06785 39.9147851 11.200558 1.484604e-28 ## poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32 ## poly(age, 4)3 125.52169 39.9147851 3.144742 1.678622e-03 ## poly(age, 4)4 -77.91118 39.9147851 -1.951938 5.103865e-02 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -184.1541797743 60.04037718327 -3.067172 0.0021802539 ## poly(age, 4, raw = TRUE)1 21.2455205321 5.88674824448 3.609042 0.0003123618 ## poly(age, 4, raw = TRUE)2 -0.5638593126 0.20610825640 -2.735743 0.0062606446 ## poly(age, 4, raw = TRUE)3 0.0068106877 0.00306593115 2.221409 0.0263977518 ## poly(age, 4, raw = TRUE)4 -0.0000320383 0.00001641359 -1.951938 0.0510386498 Even though the coefficients are different and the p-values hereof, the fitted values will be indistinguishable (Hastie et al. 2013, 288). This is also shown later. Alternatives to using poly()?? We have two alternatives: Using I() Using cbind() Using I() fit2a &lt;- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4) #Note that &#39;I()&#39; is added ,data = df) coef(fit2a) ## (Intercept) age I(age^2) I(age^3) I(age^4) ## -184.1541797743 21.2455205321 -0.5638593126 0.0068106877 -0.0000320383 Notice I() as ‘^’ has another special meaning in formulas Hence we see that the coefficients are the same. Using cbind() fit2b &lt;- lm(wage ~ cbind(age,age^2,age^3,age^4) ,data = df) coef(fit2b) ## (Intercept) cbind(age, age^2, age^3, age^4)age ## -184.1541797743 21.2455205321 ## cbind(age, age^2, age^3, age^4) cbind(age, age^2, age^3, age^4) ## -0.5638593126 0.0068106877 ## cbind(age, age^2, age^3, age^4) ## -0.0000320383 We see that we are now able to use ‘^’ within the cbind(). proceeding with the lab sections. We can now present a grid of values for age, at which we want predictions and then call the predict() and also plot the standard errors. agelims &lt;- range(df$age) #The min and max age.grid &lt;- seq(from = agelims[1],to = agelims[2]) #Creating a counter within the range preds &lt;- predict(object = fit ,newdata = list(age = age.grid) #Creating a list with the counter named age, so it fits the IV naming ,se.fit = TRUE) se.bands &lt;- cbind(preds$fit + 2*preds$se.fit #Upper band ,preds$fit-2*preds$se.fit) #Lower band Now we can plot the data par(mfrow = c(1,2) ,mar = c(3.1,4.5,4.5,1.1) #Controls the margins ,oma = c(0,0,4,0)) #Controls the margins {plot(x = df$age,y = df$wage ,xlim = agelims ,cex = 0.5 #Size of dots ,col = &quot;darkgrey&quot;) title(&quot;Degree-4 Polynomial&quot;,outer = TRUE) lines(x = age.grid,y = preds$fit ,lwd = 2 ,col = &quot;blue&quot;)} With the following we see that the difference of the fitted values are practically 0. preds2 &lt;- predict(object = fit2 ,newdata = list(age = age.grid) ,se.fit = TRUE) max(abs(preds$fit-preds2$fit)) ## [1] 7.81597e-11 Now we can compare models with different orthogonal polynomials. fit.1 &lt;- lm(wage~age,data=df) fit.2 &lt;- lm(wage~poly(df$age,2),data=df) fit.3 &lt;- lm(wage~poly(df$age,3),data=df) fit.4 &lt;- lm(wage~poly(df$age,4),data=df) fit.5 &lt;- lm(wage~poly(df$age,5),data=df) anova(fit.1,fit.2,fit.3,fit.4,fit.5) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 2998 5022216 NA NA NA NA 2997 4793430 1 228786.010 143.5931074 0.0000000 2996 4777674 1 15755.694 9.8887559 0.0016792 2995 4771604 1 6070.152 3.8098134 0.0510462 2994 4770322 1 1282.563 0.8049758 0.3696820 based on the anova we see that the errors change significantly util the 5th degree, hence the decision should be to take the model with order 4 of polynomials. Notice, that the model will never become worse in sample when complexity is added, as we fit the model more to the data. Alternative We could also have obtained the same output using coef() instead of the anove, where we see that teh p-values are the same, also the squared value of t (\\(t^2=F\\)). coef(summary(fit.5)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.70361 0.7287647 153.2780243 0.000000e+00 ## poly(df$age, 5)1 447.06785 39.9160847 11.2001930 1.491111e-28 ## poly(df$age, 5)2 -478.31581 39.9160847 -11.9830341 2.367734e-32 ## poly(df$age, 5)3 125.52169 39.9160847 3.1446392 1.679213e-03 ## poly(df$age, 5)4 -77.91118 39.9160847 -1.9518743 5.104623e-02 ## poly(df$age, 5)5 -35.81289 39.9160847 -0.8972045 3.696820e-01 Notice: this is only an alternative when we exclusively have polynomials in the model! The following is another example of using ANOVA where different variables are used: fit.1 = lm(wage~education +age ,data=df) fit.2 = lm(wage~education +poly(age ,2) ,data=df) fit.3 = lm(wage~education +poly(age ,3) ,data=df) anova(fit.1,fit.2,fit.3) Res.Df RSS Df Sum of Sq F Pr(&gt;F) 2994 3867992 NA NA NA NA 2993 3725395 1 142597.10 114.696898 0.0000000 2992 3719809 1 5586.66 4.493588 0.0341043 We could also have chosen the order of polynomials using cross validation. 2.2.1.2 Logarithmic model The procedure is per se the same, but now we are working with a probabilistic model instead of. Hence the outcome must be binary. Thus, it is decided to predict whether a persons wage is higher or lower than 250. fit &lt;- glm(I(wage &gt; 250) ~ poly(age,4) #Note the use of I() ,data = df ,family = binomial) Note, that again I() is used, where the expression is evaluated on the fly, one could naturally also had made a vector of the classes. Note, by default glm() will transform TRUE and FALSE to respectively 1 and 0. Now we can make predictions. preds = predict(fit ,newdata = list(age=age.grid) ,se.fit = TRUE) To make confidence intevals for Pr(Y = 1|X), i.e. \\[\\begin{equation} Pr(Y=1|X)= \\frac{exp(X\\beta)}{1+exp(X\\beta)} \\tag{2.5} \\end{equation}\\] Where \\(X\\beta\\) can be explained by: \\[\\begin{equation} log(\\frac{Pr(Y=1|X)}{1-Pr(Y=1|X)})=X\\beta \\tag{2.6} \\end{equation}\\] Hence we must first calculate \\(X\\beta\\) to find Pr(Y=1|X). #Prbabilities pfit = exp(preds$fit)/(1+exp(preds$fit)) #See equation above #X beta se.bands.logit = cbind(preds$fit+2*preds$se.fit #Upper level ,preds$fit-2*preds$se.fit) #Lower level #Pr(Y = 1|X) se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit)) Notice, that the posterior probabilities could also have been found by using predict(), see the following: preds=predict (fit ,newdata = list(age = age.grid) ,type = &quot;response&quot; ,se.fit = TRUE) NOTICE: for some reason this will lead to wrong confidence intervals (Hastie et al. 2013, 292) Now we can make the right hand plot, so we can compare with continous result. par(mfrow = c(1,2) ,mar = c(3.1,4.5,4.5,1.1) #Controls the margins ,oma = c(0,0,1,0)) #Controls the margins #Copy from earlier to combine plots fit &lt;- lm(wage ~ poly(age,4) #Orthogonal polynomials ,data = df) preds &lt;- predict(object = fit ,newdata = list(age = age.grid) ,se.fit = TRUE) plot(x = df$age,y = df$wage ,xlim = agelims ,cex = 0.5 #Size of dots ,col = &quot;darkgrey&quot;) title(&quot;Degree-4 Polynomial&quot;,outer = TRUE) lines(x = age.grid,y = preds$fit ,lwd = 2 ,col = &quot;blue&quot;) #The new plot plot(x = age,y = I(wage &gt;250) ,xlim = agelims ,type =&quot;n&quot; ,ylim = c(0,.2)) points(jitter(age) ,I((wage&gt;250)/5) ,cex = .5 ,pch = &quot;|&quot; ,col = &quot;darkgrey&quot;) lines(x = age.grid,y = pfit ,lwd = 2 ,col= &quot;blue&quot;) matlines(x = age.grid ,y = se.bands ,lwd = 1 ,col = &quot;blue&quot; ,lty = 3) We see on the right hand panel that the all the observations that have a wage above 250 is in the top and all those below hare in the bottom of the visualization. jitter() is merely an approach to avoid observations to overlap each other. 2.2.1.3 Step function To fit the step function we must do: Define the cuts, cut() is able to automatically pick cutpoints. One could also use break() to define where the cuts should be. Train the model. Notice, that lm() will automatically create dummy variables for the ranges. {table(cut(df$age,4)) %&gt;% print() fit &lt;- lm(wage ~ cut(df$age,4) ,data = df) coef(summary(fit)) %&gt;% print()} ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 94.158392 1.476069 63.789970 0.000000e+00 ## cut(df$age, 4)(33.5,49] 24.053491 1.829431 13.148074 1.982315e-38 ## cut(df$age, 4)(49,64.5] 23.664559 2.067958 11.443444 1.040750e-29 ## cut(df$age, 4)(64.5,80.1] 7.640592 4.987424 1.531972 1.256350e-01 We see that the p value of the cuts are significant. Notice, that the first range is the base level, thus it is also left out. We can then use the intercept as the average wage for all in the range of up to 33.5 years. rm(list = ls()) 2.2.2 Splines The different approaches to splines are presented in the following. 2.2.2.1 Basis Function Splines library(ISLR) df &lt;- Wage library(splines) agelims &lt;- range(df$age) #The min and max age.grid &lt;- seq(from = agelims[1],to = agelims[2]) #Creating a counter within the range The splines library contain what we need. We introduce the following functions: bs(): Basis functions for splines. Generates entire matrix of basis functions for splines with the specified set of knots. ns(): Natural splines. smooth.spline(): Used when fitting smoothing splines. loess(): When fitting local regression. par(mfrow = c(1,1),oma = c(0,0,0,0)) fit.bs &lt;- lm(wage ~ bs(age ,knots = c(25,40,60)) #Note, degree is by default 3 ,data = df) pred.bs &lt;- predict(fit.bs ,newdata = list(age = age.grid) ,se.fit = TRUE) plot(df$age ,df$wage ,col = &quot;gray&quot;) lines(age.grid ,pred.bs$fit ,lwd = 2) lines(age.grid ,pred.bs$fit+2*pred.bs$se ,lty = &quot;dashed&quot;) lines(age.grid ,pred.bs$fit-2*pred.bs$se ,lty = &quot;dashed&quot;) title(&quot;Splines - Basis Functions&quot;) We see that the splines have been fitted to the data and notice that the tails have wider confidence intervals. We can get the amount of degrees of freedom by calling the dim()function. {dim(bs(age ,knots = c(25,40,60)) #Specifying the knots ) %&gt;% print() dim(bs(age ,df = 6) #df can be specified instead of knots ) %&gt;% print() } ## [1] 3000 6 ## [1] 3000 6 We see that the two alternatives produce the same results We can assess where the bs() placed the knots, by calling the attr(). attr(bs(age,df=6),&quot;knots&quot;) ## 25% 50% 75% ## 33.75 42.00 51.00 In this case, R chose the 25%, 50% and 75% quantiles. 2.2.2.2 Natural Splines The fitting procedure is the same, but now we just use ns() instead of bs(). fit.ns = lm(wage ~ ns(age ,df = 4 #Note, as with bs() we could have specified the knots instead of. ) ,data = df) pred.ns = predict(fit.ns ,newdata = list(age=age.grid) ,se.fit = TRUE) #Copy of old plot plot(df$age ,df$wage ,col = &quot;gray&quot;) lines(age.grid ,pred.bs$fit ,lwd = 2) lines(age.grid ,pred.bs$fit+2*pred.bs$se ,lty = &quot;dashed&quot;) lines(age.grid ,pred.bs$fit-2*pred.bs$se ,lty = &quot;dashed&quot;) #Adding natural splines lines(age.grid ,pred.ns$fit ,col =&quot;red&quot; ,lwd =2) title(&quot;Splines - Basis Functions + Natural Splines&quot;) legend(&quot;topright&quot;,c(&quot;Basis&quot;,&quot;Natural&quot;),lty = 1,col = c(&quot;Black&quot;,&quot;Red&quot;),cex = 0.6) 2.2.2.3 Smooth Splines The code show the procedure. fit.ss &lt;- smooth.spline(x = df$age,y = df$wage ,df = 16) #Remember that we must impose constraints #Choosing smoothing param with CV fit.ss2 &lt;- smooth.spline (df$age ,df$wage ,cv = TRUE) #we choose cv instead of fixed amount of df fit.ss2$df ## [1] 6.794596 We get sparsity hence we have degrees of freedom of 6.8. That is due to the tuning parameter. plot(age,wage ,xlim = agelims ,cex = .5 ,col = &quot;darkgrey&quot;) title(&quot;Smoothing Spline&quot;) lines(fit.ss,col = &quot;red&quot;,lwd = 2) lines(fit.ss2,col = &quot;blue&quot;,lwd =2) legend(&quot;topright&quot;,legend = c(&quot;16 DF&quot;,&quot;6.8 DF&quot;) ,col = c(&quot;red&quot;,&quot;blue&quot;) ,lty = 1 ,lwd = 2 ,cex = .8) As expected, we see that the more complex model (highest amount of df) is the more flexible model. Note: tuning parameter = \\(\\lambda\\), where the CV seeks to choose the parameter that leads to the lowest error and return the df that leads to this level. 2.2.2.4 Local Regression Recall that local regression makes a linear regression for the observations that are close to the observation under evaluation (\\(x_0\\)). Thus we have to specify the span, the larger the span the smoother the fit, as we will include more observations. NB: locfit library can also be used for fitting local regress plot(x = df$age,y = df$wage ,xlim = agelims ,cex = .5 ,col = &quot;darkgrey&quot;) title (&quot;Local Regression&quot;) fit.lr &lt;- loess(wage ~ age ,span = .2 #Degree of smoothing / neighborhood to be included ,data = df) fit.lr2 &lt;- loess(wage ~ age ,span = .5 #Degree of smoothing / neighborhood to be included ,data = df) lines(x = age.grid,y = predict(object = fit.lr,newdata = data.frame(age=age.grid)) ,col = &quot;red&quot; ,lwd = 2) lines(x = age.grid,y = predict(object = fit.lr2,newdata = data.frame(age=age.grid)) ,col =&quot; blue&quot; ,lwd = 2) legend(x = &quot;topright&quot; ,legend = c(&quot;Span = 0.2&quot;,&quot;Span = 0.5&quot;) ,col=c(&quot;red&quot;,&quot;blue&quot;) ,lty = 1 ,lwd = 2 ,cex = .8) From the plot we also see that the model with the largest span has the smoothest fit. 2.2.3 GAMs To be done (Hastie et al. 2013, 294) "],["exercises.html", "2.3 Exercises", " 2.3 Exercises 2.3.1 Exercise 6 (Lecture 1) "],["tree-based-methods.html", "3 Tree Based Methods", " 3 Tree Based Methods "],["subject-1.html", "4 Subject 1", " 4 Subject 1 "],["subject-1-1.html", "5 Subject 1", " 5 Subject 1 "],["references.html", "6 References", " 6 References Hastie, Trevor, Gareth James, Daniela Witten, and Tibshirani Robert. 2013. An Introduction to Statistical Learning with Applications in r. 1st, 8th print. Boca Raton, Florida: Springer. "]]
