<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Different methods explained | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="2.1 Different methods explained | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Different methods explained | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Different methods explained | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moving-beyond-linearity.html"/>
<link rel="next" href="tree-based-methods.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="different-methods-explained.html"><a href="different-methods-explained.html"><i class="fa fa-check"></i><b>2.1</b> Different methods explained</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="different-methods-explained.html"><a href="different-methods-explained.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="different-methods-explained.html"><a href="different-methods-explained.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="different-methods-explained.html"><a href="different-methods-explained.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="different-methods-explained.html"><a href="different-methods-explained.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="different-methods-explained.html"><a href="different-methods-explained.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="different-methods-explained.html"><a href="different-methods-explained.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="different-methods-explained.html"><a href="different-methods-explained.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="different-methods-explained.html"><a href="different-methods-explained.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="different-methods-explained.html"><a href="different-methods-explained.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.4</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="different-methods-explained.html"><a href="different-methods-explained.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="different-methods-explained.html"><a href="different-methods-explained.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="different-methods-explained.html"><a href="different-methods-explained.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="different-methods-explained.html"><a href="different-methods-explained.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="different-methods-explained.html"><a href="different-methods-explained.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.1</b> GAM for regression problems</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="different-methods-explained.html"><a href="different-methods-explained.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for classification problems</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a></li>
<li class="chapter" data-level="4" data-path="subject-1.html"><a href="subject-1.html"><i class="fa fa-check"></i><b>4</b> Subject 1</a></li>
<li class="chapter" data-level="5" data-path="subject-1-1.html"><a href="subject-1-1.html"><i class="fa fa-check"></i><b>5</b> Subject 1</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="different-methods-explained" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Different methods explained</h2>
<p>Notice that all approaches despite GAM are extensions of simple linear regression, as it only takes on one predictor variable.</p>
<div id="polynomial-regression" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Polynomial Regression</h3>
<p>Can be defined by the following</p>
<p><span class="math display" id="eq:PolynomialRegression">\[\begin{equation}
y_{i\ }=\ \beta_0+\beta_1x_i+\beta_2x_i^2+...+\ \beta_dx_i^d\ +\ \epsilon_i
\tag{2.1}
\end{equation}\]</span></p>
<p>Rules of thumb:</p>
<ul>
<li>We don’t take on more than 3 or 4 degrees of d, as that yields strange lines</li>
</ul>
<p>Note that we can still use standard errors for coefficient estimates.</p>
<div id="beta-coefficients-and-variance" class="section level4" number="2.1.1.1">
<h4><span class="header-section-number">2.1.1.1</span> Beta coefficients and variance</h4>
<p>Each beta coefficient has its own variance (just as in linear regression).</p>
<p>It can be defined by a matrix of j dimensions, e.g., if you have 5 betas (including beta 0) we can construct the correlation matrix.</p>
<p>Covariance matrix can be identified by <span class="math inline">\(\hat{C}\)</span>.</p>
<p>Generally we get point estimates, but it is also interesting to show the confidence intervals (using 2 standard errors).</p>
</div>
<div id="application-procedure" class="section level4" number="2.1.1.2">
<h4><span class="header-section-number">2.1.1.2</span> Application procedure</h4>
<ol style="list-style-type: decimal">
<li>Use <code>lm()</code> or <code>glm()</code></li>
<li>Use DV~poly(IV,degree)</li>
<li>Perform CV with <code>cv.glment()</code> / aonva F-test to select degree
<ul>
<li>This is basically either visually selecting the degrees that are the best using CV or using an ANOVA to assess if the MSE are significantly different from each other, hence an ANOVA test. Regarding ANOVA, if there stop being significance, then significant changes, e.g., in a poly 8, where the previous polynomials was not significant, then we can also disregard the 8’th polynomial.</li>
</ul></li>
<li>Fit the selected model</li>
<li>Look at <code>coef(summary(fit))</code></li>
<li>Plot data and predictions with <code>predict()</code></li>
<li>Check residuals</li>
<li>Interpret</li>
</ol>
<p>The lecture shows exercise number 6 in chapter 7.</p>
</div>
</div>
<div id="step-functions" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Step Functions</h3>
<p>This is literally just fitting a constant in different bins, see example on page 269. This is also called discretizing x.</p>
<p>It is often applied when we see, e.g., five year age bins, e.g., a 20-25 year old is expected to earn so and so much etc.</p>
<p><strong>Major disadvantage:</strong> If there are no natural breakpoints, then the model is likely to miss variance and also generalize too much.</p>
<p>Remember, that the steps reflect the average increase in Y for each step. Hence the first bin (range) is defined by <span class="math inline">\(\beta_0\)</span>, and can be regarded as the average of that range of x. Thus each coefficient of the ranges of x is to be understood as the average increase of response.</p>
<p>In other words, <span class="math inline">\(\beta_0\)</span> is the reference level, where the following cuts reflect the average increase or decrease hereon.</p>
<p>Note that we can still use standard errors for coefficient estimates.</p>
</div>
<div id="regression-splines" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Regression Splines</h3>
<div id="piecewise-polynomials" class="section level4" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Piecewise Polynomials</h4>
<p>This is basically polynomial regression, where the coefficients apply to specified ranges of X. The points where the coefficients changes are called <strong><em>knots</em></strong>. Hence a cubic function will look like the following.</p>
<p>Notice, that a piecewise polynomial function with no knots, is merely a standard polynomial function.</p>
<p><span class="math display" id="eq:PiecewisePolynomials">\[\begin{equation}
y_{i\ }=\ \beta_0+\beta_1x_i+\beta_2x_i^2+\ \beta_3x_i^3\ +\ \epsilon_i
\tag{2.2}
\end{equation}\]</span></p>
<p>Where it can be extended to be written for each range.</p>
<p>The coefficients can then be written with: <span class="math inline">\(\beta_{01},\beta_{11},\beta_{21}\)</span> etc. and for the second set: <span class="math inline">\(\beta_{02},\beta_{12},\beta_{22}\)</span></p>
<p>Rule of thumb:</p>
<ul>
<li>The more knots, the more complex = the more variance capture and noise trailed, hence low model bias but large model variance.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="Images/Piecewise%20regression.png" alt="Piecewise Polynomials" width="488" />
<p class="caption">
Figure 2.1: Piecewise Polynomials
</p>
</div>
</div>
<div id="constraints-and-splines" class="section level4" number="2.1.3.2">
<h4><span class="header-section-number">2.1.3.2</span> Constraints and Splines</h4>
<p>Figure 7.3 show how the splines look, as the top left window tell, the jump is rather odd. Hence, we can force the fit to be continuous, by imposing a constraint.</p>
<p>We can futher contstrain the model, with adding derivatives of the functions, hence the first derivative and the second derivative (where it in this instance created linear splines, that is because we have added <span class="math inline">\(d-1\)</span> i.e. 2 derivates, if the function was to the power of 4, then we should have imposed 3 derivatives to achieve linearity in the splines.)</p>
<p>Hence, the <em>linear spline</em> can be defined by: It is piecewise splines of degree-d polynomials, with continoutity in derivatives up to degree <span class="math inline">\(d-1\)</span> at each knot.</p>
<p>Hence we have the following constraints:</p>
<ol style="list-style-type: decimal">
<li>Continuity</li>
<li>Derivatives</li>
</ol>
</div>
<div id="choosing-the-number-and-location-of-the-knots" class="section level4" number="2.1.3.3">
<h4><span class="header-section-number">2.1.3.3</span> Choosing the number and location of the Knots</h4>
<p>Choosing amount of knots? One may ask themself, how many degrees of freedom do you want to include in the model?</p>
<p>Amount of knots is therefore corresponding to amount of degrees of freedom.</p>
<p>We can let software estimate the best amount and the best locations. Here one can use:</p>
<ol style="list-style-type: decimal">
<li>In-sample performance</li>
<li>Out-of-sample performance, e.g. with CV, perhaps extent to K folds with K tests, to ensure, that each variable has beem held out once.</li>
</ol>
<p>This can be followed by visualizing the MSE for the different simulations with different amount of knots.</p>
</div>
<div id="comparison-with-polynomial-regression" class="section level4" number="2.1.3.4">
<h4><span class="header-section-number">2.1.3.4</span> Comparison with Polynomial Regression</h4>
<p>With regression splines we are able to introduce knots that account for variance as it slightly resets the model in each section, hence we can fit the model to the data without having to impose as much complexity as we would in normal polynomial regression.</p>
<p>Hence one often observes that regression splines have more stability than polynomial regression.</p>
</div>
</div>
<div id="smoothing-splines" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Smoothing Splines</h3>
<p>This is basically attempting to find a model that captures the variance by a smoothing line. Doing so, we fit a very flexible model and impose restrictrions upon this, to achieve a shrunken model, just as with Lasso and Ridge Regression.</p>
<p>This can be defind as a cubic spline with knot at every unique value of <span class="math inline">\(x_i\)</span></p>
<p>Hence we have the following model:</p>
<p><span class="math display" id="eq:SmoothingSplines">\[\begin{equation}
\sum_{i=1}^n\left(y_i-g\left(x_i\right)\right)^{^2}+\lambda\int g&#39;&#39;\left(t\right)^{^2}dt
\tag{2.3}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li>We define model g(x)</li>
<li><span class="math inline">\((y_i-g(x_i))^{^2}\)</span> = the loss, meaning the difference between the fitted model and the actual y’s</li>
<li><span class="math inline">\(\lambda\)</span> = the tuning parameter, hence the restriction that we want to impose. If lambda is low, then much flexibility, if lambda is high, then low flexibility. Hence, controls the bias variance tradeoff.</li>
<li><span class="math inline">\(\int g&#39;&#39;\left(t\right)^{^2}dt\)</span> = a measure of how much <span class="math inline">\(g&#39;(x)\)</span> changes oer time. Hence, the higher we set <span class="math inline">\(\lambda\)</span> the more imposed restrictions, meaning the smoother the model, as lambda gets closer to infinity, the model becomes linear.</li>
</ul>
<div id="choosing-optimal-tuning-parameter" class="section level4" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> Choosing optimal tuning parameter</h4>
<p>The analytical LOOCV can be calculated, the procedure appears to be the same as for lasso and ridge regression. The book (page 279) describes this a bit in details. However it says that software is able to do this.</p>
<p>Basically what is done, is LOOCV and simulating different tuning parameters to assess what model that performs the best.</p>
</div>
</div>
<div id="local-regression" class="section level3" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Local Regression</h3>
<p>This i basically fitting a linear regression to each x, where s observations are included in the fitting procedure. Thus, one creates several fits, that are based on the observations weighted, where observations close to <span class="math inline">\(x_0\)</span> (the center of the regression) are given the highest weight and then the weight is gradually decreasing.</p>
<p>This can be visualized with:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="Images/Local%20Regression.png" alt="Local regression" width="494" />
<p class="caption">
Figure 2.2: Local regression
</p>
</div>
<p>Doing local regression has the following procedure (algorithm)</p>
<ol style="list-style-type: decimal">
<li>Gather the fraction <span class="math inline">\(s = k/n\)</span> of training points whose <span class="math inline">\(x_i\)</span> are closest to <span class="math inline">\(x_0\)</span>.</li>
<li>Assign a weight <span class="math inline">\(K_{i0} = K(x_i, x_0)\)</span> to each point in this neighborhood, so that the point furthest from x0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.</li>
<li>Fit a weighted least squares regression of the <span class="math inline">\(y_i\)</span> on the <span class="math inline">\(x_i\)</span> using the aforementioned weights, by finding <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> that minimize</li>
</ol>
<p><span class="math display" id="eq:LocalRegression">\[\begin{equation}
\sum_{i=1}^nK_{i0}\left(y_i-\beta_0-\beta_1x_i\right)^{^2}
\tag{2.4}
\end{equation}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>The fitted value at <span class="math inline">\(x_0\)</span> is given by <span class="math inline">\(\hat{f}(x_0)=\hat\beta_0+\hat\beta_1x_0\)</span></li>
</ol>
<p>Where we see how the model is</p>
</div>
<div id="generalized-additive-models" class="section level3" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> Generalized Additive Models</h3>
<p>This can naturally both be applied in regression and classification problems, futher elaborated in the following.</p>
<div id="gam-for-regression-problems" class="section level4" number="2.1.6.1">
<h4><span class="header-section-number">2.1.6.1</span> GAM for regression problems</h4>
<p>Now we move beyond being constrained to only one predictor variable, hence GAM can be seen more as an extension of multiple linear regression. Hence GAM is a combination of different functions, where they are each fitted while holding the other variables fixed. GAM can consist of any different non-linear model, e.g., we can just as well use local regression, polynomial regression, or any combination of the approaches seen above in this subsection.</p>
<p><strong>Disadvantages of GAM:</strong> The fitting procedure holds the other variables fixed, hence it does not count for interactions. Therefore, one may manually construct interaction variables to account for this, just like in mulitple linear regression.</p>
<p><strong>Advantages of GAM:</strong></p>
<ol style="list-style-type: decimal">
<li>Allowing to fit non linear function for j variables (<span class="math inline">\(f_j\)</span>)</li>
<li>Has potential of making more accurate predictions</li>
<li>As the model is additive (meaning that each function is fitted holding the other variables fixed) we are still able to make inference, e.g., assessing how one variable affects the y variable.</li>
<li>Smoothness of function <span class="math inline">\(f_j\)</span> can be summarized with degrees of freedom.</li>
</ol>
</div>
<div id="gam-for-classification-problems" class="section level4" number="2.1.6.2">
<h4><span class="header-section-number">2.1.6.2</span> GAM for classification problems</h4>
<p>When y is qualitative (categorical), GAM can also be applied in the logistical form.</p>
<p>As discovered in the classification section, we can apply logits (log of odds) and odds, see material from first semester.</p>
<p>The same advantages and disadvantages as in the prior section applies.</p>

</div>
</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="moving-beyond-linearity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tree-based-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
