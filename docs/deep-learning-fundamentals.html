<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Deep Learning Fundamentals | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="5 Deep Learning Fundamentals | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Deep Learning Fundamentals | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Deep Learning Fundamentals | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="support-vector-machines.html"/>
<link rel="next" href="structuring-data-transformation-and-model-assessments.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#models-beyond-linearity"><i class="fa fa-check"></i><b>2.1</b> Models Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#degrees-of-freedom"><i class="fa fa-check"></i><b>2.1.3.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="2.1.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.5</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#feature-selection"><i class="fa fa-check"></i><b>2.1.6.1</b> Feature Selection</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for regression problems</a>
<ul>
<li class="chapter" data-level="2.1.6.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#interpretation-of-output"><i class="fa fa-check"></i><b>2.1.6.2.1</b> Interpretation of output</a></li>
</ul></li>
<li class="chapter" data-level="2.1.6.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.3</b> GAM for classification problems</a></li>
</ul></li>
<li class="chapter" data-level="2.1.7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#model-assessment"><i class="fa fa-check"></i><b>2.1.7</b> Model assessment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lecture-notes"><i class="fa fa-check"></i><b>2.2</b> Lecture notes</a></li>
<li class="chapter" data-level="2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lab-section"><i class="fa fa-check"></i><b>2.3</b> Lab section</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression-and-step-functions"><i class="fa fa-check"></i><b>2.3.1</b> Polynomial Regression and Step Functions</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#continous-model"><i class="fa fa-check"></i><b>2.3.1.1</b> Continous model</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logarithmic-model"><i class="fa fa-check"></i><b>2.3.1.2</b> Logarithmic model</a></li>
<li class="chapter" data-level="2.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-function"><i class="fa fa-check"></i><b>2.3.1.3</b> Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#splines"><i class="fa fa-check"></i><b>2.3.2</b> Splines</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#basis-function-splines"><i class="fa fa-check"></i><b>2.3.2.1</b> Basis Function Splines</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#natural-splines"><i class="fa fa-check"></i><b>2.3.2.2</b> Natural Splines</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smooth-splines"><i class="fa fa-check"></i><b>2.3.2.3</b> Smooth Splines</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression-1"><i class="fa fa-check"></i><b>2.3.2.4</b> Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gams"><i class="fa fa-check"></i><b>2.3.3</b> GAMs</a>
<ul>
<li class="chapter" data-level="2.3.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-only-natural-splines"><i class="fa fa-check"></i><b>2.3.3.1</b> With only natural splines</a></li>
<li class="chapter" data-level="2.3.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-different-splines"><i class="fa fa-check"></i><b>2.3.3.2</b> With different splines</a></li>
<li class="chapter" data-level="2.3.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#but-what-variables-to-include"><i class="fa fa-check"></i><b>2.3.3.3</b> But what variables to include?</a></li>
<li class="chapter" data-level="2.3.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-with-local-regression"><i class="fa fa-check"></i><b>2.3.3.4</b> GAM with local regression</a></li>
<li class="chapter" data-level="2.3.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logistic-regression"><i class="fa fa-check"></i><b>2.3.3.5</b> Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a>
<ul>
<li class="chapter" data-level="2.4.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-polynomial-regression"><i class="fa fa-check"></i><b>2.4.1.1</b> 6.a Polynomial Regression</a></li>
<li class="chapter" data-level="2.4.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-step-function"><i class="fa fa-check"></i><b>2.4.1.2</b> 6.b Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a>
<ul>
<li class="chapter" data-level="2.4.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-using-poly-function-to-fit-cubic-polynomial-regression"><i class="fa fa-check"></i><b>2.4.4.1</b> (a) using poly function to fit cubic polynomial regression</a></li>
<li class="chapter" data-level="2.4.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-plotting-polynomial-fits-for-a-range-of-polynomials"><i class="fa fa-check"></i><b>2.4.4.2</b> (b) Plotting polynomial fits for a range of polynomials</a></li>
<li class="chapter" data-level="2.4.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-using-cv-to-select-best-degree-of-d"><i class="fa fa-check"></i><b>2.4.4.3</b> (c) Using CV to select best degree of d</a></li>
<li class="chapter" data-level="2.4.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-use-bs-to-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.4</b> (d) Use <code>bs()</code> to fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#e-now-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.5</b> (e) Now fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#f-perform-cross-validation-to-select-degrees"><i class="fa fa-check"></i><b>2.4.4.6</b> (f) Perform cross-validation, to select degrees</a></li>
</ul></li>
<li class="chapter" data-level="2.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a>
<ul>
<li class="chapter" data-level="2.4.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-partitioning-the-data"><i class="fa fa-check"></i><b>2.4.5.1</b> (a) Partitioning the data</a></li>
<li class="chapter" data-level="2.4.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-fitting-a-gam"><i class="fa fa-check"></i><b>2.4.5.2</b> (b) Fitting a GAM</a></li>
<li class="chapter" data-level="2.4.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-evaluating-on-the-test-set"><i class="fa fa-check"></i><b>2.4.5.3</b> (c) Evaluating on the test set</a></li>
<li class="chapter" data-level="2.4.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-which-variables-appear-to-have-a-non-linear-relationship"><i class="fa fa-check"></i><b>2.4.5.4</b> (d) Which variables appear to have a non linear relationship?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#FacebookCasestudy"><i class="fa fa-check"></i><b>2.5</b> Casestudy - Predicting the Return on Advertising Spent</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#background"><i class="fa fa-check"></i><b>2.5.1</b> 1. Background</a></li>
<li class="chapter" data-level="2.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#case-study-business-understanding-phase"><i class="fa fa-check"></i><b>2.5.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#the-data-data-understanding-phase"><i class="fa fa-check"></i><b>2.5.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#specific-requirements"><i class="fa fa-check"></i><b>2.5.4</b> 4. Specific requirements:</a>
<ul>
<li class="chapter" data-level="2.5.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-1---import-and-overview"><i class="fa fa-check"></i><b>2.5.4.1</b> 4.1 Task 1 - Import and overview</a></li>
<li class="chapter" data-level="2.5.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-2---data-inspection"><i class="fa fa-check"></i><b>2.5.4.2</b> 4.2 Task 2 - Data inspection</a></li>
<li class="chapter" data-level="2.5.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-3---building-different-models"><i class="fa fa-check"></i><b>2.5.4.3</b> 4.3 Task 3 - Building different models</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-generalized-additive-model-gam-to-predict-roas"><i class="fa fa-check"></i><b>2.5.4.3.1</b> A Generalized Additive Model (GAM) to predict ROAS</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c1-feature-selection-using-regsubsets"><i class="fa fa-check"></i><b>2.5.4.3.1.1</b> c1) Feature selection using regsubsets()</a></li>
<li class="chapter" data-level="2.5.4.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c2-feature-selection-using-step.gam"><i class="fa fa-check"></i><b>2.5.4.3.1.2</b> c2) Feature selection using step.GAM</a></li>
<li class="chapter" data-level="2.5.4.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c3-fetaure-selection-using-random-forest"><i class="fa fa-check"></i><b>2.5.4.3.1.3</b> c3) Fetaure selection using random forest</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>3.1</b> The Basics of Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-trees"><i class="fa fa-check"></i><b>3.1.1</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#how-to-make-the-decision-trees"><i class="fa fa-check"></i><b>3.1.1.1</b> How to make the decision trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-goal-of-regression"><i class="fa fa-check"></i><b>3.1.1.1.1</b> The goal of regression</a></li>
<li class="chapter" data-level="3.1.1.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning-algoritm"><i class="fa fa-check"></i><b>3.1.1.1.2</b> Tree Pruning &amp; Algoritm</a></li>
<li class="chapter" data-level="3.1.1.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#setting-contrains-of-the-tree-sise"><i class="fa fa-check"></i><b>3.1.1.1.3</b> Setting contrains of the tree sise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#classification-trees"><i class="fa fa-check"></i><b>3.1.2</b> Classification Trees</a></li>
<li class="chapter" data-level="3.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-vs.-lienar-models"><i class="fa fa-check"></i><b>3.1.3</b> Tree vs. Lienar Models</a></li>
<li class="chapter" data-level="3.1.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advantages-and-disadvantages-of-trees"><i class="fa fa-check"></i><b>3.1.4</b> Advantages and Disadvantages of Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-random-forests-boosting"><i class="fa fa-check"></i><b>3.2</b> Bagging, Random Forests, Boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-bootstrap-aggregation"><i class="fa fa-check"></i><b>3.2.1</b> Bagging (Bootstrap Aggregation)</a></li>
<li class="chapter" data-level="3.2.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="3.2.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-i.e.-gradient-boosting"><i class="fa fa-check"></i><b>3.2.3</b> Boosting (i.e. Gradient Boosting)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost"><i class="fa fa-check"></i><b>3.3</b> XGBoost</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#application-in-r"><i class="fa fa-check"></i><b>3.4</b> Application in R</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.4.1</b> Decision trees</a></li>
<li class="chapter" data-level="3.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.4.2</b> Bagging</a></li>
<li class="chapter" data-level="3.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-1"><i class="fa fa-check"></i><b>3.4.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4.4</b> Boosting</a></li>
<li class="chapter" data-level="3.4.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost-1"><i class="fa fa-check"></i><b>3.4.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#lab-section-1"><i class="fa fa-check"></i><b>3.5</b> Lab section</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#LabClassification"><i class="fa fa-check"></i><b>3.5.1</b> Fitting Classification Trees</a></li>
<li class="chapter" data-level="3.5.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#fitting-regression-trees"><i class="fa fa-check"></i><b>3.5.2</b> Fitting Regression Trees</a></li>
<li class="chapter" data-level="3.5.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-and-random-forests"><i class="fa fa-check"></i><b>3.5.3</b> Bagging and Random Forests</a>
<ul>
<li class="chapter" data-level="3.5.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-1"><i class="fa fa-check"></i><b>3.5.3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.5.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forest"><i class="fa fa-check"></i><b>3.5.3.2</b> Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="3.5.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-1"><i class="fa fa-check"></i><b>3.5.4</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercies"><i class="fa fa-check"></i><b>3.6</b> Exercies</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-7---decision-tree-assessment"><i class="fa fa-check"></i><b>3.6.1</b> Exercise 7 - Decision Tree Assessment</a></li>
<li class="chapter" data-level="3.6.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#but-what-does-it-really-show-ntrees-are-fixed-at-500"><i class="fa fa-check"></i><b>3.6.2</b> But what does it really show?, ntrees are fixed at 500</a></li>
<li class="chapter" data-level="3.6.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-8---dtrfboosting"><i class="fa fa-check"></i><b>3.6.3</b> Exercise 8 - DT/RF/Boosting</a></li>
<li class="chapter" data-level="3.6.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-9---decision-tree-pruning"><i class="fa fa-check"></i><b>3.6.4</b> Exercise 9 - Decision Tree / Pruning</a></li>
<li class="chapter" data-level="3.6.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-10---boostinggamlinearregbagging---comparison"><i class="fa fa-check"></i><b>3.6.5</b> Exercise 10 - Boosting/GAM/LinearReg/Bagging - Comparison</a></li>
<li class="chapter" data-level="3.6.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-11---boosting"><i class="fa fa-check"></i><b>3.6.6</b> Exercise 11 - Boosting</a></li>
<li class="chapter" data-level="3.6.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-12"><i class="fa fa-check"></i><b>3.6.7</b> Exercise 12</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#casestudy---predicting-algae-blooms"><i class="fa fa-check"></i><b>3.7</b> Casestudy - Predicting Algae Blooms</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#background-1"><i class="fa fa-check"></i><b>3.7.1</b> 1. Background</a></li>
<li class="chapter" data-level="3.7.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#case-study-business-understanding-phase-1"><i class="fa fa-check"></i><b>3.7.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="3.7.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-data-data-understanding-phase-1"><i class="fa fa-check"></i><b>3.7.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="3.7.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#requirements"><i class="fa fa-check"></i><b>3.7.4</b> 4. Requirements:</a>
<ul>
<li class="chapter" data-level="3.7.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-1-understand-and-import-data-properly"><i class="fa fa-check"></i><b>3.7.4.1</b> 4.1 Task 1: Understand and import data properly</a></li>
<li class="chapter" data-level="3.7.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-2-inspect-your-data-and-do-the-required-variable-adaptations-and-transformations"><i class="fa fa-check"></i><b>3.7.4.2</b> 4.2 Task 2: Inspect your data and do the required variable adaptations and transformations</a></li>
<li class="chapter" data-level="3.7.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-3-build-one-or-several-predictive-models-and-evaluate-their-performance."><i class="fa fa-check"></i><b>3.7.4.3</b> 4.3 Task 3: Build one or several predictive models and evaluate their performance.</a>
<ul>
<li class="chapter" data-level="3.7.4.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#growing-a-regression-tree"><i class="fa fa-check"></i><b>3.7.4.3.1</b> 4.3.1. Growing a regression tree</a></li>
<li class="chapter" data-level="3.7.4.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#pruning-the-tree"><i class="fa fa-check"></i><b>3.7.4.3.2</b> 4.3.2 Pruning the tree</a></li>
<li class="chapter" data-level="3.7.4.3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#model-evaluation-and-selection"><i class="fa fa-check"></i><b>3.7.4.3.3</b> 4.3.3.Model evaluation and selection</a></li>
<li class="chapter" data-level="3.7.4.3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#ensemble-methods-random-forests"><i class="fa fa-check"></i><b>3.7.4.3.4</b> 4.3.4. Ensemble methods: Random Forests</a></li>
<li class="chapter" data-level="3.7.4.3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#predicting-in-the-test-sample"><i class="fa fa-check"></i><b>3.7.4.3.5</b> 4.3.5. Predicting in the test sample</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="4.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-vector-machines"><i class="fa fa-check"></i><b>4.1</b> Maximal Vector Machines</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#what-is-a-hyperplane"><i class="fa fa-check"></i><b>4.1.1</b> What is a hyperplane?</a></li>
<li class="chapter" data-level="4.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#classification-using-a-separating-hyperplane"><i class="fa fa-check"></i><b>4.1.2</b> Classification Using a Separating Hyperplane</a></li>
<li class="chapter" data-level="4.1.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-maximal-margin-classifier"><i class="fa fa-check"></i><b>4.1.3</b> The Maximal Margin Classifier</a></li>
<li class="chapter" data-level="4.1.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#ConstructMMC"><i class="fa fa-check"></i><b>4.1.4</b> Construction of the Maximal Margin Classifer</a></li>
<li class="chapter" data-level="4.1.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>4.1.5</b> The Non-separable Case</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifiers"><i class="fa fa-check"></i><b>4.2</b> Support Vector Classifiers</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#overview-of-the-support-vector-classifier"><i class="fa fa-check"></i><b>4.2.1</b> Overview of the Support Vector Classifier</a></li>
<li class="chapter" data-level="4.2.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#details-of-the-support-vector-classifer"><i class="fa fa-check"></i><b>4.2.2</b> Details of the Support Vector Classifer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>4.3</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#classification-with-non-linear-decision-boundary"><i class="fa fa-check"></i><b>4.3.1</b> Classification with non linear decision boundary</a></li>
<li class="chapter" data-level="4.3.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-support-vector-machine"><i class="fa fa-check"></i><b>4.3.2</b> The Support Vector Machine</a></li>
<li class="chapter" data-level="4.3.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#MoreThanTwoClasses"><i class="fa fa-check"></i><b>4.3.3</b> SVMs with More than Two Classes</a></li>
<li class="chapter" data-level="4.3.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#relationship-to-logistic-regression"><i class="fa fa-check"></i><b>4.3.4</b> Relationship to logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#process-of-kernels-methods"><i class="fa fa-check"></i><b>4.4</b> Process of kernels methods</a></li>
<li class="chapter" data-level="4.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#lab"><i class="fa fa-check"></i><b>4.5</b> Lab</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>4.5.1</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="4.5.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machine"><i class="fa fa-check"></i><b>4.5.2</b> Support Vector Machine</a></li>
<li class="chapter" data-level="4.5.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#roc-curves"><i class="fa fa-check"></i><b>4.5.3</b> ROC Curves</a></li>
<li class="chapter" data-level="4.5.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-with-multiple-classes"><i class="fa fa-check"></i><b>4.5.4</b> SVM with Multiple Classes</a></li>
<li class="chapter" data-level="4.5.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-to-gene-expression-data"><i class="fa fa-check"></i><b>4.5.5</b> Application to Gene Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#exercises-1"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Deep Learning Fundamentals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#basic-deep-learning"><i class="fa fa-check"></i><b>5.1</b> Basic Deep Learning</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#terms"><i class="fa fa-check"></i><b>5.1.1</b> Terms</a></li>
<li class="chapter" data-level="5.1.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#optimizers-loss-metrics-and-activation-rules"><i class="fa fa-check"></i><b>5.1.2</b> Optimizers, Loss, Metrics and Activation rules</a>
<ul>
<li class="chapter" data-level="5.1.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#gradient-descents"><i class="fa fa-check"></i><b>5.1.2.1</b> Gradient Descents</a></li>
</ul></li>
<li class="chapter" data-level="5.1.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#example-with-image-recognizion"><i class="fa fa-check"></i><b>5.1.3</b> Example with image recognizion</a></li>
<li class="chapter" data-level="5.1.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#model-building"><i class="fa fa-check"></i><b>5.1.4</b> Model building</a></li>
<li class="chapter" data-level="5.1.5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#validating-the-model"><i class="fa fa-check"></i><b>5.1.5</b> Validating the model</a></li>
<li class="chapter" data-level="5.1.6" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#overfitting-underfitting"><i class="fa fa-check"></i><b>5.1.6</b> Overfitting / Underfitting</a>
<ul>
<li class="chapter" data-level="5.1.6.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#hyperparameters"><i class="fa fa-check"></i><b>5.1.6.1</b> Hyperparameters:</a></li>
<li class="chapter" data-level="5.1.6.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#regularization"><i class="fa fa-check"></i><b>5.1.6.2</b> Regularization:</a></li>
<li class="chapter" data-level="5.1.6.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#dropout"><i class="fa fa-check"></i><b>5.1.6.3</b> Dropout</a></li>
<li class="chapter" data-level="5.1.6.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#then-how-do-we-control-for-a-good-fit"><i class="fa fa-check"></i><b>5.1.6.4</b> Then how do we control for a good fit?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#the-workflow-for-building-the-neural-network"><i class="fa fa-check"></i><b>5.2</b> The workflow for building the neural network</a></li>
<li class="chapter" data-level="5.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#examples"><i class="fa fa-check"></i><b>5.3</b> Examples</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#chapter-3---positive-negative-imdb-reviews"><i class="fa fa-check"></i><b>5.3.1</b> Chapter 3 - Positive / Negative IMDB reviews</a>
<ul>
<li class="chapter" data-level="5.3.1.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#extracting-the-data"><i class="fa fa-check"></i><b>5.3.1.1</b> Extracting the data</a></li>
<li class="chapter" data-level="5.3.1.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#preparing-the-data"><i class="fa fa-check"></i><b>5.3.1.2</b> Preparing the data</a></li>
<li class="chapter" data-level="5.3.1.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#building-the-model"><i class="fa fa-check"></i><b>5.3.1.3</b> Building the model</a></li>
<li class="chapter" data-level="5.3.1.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#assessing-model-performance"><i class="fa fa-check"></i><b>5.3.1.4</b> Assessing model performance</a></li>
</ul></li>
<li class="chapter" data-level="5.3.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#chapter-3---multiclass-classification---classifying-newswires"><i class="fa fa-check"></i><b>5.3.2</b> Chapter 3 - Multiclass classification - Classifying newswires</a>
<ul>
<li class="chapter" data-level="5.3.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#loading-the-data"><i class="fa fa-check"></i><b>5.3.2.1</b> Loading the data</a></li>
<li class="chapter" data-level="5.3.2.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#preparing-the-data-1"><i class="fa fa-check"></i><b>5.3.2.2</b> Preparing the data</a></li>
<li class="chapter" data-level="5.3.2.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#building-the-model-1"><i class="fa fa-check"></i><b>5.3.2.3</b> Building the model</a></li>
<li class="chapter" data-level="5.3.2.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#validating-the-model-model-assessment"><i class="fa fa-check"></i><b>5.3.2.4</b> Validating the model + model assessment</a></li>
<li class="chapter" data-level="5.3.2.5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#training-model-with-optimal-epochs"><i class="fa fa-check"></i><b>5.3.2.5</b> Training model with optimal epochs</a></li>
<li class="chapter" data-level="5.3.2.6" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#predictions-on-new-data"><i class="fa fa-check"></i><b>5.3.2.6</b> Predictions on new data</a></li>
</ul></li>
<li class="chapter" data-level="5.3.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#chapter-3---continous-prediction-a-regression-example---predicting-houseprices"><i class="fa fa-check"></i><b>5.3.3</b> Chapter 3 - Continous prediction / a regression example - Predicting houseprices</a>
<ul>
<li class="chapter" data-level="5.3.3.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#loading-the-data-1"><i class="fa fa-check"></i><b>5.3.3.1</b> Loading the data</a></li>
<li class="chapter" data-level="5.3.3.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#preparing-the-data-2"><i class="fa fa-check"></i><b>5.3.3.2</b> Preparing the data</a></li>
<li class="chapter" data-level="5.3.3.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#building-the-model-2"><i class="fa fa-check"></i><b>5.3.3.3</b> Building the model</a></li>
<li class="chapter" data-level="5.3.3.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#validating-the-approach-using-k-fold-validation"><i class="fa fa-check"></i><b>5.3.3.4</b> Validating the approach using K-fold validation</a>
<ul>
<li class="chapter" data-level="5.3.3.4.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#validation-with-more-iterations"><i class="fa fa-check"></i><b>5.3.3.4.1</b> Validation with more iterations</a></li>
<li class="chapter" data-level="5.3.3.4.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#tuning-amount-fo-hidden-layers"><i class="fa fa-check"></i><b>5.3.3.4.2</b> Tuning amount fo hidden layers</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#chapter-5---deep-learning-for-computer-vision"><i class="fa fa-check"></i><b>5.4</b> Chapter 5 - Deep learning for computer vision</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#definition-of-convoluted-network"><i class="fa fa-check"></i><b>5.4.1</b> Definition of convoluted network</a>
<ul>
<li class="chapter" data-level="5.4.1.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#tuning-parameters"><i class="fa fa-check"></i><b>5.4.1.1</b> Tuning parameters</a></li>
<li class="chapter" data-level="5.4.1.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#data-modeling-techniques"><i class="fa fa-check"></i><b>5.4.1.2</b> Data modeling techniques</a>
<ul>
<li class="chapter" data-level="5.4.1.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#padding"><i class="fa fa-check"></i><b>5.4.1.2.1</b> Padding</a></li>
<li class="chapter" data-level="5.4.1.2.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#strides"><i class="fa fa-check"></i><b>5.4.1.2.2</b> Strides</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.4.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#the-max-pooling-operation"><i class="fa fa-check"></i><b>5.4.2</b> The max-pooling operation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#lecture-notes-1"><i class="fa fa-check"></i><b>5.5</b> Lecture notes</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#lecture-1"><i class="fa fa-check"></i><b>5.5.1</b> Lecture 1</a></li>
<li class="chapter" data-level="5.5.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#lecture-2"><i class="fa fa-check"></i><b>5.5.2</b> Lecture 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="structuring-data-transformation-and-model-assessments.html"><a href="structuring-data-transformation-and-model-assessments.html"><i class="fa fa-check"></i><b>6</b> Structuring data transformation and model assessments</a></li>
<li class="chapter" data-level="7" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html"><i class="fa fa-check"></i><b>7</b> Github and CSS styling</a>
<ul>
<li class="chapter" data-level="7.1" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html#managing-github"><i class="fa fa-check"></i><b>7.1</b> Managing GitHub</a></li>
<li class="chapter" data-level="7.2" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html#css-styling"><i class="fa fa-check"></i><b>7.2</b> CSS Styling</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>8</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning-fundamentals" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Deep Learning Fundamentals</h1>
<div id="basic-deep-learning" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Basic Deep Learning</h2>
<p>The reason we call it deep learning, is that the method apply many levels where the data is manipulated and wrangled to attempt to extract meaning behind the input data. All the layers are increasingly different from the input data. In the book the apply the analogy, <em>that one can think of this as a multistage way to learn data representations <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018</a>, p 9)</span>.</em></p>
<p>The overall process and list of tasks to do is very similar to what we know from machine learning. It goes:</p>
<div class="figure">
<img src="Images/fig.1.9.png" id="feedbackcycle" width="414" alt="" />
<p class="caption">Feedback Cycle</p>
</div>
<p>Hence we see that first we train the model, then validate. Based on the validation to the true targets we assess the appropiate loss function and then optimize each weight of the layers.</p>
<p>The model starts in practice with random weights and then starts mutating to see if the loss function is optimized, at a certain point, one may say, that the model should not be altered any more.</p>
<p>Therefore, we still see that the bias-variance-tradeoff still applies and thus also train optimism, which one has to look out for.</p>
<div id="terms" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Terms</h3>
<table>
<caption>Terminology</caption>
<colgroup>
<col width="4%" />
<col width="95%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tensor</td>
<td><p>We have tensors in n-dimensions, e.g.:</p>
<ol style="list-style-type: decimal">
<li>0 dimensional, this is practically a scalar</li>
<li>1 dimensional, what we know as a vector</li>
<li>2 dimensional, what we know as matrices, hence we have rows and columns</li>
<li>3 dimensional, now we have an array</li>
<li>4 dimensional, get more difficult to draw. e.g., collection of images.</li>
<li>5 dimensional, even more difficult, e.g., video. See example on <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018, 34</a>)</span></li>
</ol></td>
</tr>
<tr class="even">
<td>Axis</td>
<td>A each <strong>dimension</strong> in a tensor is also called an <strong>axis</strong>.</td>
</tr>
<tr class="odd">
<td>Array</td>
<td><p>This is when you accumulate data in scalars, matrices or 3d tensor. Hence you have several ‘pages’ or chunks. E.g., if you have an image with 28 pixels. Then each picture is a matrix 28x28. Then if you have several matrices, then you create an array of three dimension.</p>
<p>You can also have three dimensional data and e.g., when collected over time it constructs a 4 dimensional array <strong>i.e. 4D Tensor, i.e. tensor with 4 axis’</strong>.</p>
<p>With video data, you will actually get a 5D tensor.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p></td>
</tr>
<tr class="even">
<td>Rank</td>
<td>= number of axis’ in a tensor</td>
</tr>
<tr class="odd">
<td>Batch</td>
<td>i.e., samples axis, i.e., samples dimension. DL does not process all data at once hence you load through batches. hence a 3D tensor will could have a batch of <code>train_images[1:128,,]</code></td>
</tr>
<tr class="even">
<td>Class</td>
<td>These are the categories in the scenario.</td>
</tr>
<tr class="odd">
<td>Sample</td>
<td>These are the data points</td>
</tr>
<tr class="even">
<td>Label</td>
<td>This is the specific class of each sample.</td>
</tr>
<tr class="odd">
<td>Weights</td>
<td>These are the <strong>trainable parameters</strong> that are assigned to the each layer in the learner, see <a href="#feecbackcycle">#feedbackcycle</a>.</td>
</tr>
<tr class="even">
<td>mini-SGD</td>
<td>The mini-batch stochastic gradient descent. This is an approach to iteratively alter the weights for the layers and then gradually decent towards a lower loss. As with gradient boosting, we can apply the same analogy and control how much the learner is changing at each iteration. Hence big chunks, then we may require fewer iterations and less computation, but we may also miss a dip, or the descent may get completely out of control.</td>
</tr>
<tr class="odd">
<td>Densely connected</td>
<td>This is a fully connected network where all neurons in the adjacent layers are connected.</td>
</tr>
<tr class="even">
<td>Epochs</td>
<td>The amount of iterations over all samples in the train tensors. Notice that each epoch (iteration) can run in several batches.</td>
</tr>
<tr class="odd">
<td>Forward Pass</td>
<td>This is the first time you build the model. This often selects random weights for each of the layers</td>
</tr>
<tr class="even">
<td>Backward Pass</td>
<td>This is when we have built the model and now we want to update the weights to reduce the loss.</td>
</tr>
<tr class="odd">
<td>Backpropagation</td>
<td>This is the concept of updating the weights. For this, one must be aware of the term - <em>momentum</em>.</td>
</tr>
<tr class="even">
<td>Momentum</td>
<td>For the backpropagation, the momentum is the thrust of the descent. This is an attempt to find the global minimu, so we don’t get stuck at a local minimum.</td>
</tr>
<tr class="odd">
<td>Identity function</td>
<td>This is merely a function the entirely replicates the training data.</td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Tensors:</p>
<p>4D: that is for instance pictures, we see that we have a height, width and the color channels (RGB channels) and then you have n sets of these. Hence if you had 100 pictures, then you have 100 times 3 slices, one for each of the color channel. See it graphically below.</p>
<p><img src="Images/paste-53ED65D3.png" width="305" /></p>
<p>5D: These are represented as arrays and this captures video data.</p>
</div>
<div id="optimizers-loss-metrics-and-activation-rules" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Optimizers, Loss, Metrics and Activation rules</h3>
<p>As written previously, we want to mimize the loss and thus optimally find the global minimum, although that is often easier said than done. If we are working with only few layers, e.g. N = 3, then the minimum can analytically be derived, but in practice one will often have thousands of layers. Hence it gets practically impossible. Therefore, we attempt to lower the loss gradually, as quick as possible/reasonable, while still preserving control over the descent.</p>
<p>Loss function is also called the objection function.</p>
<p>To minimize the loss, we apply <strong>optimizers</strong>, these are examples:</p>
<ol style="list-style-type: decimal">
<li>Momentum</li>
<li>Adagrad</li>
<li>RMSProp</li>
<li>And others</li>
</ol>
<p>The <strong>loss</strong> that we want to minimize, examples:</p>
<ol style="list-style-type: decimal">
<li>Binary Crossentropy. Often applied in a binary prediction setting. This is very much applied when you want to account for the probability of class 0 or 1, hence if you predict 0.45 and 0.1 (indicating that it will be class 0), then the 0.1 will lead to a bigger penalty than the 0.45 level. Compared to for instance accuracy, where you are either right or wrong.</li>
<li>Mean Squared error.</li>
<li>Categorical crossentropy, used in a multiclass setting, where classes are one-hot encoded.</li>
<li>Sparse Categorical Crossentropy, used in a multiclass setting, where classes are assigned a number, sort of an ID.</li>
<li>Mean absolute error.</li>
</ol>
<p><strong>Metrics:</strong></p>
<p><em>This is merely an additional metric, that one wants to track, but it is not intended to optimize against.</em></p>
<ol style="list-style-type: decimal">
<li>Accuracy</li>
<li>Metric Binary accuracy</li>
</ol>
<p><strong>Activation rules:</strong></p>
<p><em>Activation rules are what makes the NNET dynamic. They also directly influence the output of the layer and one can use different activation in each layer. Here are some examples:</em></p>
<ol style="list-style-type: decimal">
<li><p>Relu: very commonly used, rules</p></li>
<li><p>Sigmoid: grants probabilities similar to logistic regression</p></li>
<li><p>Elu</p></li>
<li><p>Prely</p></li>
<li><p>Tanh</p></li>
</ol>
<div id="gradient-descents" class="section level4" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> Gradient Descents</h4>
<p>This is about finding the global minimum in the loss. We have three approaches to this:</p>
<ol style="list-style-type: decimal">
<li><p>Batch Gradient Descent: This use all the data</p></li>
<li><p>Stochastic Gradient Descent: This use only single observations</p></li>
<li><p>Mini-batch Gradient Descent: This is more of a hybrid, where we must specify the batch size. This also introduce a new hyperparameter, that we can tune the model with.</p>
<ol style="list-style-type: decimal">
<li><p>We often see that the batch size is $2^s$, as that will lead to 32, 64, 128 etc. that fits computer memory very well.</p></li>
<li><p>Notice, that this is not epochs. The difference between this and epochs, is that epochs decide how many times we will circulate the training set.</p></li>
</ol></li>
</ol>
</div>
</div>
<div id="example-with-image-recognizion" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Example with image recognizion</h3>
<p><em>This is based on <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018</a>, ch. 2.5.)</span>.</em></p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="deep-learning-fundamentals.html#cb450-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb450-2"><a href="deep-learning-fundamentals.html#cb450-2" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span></code></pre></div>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="deep-learning-fundamentals.html#cb451-1" aria-hidden="true" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb451-2"><a href="deep-learning-fundamentals.html#cb451-2" aria-hidden="true" tabindex="-1"></a>train_labels <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb451-3"><a href="deep-learning-fundamentals.html#cb451-3" aria-hidden="true" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb451-4"><a href="deep-learning-fundamentals.html#cb451-4" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span>
<span id="cb451-5"><a href="deep-learning-fundamentals.html#cb451-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb451-6"><a href="deep-learning-fundamentals.html#cb451-6" aria-hidden="true" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(train_images, <span class="fu">c</span>(<span class="dv">60000</span>, <span class="dv">28</span> <span class="sc">*</span> <span class="dv">28</span>))</span>
<span id="cb451-7"><a href="deep-learning-fundamentals.html#cb451-7" aria-hidden="true" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> train_images <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb451-8"><a href="deep-learning-fundamentals.html#cb451-8" aria-hidden="true" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(test_images, <span class="fu">c</span>(<span class="dv">10000</span>, <span class="dv">28</span> <span class="sc">*</span> <span class="dv">28</span>))</span>
<span id="cb451-9"><a href="deep-learning-fundamentals.html#cb451-9" aria-hidden="true" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> test_images <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb451-10"><a href="deep-learning-fundamentals.html#cb451-10" aria-hidden="true" tabindex="-1"></a>train_labels <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(train_labels)</span>
<span id="cb451-11"><a href="deep-learning-fundamentals.html#cb451-11" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(test_labels)</span></code></pre></div>
</div>
<div id="model-building" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Model building</h3>
<p>There are three steps:</p>
<p><strong>Step 1: Define</strong></p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="deep-learning-fundamentals.html#cb452-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb452-2"><a href="deep-learning-fundamentals.html#cb452-2" aria-hidden="true" tabindex="-1"></a>network <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> <span class="co">#We create a sequential NNet.</span></span>
<span id="cb452-3"><a href="deep-learning-fundamentals.html#cb452-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">512</span> <span class="co">#The hidden layer</span></span>
<span id="cb452-4"><a href="deep-learning-fundamentals.html#cb452-4" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">activation =</span> <span class="st">&quot;relu&quot;</span></span>
<span id="cb452-5"><a href="deep-learning-fundamentals.html#cb452-5" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">28</span> <span class="sc">*</span> <span class="dv">28</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb452-6"><a href="deep-learning-fundamentals.html#cb452-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">10</span> <span class="co">#The final layer</span></span>
<span id="cb452-7"><a href="deep-learning-fundamentals.html#cb452-7" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span></code></pre></div>
<p>A sequencial model:</p>
<p><img src="Images/paste-791EFBCF.png" width="262" /></p>
<p>We see that the pipe operater creates the two layeres. This is the most common type of neural network.</p>
<p><code>layer_dense()</code> is creating a <em>fully connected network</em>. Menaing that we see that each neurons are connected to each of the neurons.</p>
<p><em>It is also a feed forward neural network,</em> meaning that it is going forward, it cannot go backwards.</p>
<p>Notice that the last <code>layer_dense()</code> will be the output layer, and then all layers before that, are hidden networks.</p>
<p>We see that the hidden layers are one of the tuning parameters and try to find out by tuning. This is very much engineering and not very theory heavy. It is key to experiment and test you way forward.</p>
<p><strong><em>Then what can we put into each layer?</em></strong></p>
<ul>
<li><p>Units: That is the number of nodes (neurons) in the layer. <span style="color: red">This is a tuning parameter!</span>. <em>Notice that if the layer is the last layer</em>, hence the output, then the amount of neurons must mach the amount of categories that we want to predict. Examples:</p>
<ul>
<li><p>If we have a continous variable, then we have units = 1</p></li>
<li><p>If we have a binary variable, then we have units = 2</p></li>
<li><p>If we want to categorize numbers 0 - 9, then units = 10</p></li>
</ul></li>
<li><p>Activation: We see that we use relu above. When data goes through a node, it goes through two steps:</p>
<ul>
<li><p>Relu:</p>
<ul>
<li><div>
<ol style="list-style-type: decimal">
<li>Linear transformation. This assigning some weights to each of the variables. We also have a constant called <em>bias</em>, but it is not like bias variance tradeoff as we know it. The operations are very similar to linear regression, when you zoom into each note. This score will then be passed on to the <code>relu</code> function. This makes sense as the show with momentum in the book, where it gets an idea of whether you are going up or down on the loss.</li>
<li>Activation function, here we see <code>relu</code>, it says that, if the input is below 0 (negative), then return 0, but if it is positive, then it returns the value itself. That is extremely simple but works really well. <span style="color: red">This can very often be the default to use.</span> Hence this is going to decide neuron is going to ignite, and then it defines ‘by how much.’ Then this will get passed on to the following node. And the higher the number for Relu, the larger is the probability for the next node is also going to ignite. <em>Notice that this is based on the score from step 1.</em></li>
</ol>
</div></li>
</ul></li>
<li><p>Softmax: This is basically generalization of logistic regression, but where we have more than 2 outcomes. With this we get probabilities out, where ALL of the outcomes sum to 1. This is the default for the multi-class setting.</p></li>
<li><p>Sigmoid: When we have a binary outcome. sigmoid is the s-curve we see, e.g., with logistic regression. So we get something on the probability scale.</p></li>
<li><p>Null: When we have a regression setting, i.e., continous variable.</p></li>
</ul></li>
<li><p>Input_shape:</p></li>
</ul>
<p><strong><em>Then what happens in each layer?</em></strong></p>
<p>It basically unfolds the complicated data and Jesper used origami to visualize how we unfold the figure to see the original shape of the figure.</p>
<p><strong>Step 2: Compile</strong></p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="deep-learning-fundamentals.html#cb453-1" aria-hidden="true" tabindex="-1"></a>network <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb453-2"><a href="deep-learning-fundamentals.html#cb453-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb453-3"><a href="deep-learning-fundamentals.html#cb453-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb453-4"><a href="deep-learning-fundamentals.html#cb453-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb453-5"><a href="deep-learning-fundamentals.html#cb453-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We see that the loss is the categorical_crossentropy. This measure is going to define how far the predicted values and actual values are from each other.</p>
<p><strong>Step 3: Train</strong></p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="deep-learning-fundamentals.html#cb454-1" aria-hidden="true" tabindex="-1"></a>network <span class="sc">%&gt;%</span> <span class="fu">fit</span>(train_images, train_labels, <span class="at">epochs =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">128</span>)</span></code></pre></div>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="deep-learning-fundamentals.html#cb455-1" aria-hidden="true" tabindex="-1"></a>metrics <span class="ot">&lt;-</span> network <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(test_images, test_labels)</span>
<span id="cb455-2"><a href="deep-learning-fundamentals.html#cb455-2" aria-hidden="true" tabindex="-1"></a>metrics</span></code></pre></div>
<pre><code>##       loss   accuracy 
## 0.06696557 0.97970003</code></pre>
</div>
<div id="validating-the-model" class="section level3" number="5.1.5">
<h3><span class="header-section-number">5.1.5</span> Validating the model</h3>
<p>One should always use a hold-out sample both consisting of a validation and a test. If we want a really good estimate of the out-of sample performance, then we want to go for a large test set, but it will also take out a lot of information that the model can learn from.</p>
<p>If we want to validate using all of the train data, we can estimate the out of sample using cross-validation. If we don’t have much data, then we can use iterated k-fold cross validation. Although, using iterated k-fold CV is often a symptom of having too little data.</p>
</div>
<div id="overfitting-underfitting" class="section level3" number="5.1.6">
<h3><span class="header-section-number">5.1.6</span> Overfitting / Underfitting</h3>
<p>As with any other learner we are interested in making the right fit to the data, but when is that?</p>
<p>We know that adding complexity also adds more assumptions and possibility of overfitting to the training data. But without some complexity in the model, we see that the learner will not learn enough.</p>
<p>Hence the question is, how are we able to introduce complexity to the model without overfitting to the train data, and perhaps enabling the model to merely memorize the train data. In general we have three themes that we can tune and introduce to the model, namely hyperparameters and regularization. Outside of the model, we can also play around with the variables through feature engineering.</p>
<div id="hyperparameters" class="section level4" number="5.1.6.1">
<h4><span class="header-section-number">5.1.6.1</span> Hyperparameters:</h4>
<p>We have a wide range of hyperparameters that we are able to tune to optimize the model. For instance:</p>
<ol style="list-style-type: decimal">
<li>Adding / removing layers</li>
<li>Adjusting amount of units, but remember the rule of thumb, that it should either be a funnel or a tunnel.</li>
</ol>
</div>
<div id="regularization" class="section level4" number="5.1.6.2">
<h4><span class="header-section-number">5.1.6.2</span> Regularization:</h4>
<p>As we know from any other machine learning, we have regularization methods, that deal with overfitting in a high dimensional space.</p>
<ol style="list-style-type: decimal">
<li>L1 Regularization: The cost is added proportion to <em>the absolute value</em> of the the weight coefficients.<em>Recall that the weight coefficients decide the effect of the layers. This is an iterative process.</em> This is similar to Lasso, hence the weights can be exactly 0.</li>
<li>L2 Regularization: The cost added is proportional to the <em>square of the value of the coefficients</em>. This is also called <em>weight decay (but only in context of neural networks)</em>. This is similar to Ridge Regression, where the weights can be set to close to 0.</li>
<li>L1 L2 Regularization: We can also introduce an L1 and L2 regularization, which is similar to the elastic net.</li>
</ol>
<div class="lightgreybox">
<p>In R, this is called by the `kernel_regularizer = ‘regularizer_l2’ or ‘regularizer_l1’`</p>
</div>
<p>Thus in the fitting process when introducing regularization each weight will add more penalty to the loss, hence in the training process one constrains the model to overfit to the training data as the weights will add a loss.</p>
<p>So in principle, when we add penalty, we make it harder for the model to learn, hence it leads to one must use more epochs.</p>
</div>
<div id="dropout" class="section level4" number="5.1.6.3">
<h4><span class="header-section-number">5.1.6.3</span> Dropout</h4>
<p>We see that we add an artificial layer in the network, that is the dropout. This layer will then in the train process randomly remove neurons and in the test run, it will scale neurons randomly.</p>
<p>The purpose is to intentionally introduce noise in the model to prevent the network to simply memorize the data.</p>
<p>The analogy from the developers of the method is that in the bank they are often changing the tellers at the desk. The reason is that we need different people to deal with the customers and perhaps detect fraud, so a potential fraudulent person can’t plan out what teller to address. Hence we introduce an element of randomness.</p>
<p>Visually it looks like this:</p>
<div class="figure">
<img src="Images/paste-40E9CE44.png" title="Dropout" alt="" />
<p class="caption">Dropout</p>
</div>
</div>
<div id="then-how-do-we-control-for-a-good-fit" class="section level4" number="5.1.6.4">
<h4><span class="header-section-number">5.1.6.4</span> Then how do we control for a good fit?</h4>
<p>One must intentionally overfit, or intentionally start underfitting, to then start tampering up or down.</p>
</div>
</div>
</div>
<div id="the-workflow-for-building-the-neural-network" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> The workflow for building the neural network</h2>
<p>It is a seven step process.</p>
<ol style="list-style-type: decimal">
<li><p>Identify the problem</p></li>
<li><p>Choosing the measure of success / loss function and optimizer</p></li>
<li><p>Deciding on an evaluation protocol: e.g.,</p>
<ol style="list-style-type: decimal">
<li><p>K-fold CV,</p></li>
<li><p>hold-out validation set,</p></li>
<li><p>doing iterated K-fold CV. Often used when little data is available.</p></li>
</ol></li>
<li><p>Preparing the data in tensors, see rules of thumb <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018</a>, p.g 104)</span></p></li>
<li><p>Developing the mode. Remember to compare with the baseline, e.g., by predicting the most dominant class, if we are in a classification setting.</p>
<ol style="list-style-type: decimal">
<li><p>Also this is about setting the right activation for the last layer.</p></li>
<li><p><img src="Images/paste-7F3DC7AE.png" /></p></li>
</ol></li>
<li><p>Scaling up: develop a model that overfits the data and then is able to learn a lot from the train data.</p>
<ol style="list-style-type: decimal">
<li><p>Add layers</p></li>
<li><p>Make the layers bigger</p></li>
<li><p>Train for more epochs</p></li>
</ol></li>
<li><p>Adding regularization to the model and tuning hyperparameters</p></li>
</ol>
</div>
<div id="examples" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Examples</h2>
<div id="chapter-3---positive-negative-imdb-reviews" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Chapter 3 - Positive / Negative IMDB reviews</h3>
<div id="extracting-the-data" class="section level4" number="5.3.1.1">
<h4><span class="header-section-number">5.3.1.1</span> Extracting the data</h4>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="deep-learning-fundamentals.html#cb457-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Loading the data</span></span>
<span id="cb457-2"><a href="deep-learning-fundamentals.html#cb457-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(keras)</span>
<span id="cb457-3"><a href="deep-learning-fundamentals.html#cb457-3" aria-hidden="true" tabindex="-1"></a>  imdb <span class="ot">&lt;-</span> <span class="fu">dataset_imdb</span>(<span class="at">num_words =</span> <span class="dv">10000</span>) <span class="co">#We restrict outselves to the top 10.000 words.</span></span>
<span id="cb457-4"><a href="deep-learning-fundamentals.html#cb457-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">c</span>(train_data, train_labels), <span class="fu">c</span>(test_data, test_labels)) <span class="sc">%&lt;-%</span> imdb <span class="co">#Unpacks the elements</span></span></code></pre></div>
<p>The words are all indexed and ranked, hence the reviews consist of numbers, we will later revert this.</p>
<p>Before we start building the model, we can explore the data a bit. One must notice, that the data that is in the package is already prepared to b</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="deep-learning-fundamentals.html#cb458-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(train_data[[<span class="dv">1</span>]]) <span class="co">#We see that each word is changed into a number. </span><span class="al">NOTE</span><span class="co"> this is a just a snip of the full review</span></span>
<span id="cb458-2"><a href="deep-learning-fundamentals.html#cb458-2" aria-hidden="true" tabindex="-1"></a>train_labels[[<span class="dv">1</span>]] <span class="co">#We see that 1 = positive, 0 = negative</span></span>
<span id="cb458-3"><a href="deep-learning-fundamentals.html#cb458-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-4"><a href="deep-learning-fundamentals.html#cb458-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Reverting to english words</span></span>
<span id="cb458-5"><a href="deep-learning-fundamentals.html#cb458-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># word_index is a dictionary mapping words to an integer index</span></span>
<span id="cb458-6"><a href="deep-learning-fundamentals.html#cb458-6" aria-hidden="true" tabindex="-1"></a>  word_index <span class="ot">&lt;-</span> <span class="fu">dataset_imdb_word_index</span>()</span>
<span id="cb458-7"><a href="deep-learning-fundamentals.html#cb458-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># We reverse it, mapping integer indices to words</span></span>
<span id="cb458-8"><a href="deep-learning-fundamentals.html#cb458-8" aria-hidden="true" tabindex="-1"></a>  reverse_word_index <span class="ot">&lt;-</span> <span class="fu">names</span>(word_index)</span>
<span id="cb458-9"><a href="deep-learning-fundamentals.html#cb458-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(reverse_word_index) <span class="ot">&lt;-</span> word_index</span>
<span id="cb458-10"><a href="deep-learning-fundamentals.html#cb458-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># We decode the review; note that our indices were offset by 3</span></span>
<span id="cb458-11"><a href="deep-learning-fundamentals.html#cb458-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># because 0, 1 and 2 are reserved indices for &quot;padding&quot;, &quot;start of sequence&quot;, and &quot;unknown&quot;.</span></span>
<span id="cb458-12"><a href="deep-learning-fundamentals.html#cb458-12" aria-hidden="true" tabindex="-1"></a>  decoded_review <span class="ot">&lt;-</span> <span class="fu">sapply</span>(train_data[[<span class="dv">1</span>]], <span class="cf">function</span>(index) {</span>
<span id="cb458-13"><a href="deep-learning-fundamentals.html#cb458-13" aria-hidden="true" tabindex="-1"></a>    word <span class="ot">&lt;-</span> <span class="cf">if</span> (index <span class="sc">&gt;=</span> <span class="dv">3</span>) reverse_word_index[[<span class="fu">as.character</span>(index <span class="sc">-</span> <span class="dv">3</span>)]]</span>
<span id="cb458-14"><a href="deep-learning-fundamentals.html#cb458-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(word)) word <span class="cf">else</span> <span class="st">&quot;?&quot;</span></span>
<span id="cb458-15"><a href="deep-learning-fundamentals.html#cb458-15" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb458-16"><a href="deep-learning-fundamentals.html#cb458-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb458-17"><a href="deep-learning-fundamentals.html#cb458-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Printing one review</span></span>
<span id="cb458-18"><a href="deep-learning-fundamentals.html#cb458-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(decoded_review)</span></code></pre></div>
<pre><code>##  int [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...
## [1] 1
## ? this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&#39;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all</code></pre>
</div>
<div id="preparing-the-data" class="section level4" number="5.3.1.2">
<h4><span class="header-section-number">5.3.1.2</span> Preparing the data</h4>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="deep-learning-fundamentals.html#cb460-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Preparing the data</span></span>
<span id="cb460-2"><a href="deep-learning-fundamentals.html#cb460-2" aria-hidden="true" tabindex="-1"></a>  vectorize_sequences <span class="ot">&lt;-</span> <span class="cf">function</span>(sequences, <span class="at">dimension =</span> <span class="dv">10000</span>) {</span>
<span id="cb460-3"><a href="deep-learning-fundamentals.html#cb460-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create an all-zero matrix of shape (len(sequences), dimension)</span></span>
<span id="cb460-4"><a href="deep-learning-fundamentals.html#cb460-4" aria-hidden="true" tabindex="-1"></a>    results <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">length</span>(sequences), <span class="at">ncol =</span> dimension)</span>
<span id="cb460-5"><a href="deep-learning-fundamentals.html#cb460-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(sequences))</span>
<span id="cb460-6"><a href="deep-learning-fundamentals.html#cb460-6" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Sets specific indices of results[i] to 1s</span></span>
<span id="cb460-7"><a href="deep-learning-fundamentals.html#cb460-7" aria-hidden="true" tabindex="-1"></a>      results[i, sequences[[i]]] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb460-8"><a href="deep-learning-fundamentals.html#cb460-8" aria-hidden="true" tabindex="-1"></a>    results</span>
<span id="cb460-9"><a href="deep-learning-fundamentals.html#cb460-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb460-10"><a href="deep-learning-fundamentals.html#cb460-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb460-11"><a href="deep-learning-fundamentals.html#cb460-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Our vectorized training data</span></span>
<span id="cb460-12"><a href="deep-learning-fundamentals.html#cb460-12" aria-hidden="true" tabindex="-1"></a>  x_train <span class="ot">&lt;-</span> <span class="fu">vectorize_sequences</span>(train_data)</span>
<span id="cb460-13"><a href="deep-learning-fundamentals.html#cb460-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Our vectorized test data</span></span>
<span id="cb460-14"><a href="deep-learning-fundamentals.html#cb460-14" aria-hidden="true" tabindex="-1"></a>  x_test <span class="ot">&lt;-</span> <span class="fu">vectorize_sequences</span>(test_data)</span>
<span id="cb460-15"><a href="deep-learning-fundamentals.html#cb460-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb460-16"><a href="deep-learning-fundamentals.html#cb460-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Our vectorized labels</span></span>
<span id="cb460-17"><a href="deep-learning-fundamentals.html#cb460-17" aria-hidden="true" tabindex="-1"></a>  y_train <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(train_labels)</span>
<span id="cb460-18"><a href="deep-learning-fundamentals.html#cb460-18" aria-hidden="true" tabindex="-1"></a>  y_test <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(test_labels)</span></code></pre></div>
</div>
<div id="building-the-model" class="section level4" number="5.3.1.3">
<h4><span class="header-section-number">5.3.1.3</span> Building the model</h4>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="deep-learning-fundamentals.html#cb461-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Building the network</span></span>
<span id="cb461-2"><a href="deep-learning-fundamentals.html#cb461-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(keras)</span>
<span id="cb461-3"><a href="deep-learning-fundamentals.html#cb461-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb461-4"><a href="deep-learning-fundamentals.html#cb461-4" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb461-5"><a href="deep-learning-fundamentals.html#cb461-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">16</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">10000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb461-6"><a href="deep-learning-fundamentals.html#cb461-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">16</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb461-7"><a href="deep-learning-fundamentals.html#cb461-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb461-8"><a href="deep-learning-fundamentals.html#cb461-8" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb461-9"><a href="deep-learning-fundamentals.html#cb461-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb461-10"><a href="deep-learning-fundamentals.html#cb461-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Defining Optimizer, loss and metrics</span></span>
<span id="cb461-11"><a href="deep-learning-fundamentals.html#cb461-11" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb461-12"><a href="deep-learning-fundamentals.html#cb461-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>, <span class="co">#Build-in, one can create functions and call external functions also</span></span>
<span id="cb461-13"><a href="deep-learning-fundamentals.html#cb461-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>, <span class="co">#Build-in, one can create functions and call external functions also</span></span>
<span id="cb461-14"><a href="deep-learning-fundamentals.html#cb461-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>) <span class="co">#Build-in, one can create functions and call external functions also</span></span>
<span id="cb461-15"><a href="deep-learning-fundamentals.html#cb461-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb461-16"><a href="deep-learning-fundamentals.html#cb461-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb461-17"><a href="deep-learning-fundamentals.html#cb461-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb461-18"><a href="deep-learning-fundamentals.html#cb461-18" aria-hidden="true" tabindex="-1"></a><span class="co">#Validating the model</span></span>
<span id="cb461-19"><a href="deep-learning-fundamentals.html#cb461-19" aria-hidden="true" tabindex="-1"></a>  val_indices <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span> <span class="co">#We want the first 10.000 observations</span></span>
<span id="cb461-20"><a href="deep-learning-fundamentals.html#cb461-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb461-21"><a href="deep-learning-fundamentals.html#cb461-21" aria-hidden="true" tabindex="-1"></a>  x_val <span class="ot">&lt;-</span> x_train[val_indices,]</span>
<span id="cb461-22"><a href="deep-learning-fundamentals.html#cb461-22" aria-hidden="true" tabindex="-1"></a>  partial_x_train <span class="ot">&lt;-</span> x_train[<span class="sc">-</span>val_indices,]</span>
<span id="cb461-23"><a href="deep-learning-fundamentals.html#cb461-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb461-24"><a href="deep-learning-fundamentals.html#cb461-24" aria-hidden="true" tabindex="-1"></a>  y_val <span class="ot">&lt;-</span> y_train[val_indices]</span>
<span id="cb461-25"><a href="deep-learning-fundamentals.html#cb461-25" aria-hidden="true" tabindex="-1"></a>  partial_y_train <span class="ot">&lt;-</span> y_train[<span class="sc">-</span>val_indices]</span></code></pre></div>
<p>Now where we have created the learner, <code>keras</code> stores the loss and defined metrics, so we are able to see how the model performed throughout the iterations.</p>
</div>
<div id="assessing-model-performance" class="section level4" number="5.3.1.4">
<h4><span class="header-section-number">5.3.1.4</span> Assessing model performance</h4>
<p><em>Notice, that if we run the following code consecutively, the model appear to remember the previous runs. Hence one should run the model above.</em></p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="deep-learning-fundamentals.html#cb462-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb462-2"><a href="deep-learning-fundamentals.html#cb462-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">512</span></span>
<span id="cb462-3"><a href="deep-learning-fundamentals.html#cb462-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb462-4"><a href="deep-learning-fundamentals.html#cb462-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Retrieving history</span></span>
<span id="cb462-5"><a href="deep-learning-fundamentals.html#cb462-5" aria-hidden="true" tabindex="-1"></a>  history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>( <span class="co">#We fit the model</span></span>
<span id="cb462-6"><a href="deep-learning-fundamentals.html#cb462-6" aria-hidden="true" tabindex="-1"></a>    partial_x_train,</span>
<span id="cb462-7"><a href="deep-learning-fundamentals.html#cb462-7" aria-hidden="true" tabindex="-1"></a>    partial_y_train,</span>
<span id="cb462-8"><a href="deep-learning-fundamentals.html#cb462-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> epochs, <span class="co">#How many iterations?</span></span>
<span id="cb462-9"><a href="deep-learning-fundamentals.html#cb462-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> batch_size, <span class="co">#How many observations in each batch?</span></span>
<span id="cb462-10"><a href="deep-learning-fundamentals.html#cb462-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(x_val, y_val)</span>
<span id="cb462-11"><a href="deep-learning-fundamentals.html#cb462-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb462-12"><a href="deep-learning-fundamentals.html#cb462-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb462-13"><a href="deep-learning-fundamentals.html#cb462-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">#The numbers behind the plot</span></span>
<span id="cb462-14"><a href="deep-learning-fundamentals.html#cb462-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(tidyverse)</span>
<span id="cb462-15"><a href="deep-learning-fundamentals.html#cb462-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>(<span class="fu">cbind</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>epochs)</span>
<span id="cb462-16"><a href="deep-learning-fundamentals.html#cb462-16" aria-hidden="true" tabindex="-1"></a>                    ,history<span class="sc">$</span>metrics<span class="sc">$</span>loss</span>
<span id="cb462-17"><a href="deep-learning-fundamentals.html#cb462-17" aria-hidden="true" tabindex="-1"></a>                    ,history<span class="sc">$</span>metrics<span class="sc">$</span>val_loss</span>
<span id="cb462-18"><a href="deep-learning-fundamentals.html#cb462-18" aria-hidden="true" tabindex="-1"></a>                    ,history<span class="sc">$</span>metrics<span class="sc">$</span>accuracy</span>
<span id="cb462-19"><a href="deep-learning-fundamentals.html#cb462-19" aria-hidden="true" tabindex="-1"></a>                    ,history<span class="sc">$</span>metrics<span class="sc">$</span>val_accuracy)) <span class="sc">%&gt;%</span> </span>
<span id="cb462-20"><a href="deep-learning-fundamentals.html#cb462-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">setNames</span>(<span class="at">nm =</span> <span class="fu">c</span>(<span class="st">&quot;Iteration&quot;</span>,<span class="fu">names</span>(history<span class="sc">$</span>metrics)))</span></code></pre></div>
<p>As with any other learners, we see that we are able to overtrain, i.e. overfit, the model to the train data and the model is in fact just memorizing the train results. Hence running too many iterations does not appear to yield an appropriate model.</p>
<p>Recall that we are optimizing the loss and not the accuracy. Hence wee see that the loss starts to increase at some point, but it is difficult to deduct from the accuracy line. <strong><em>Notice, that it is not given that the lowest Loss = the highest accuracy.</em></strong></p>
</div>
</div>
<div id="chapter-3---multiclass-classification---classifying-newswires" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Chapter 3 - Multiclass classification - Classifying newswires</h3>
<p>In this section we classify Reuters newswires into 46 mutually exclusive topics. Thus we have more than two classes to predict. That also means that the last layer in the network will have 46 units, hence one for each of the classes, see <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018, 70</a>)</span>. The data is already loaded into the package.</p>
<div id="loading-the-data" class="section level4" number="5.3.2.1">
<h4><span class="header-section-number">5.3.2.1</span> Loading the data</h4>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="deep-learning-fundamentals.html#cb463-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb463-2"><a href="deep-learning-fundamentals.html#cb463-2" aria-hidden="true" tabindex="-1"></a>reuters <span class="ot">&lt;-</span> <span class="fu">dataset_reuters</span>(<span class="at">num_words =</span> <span class="dv">10000</span>) <span class="co">#Restricting to top 10.000 words.</span></span>
<span id="cb463-3"><a href="deep-learning-fundamentals.html#cb463-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(train_data, train_labels), <span class="fu">c</span>(test_data, test_labels)) <span class="sc">%&lt;-%</span> reuters</span></code></pre></div>
<p>We could do the same exploration of the data as with IMDB if we’d like to. For this example it is skipped.</p>
</div>
<div id="preparing-the-data-1" class="section level4" number="5.3.2.2">
<h4><span class="header-section-number">5.3.2.2</span> Preparing the data</h4>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="deep-learning-fundamentals.html#cb464-1" aria-hidden="true" tabindex="-1"></a>vectorize_sequences <span class="ot">&lt;-</span> <span class="cf">function</span>(sequences, <span class="at">dimension =</span> <span class="dv">10000</span>) {</span>
<span id="cb464-2"><a href="deep-learning-fundamentals.html#cb464-2" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">length</span>(sequences), <span class="at">ncol =</span> dimension)</span>
<span id="cb464-3"><a href="deep-learning-fundamentals.html#cb464-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(sequences))</span>
<span id="cb464-4"><a href="deep-learning-fundamentals.html#cb464-4" aria-hidden="true" tabindex="-1"></a>results[i, sequences[[i]]] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb464-5"><a href="deep-learning-fundamentals.html#cb464-5" aria-hidden="true" tabindex="-1"></a>results</span>
<span id="cb464-6"><a href="deep-learning-fundamentals.html#cb464-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb464-7"><a href="deep-learning-fundamentals.html#cb464-7" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">vectorize_sequences</span>(train_data)</span>
<span id="cb464-8"><a href="deep-learning-fundamentals.html#cb464-8" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">vectorize_sequences</span>(test_data)</span></code></pre></div>
<p>Recall that we have more than 2 outcomes. To deal with this, we have two possibilities:</p>
<ol style="list-style-type: decimal">
<li>Call the label an integer tensor</li>
<li>Use “one-hot” encoding. This is the same as one making dummies after the one-hot principle.</li>
</ol>
<p>We will use the one-hot encoding, creating vectors of 0’s and 1 in the place of the the specific category.</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="deep-learning-fundamentals.html#cb465-1" aria-hidden="true" tabindex="-1"></a>to_one_hot <span class="ot">&lt;-</span> <span class="cf">function</span>(labels, <span class="at">dimension =</span> <span class="dv">46</span>) {</span>
<span id="cb465-2"><a href="deep-learning-fundamentals.html#cb465-2" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">length</span>(labels), <span class="at">ncol =</span> dimension)</span>
<span id="cb465-3"><a href="deep-learning-fundamentals.html#cb465-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(labels))</span>
<span id="cb465-4"><a href="deep-learning-fundamentals.html#cb465-4" aria-hidden="true" tabindex="-1"></a>    results[i, labels[[i]] <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb465-5"><a href="deep-learning-fundamentals.html#cb465-5" aria-hidden="true" tabindex="-1"></a>  results</span>
<span id="cb465-6"><a href="deep-learning-fundamentals.html#cb465-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb465-7"><a href="deep-learning-fundamentals.html#cb465-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-8"><a href="deep-learning-fundamentals.html#cb465-8" aria-hidden="true" tabindex="-1"></a>one_hot_train_labels <span class="ot">&lt;-</span> <span class="fu">to_one_hot</span>(train_labels)</span>
<span id="cb465-9"><a href="deep-learning-fundamentals.html#cb465-9" aria-hidden="true" tabindex="-1"></a>one_hot_test_labels <span class="ot">&lt;-</span> <span class="fu">to_one_hot</span>(test_labels)</span>
<span id="cb465-10"><a href="deep-learning-fundamentals.html#cb465-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-11"><a href="deep-learning-fundamentals.html#cb465-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Alternative using Keras built in function:</span></span>
<span id="cb465-12"><a href="deep-learning-fundamentals.html#cb465-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># one_hot_train_labels &lt;- to_categorical(train_labels)</span></span>
<span id="cb465-13"><a href="deep-learning-fundamentals.html#cb465-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># one_hot_test_labels &lt;- to_categorical(test_labels)</span></span></code></pre></div>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="deep-learning-fundamentals.html#cb466-1" aria-hidden="true" tabindex="-1"></a>one_hot_train_labels <span class="sc">%&gt;%</span> <span class="fu">dim</span>()</span>
<span id="cb466-2"><a href="deep-learning-fundamentals.html#cb466-2" aria-hidden="true" tabindex="-1"></a>one_hot_test_labels <span class="sc">%&gt;%</span> <span class="fu">dim</span>()</span></code></pre></div>
<pre><code>## [1] 8982   46
## [1] 2246   46</code></pre>
<p>We see that there is a row for each sample and then a coloumn for each of the categories.</p>
<p>If one where to print each sample and identifying whether each word appears, one can visualize this with:</p>
<p><img src="Images/paste-0B7B7C34.png" width="220" /></p>
<p>Where we see that a white pixel = the specific word appears. We see that we have a lot of sparsity, meaning that very much black space (0’s).</p>
</div>
<div id="building-the-model-1" class="section level4" number="5.3.2.3">
<h4><span class="header-section-number">5.3.2.3</span> Building the model</h4>
<p>We are going to build a dequencial model, hence each layer can only process what is given from the previous layers. In this example we have even more categories and the model must be able to distinguish between more scenarios, hence 16 units in each layer as seen in section @ref(chapter-3—positive-negative-imdb-reviews). That is because what one layer leaves out, the following layers can use, hence we will apply more layers, in this example we use 64.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="deep-learning-fundamentals.html#cb468-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb468-2"><a href="deep-learning-fundamentals.html#cb468-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">10000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb468-3"><a href="deep-learning-fundamentals.html#cb468-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb468-4"><a href="deep-learning-fundamentals.html#cb468-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">46</span>, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>) <span class="co">#Softmax for probability distribution</span></span></code></pre></div>
<p>Botice that the input_shape for the first layer is 10.000, that is corresponding to the train partition of the data. As we see in the picture above, we have a lot of sparsity, we use 64 units.</p>
<div class="lightbluebox">
<p><strong>What shape do we want?</strong> We want to have a funnel shape or a tunnel shape, not a shape we we decrease amount of neurons and then later expand amount of neurons.</p>
</div>
<p>Notice that the last layer has units = no. of classes and the we use activation “softmax.” This layer will produce a probability distribution, where all entries sum to 1.</p>
<p>Now we must define optimizer, loss and metrics, we go for:</p>
<ul>
<li><p>Optimizer = rmsprop</p></li>
<li><p>Loss = Categorical crossentropy. This appear to be good in a multiclass setting</p></li>
<li><p>Metrics = We want to see accuracy on a running basis</p></li>
</ul>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="deep-learning-fundamentals.html#cb469-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb469-2"><a href="deep-learning-fundamentals.html#cb469-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb469-3"><a href="deep-learning-fundamentals.html#cb469-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb469-4"><a href="deep-learning-fundamentals.html#cb469-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb469-5"><a href="deep-learning-fundamentals.html#cb469-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Now we can validate the model using the data partitions.</p>
</div>
<div id="validating-the-model-model-assessment" class="section level4" number="5.3.2.4">
<h4><span class="header-section-number">5.3.2.4</span> Validating the model + model assessment</h4>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="deep-learning-fundamentals.html#cb470-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Validating the model</span></span>
<span id="cb470-2"><a href="deep-learning-fundamentals.html#cb470-2" aria-hidden="true" tabindex="-1"></a>val_indices <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb470-3"><a href="deep-learning-fundamentals.html#cb470-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-4"><a href="deep-learning-fundamentals.html#cb470-4" aria-hidden="true" tabindex="-1"></a>x_val <span class="ot">&lt;-</span> x_train[val_indices,]</span>
<span id="cb470-5"><a href="deep-learning-fundamentals.html#cb470-5" aria-hidden="true" tabindex="-1"></a>partial_x_train <span class="ot">&lt;-</span> x_train[<span class="sc">-</span>val_indices,]</span>
<span id="cb470-6"><a href="deep-learning-fundamentals.html#cb470-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-7"><a href="deep-learning-fundamentals.html#cb470-7" aria-hidden="true" tabindex="-1"></a>y_val <span class="ot">&lt;-</span> one_hot_train_labels[val_indices,]</span>
<span id="cb470-8"><a href="deep-learning-fundamentals.html#cb470-8" aria-hidden="true" tabindex="-1"></a>partial_y_train <span class="ot">=</span> one_hot_train_labels[<span class="sc">-</span>val_indices,]</span>
<span id="cb470-9"><a href="deep-learning-fundamentals.html#cb470-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-10"><a href="deep-learning-fundamentals.html#cb470-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Retriving history</span></span>
<span id="cb470-11"><a href="deep-learning-fundamentals.html#cb470-11" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb470-12"><a href="deep-learning-fundamentals.html#cb470-12" aria-hidden="true" tabindex="-1"></a>  partial_x_train,</span>
<span id="cb470-13"><a href="deep-learning-fundamentals.html#cb470-13" aria-hidden="true" tabindex="-1"></a>  partial_y_train,</span>
<span id="cb470-14"><a href="deep-learning-fundamentals.html#cb470-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">20</span>,</span>
<span id="cb470-15"><a href="deep-learning-fundamentals.html#cb470-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb470-16"><a href="deep-learning-fundamentals.html#cb470-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> <span class="fu">list</span>(x_val, y_val)</span>
<span id="cb470-17"><a href="deep-learning-fundamentals.html#cb470-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Let us now see the performance over the iterations (epochs)</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="deep-learning-fundamentals.html#cb471-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-333-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the in-sample performance keep increasing while the out-of-sample performance decays over time.</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="deep-learning-fundamentals.html#cb472-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Accuracy</span></span>
<span id="cb472-2"><a href="deep-learning-fundamentals.html#cb472-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(history<span class="sc">$</span>metrics<span class="sc">$</span>val_accuracy,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>),<span class="at">type =</span>  <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&quot;darkblue&quot;</span>)</span>
<span id="cb472-3"><a href="deep-learning-fundamentals.html#cb472-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(history<span class="sc">$</span>metrics<span class="sc">$</span>accuracy,<span class="at">type =</span> <span class="st">&quot;l&quot;</span>,<span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</span>
<span id="cb472-4"><a href="deep-learning-fundamentals.html#cb472-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">which.max</span>(history<span class="sc">$</span>metrics<span class="sc">$</span>val_accuracy),<span class="at">lty =</span> <span class="dv">2</span>,<span class="at">col =</span> <span class="st">&quot;purple&quot;</span>,<span class="at">lwd =</span> <span class="fl">0.7</span>)</span>
<span id="cb472-5"><a href="deep-learning-fundamentals.html#cb472-5" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb472-6"><a href="deep-learning-fundamentals.html#cb472-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb472-7"><a href="deep-learning-fundamentals.html#cb472-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Standard errors</span></span>
<span id="cb472-8"><a href="deep-learning-fundamentals.html#cb472-8" aria-hidden="true" tabindex="-1"></a>min.point <span class="ot">=</span> <span class="fu">max</span>(history<span class="sc">$</span>metrics<span class="sc">$</span>val_accuracy)</span>
<span id="cb472-9"><a href="deep-learning-fundamentals.html#cb472-9" aria-hidden="true" tabindex="-1"></a>sd.points <span class="ot">=</span> <span class="fu">sd</span>(history<span class="sc">$</span>metrics<span class="sc">$</span>val_accuracy)</span>
<span id="cb472-10"><a href="deep-learning-fundamentals.html#cb472-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>min.point <span class="sc">+</span> <span class="fl">0.2</span> <span class="sc">*</span> sd.points, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="st">&quot;dashed&quot;</span>) <span class="co">#0.2 is just a rule of thumb, could be anything</span></span>
<span id="cb472-11"><a href="deep-learning-fundamentals.html#cb472-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>min.point <span class="sc">-</span> <span class="fl">0.2</span> <span class="sc">*</span> sd.points, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb472-12"><a href="deep-learning-fundamentals.html#cb472-12" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="st">&quot;0.2-standard deviation lines&quot;</span>, <span class="at">lty=</span><span class="st">&quot;dashed&quot;</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">cex =</span> <span class="fl">0.6</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-334-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the maximum is at 10 epochs, while the book suggests selecting 9. Thus we go for 9. 0.2 standard errors</p>
</div>
<div id="training-model-with-optimal-epochs" class="section level4" number="5.3.2.5">
<h4><span class="header-section-number">5.3.2.5</span> Training model with optimal epochs</h4>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="deep-learning-fundamentals.html#cb473-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb473-2"><a href="deep-learning-fundamentals.html#cb473-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb473-3"><a href="deep-learning-fundamentals.html#cb473-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb473-4"><a href="deep-learning-fundamentals.html#cb473-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">10000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb473-5"><a href="deep-learning-fundamentals.html#cb473-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb473-6"><a href="deep-learning-fundamentals.html#cb473-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">46</span>, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb473-7"><a href="deep-learning-fundamentals.html#cb473-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb473-8"><a href="deep-learning-fundamentals.html#cb473-8" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb473-9"><a href="deep-learning-fundamentals.html#cb473-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb473-10"><a href="deep-learning-fundamentals.html#cb473-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb473-11"><a href="deep-learning-fundamentals.html#cb473-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb473-12"><a href="deep-learning-fundamentals.html#cb473-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb473-13"><a href="deep-learning-fundamentals.html#cb473-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb473-14"><a href="deep-learning-fundamentals.html#cb473-14" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb473-15"><a href="deep-learning-fundamentals.html#cb473-15" aria-hidden="true" tabindex="-1"></a>  partial_x_train,</span>
<span id="cb473-16"><a href="deep-learning-fundamentals.html#cb473-16" aria-hidden="true" tabindex="-1"></a>  partial_y_train,</span>
<span id="cb473-17"><a href="deep-learning-fundamentals.html#cb473-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> epochs,</span>
<span id="cb473-18"><a href="deep-learning-fundamentals.html#cb473-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb473-19"><a href="deep-learning-fundamentals.html#cb473-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_data =</span> <span class="fu">list</span>(x_val, y_val)</span>
<span id="cb473-20"><a href="deep-learning-fundamentals.html#cb473-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb473-21"><a href="deep-learning-fundamentals.html#cb473-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb473-22"><a href="deep-learning-fundamentals.html#cb473-22" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(x_test, one_hot_test_labels)</span></code></pre></div>
<p>Now we can print the results</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="deep-learning-fundamentals.html#cb474-1" aria-hidden="true" tabindex="-1"></a>results</span></code></pre></div>
<pre><code>##      loss  accuracy 
## 0.9756852 0.7894034</code></pre>
<p>We see in-sample that the model has an accuracy of 77.3%. This we would like to compare with predictions on new data.</p>
</div>
<div id="predictions-on-new-data" class="section level4" number="5.3.2.6">
<h4><span class="header-section-number">5.3.2.6</span> Predictions on new data</h4>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="deep-learning-fundamentals.html#cb476-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(x_test)</span></code></pre></div>
<p>Recall that we set the last layer to <code>activation = 'softmax'</code> which are the probabilities distributed, hence they should all sum to 1.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="deep-learning-fundamentals.html#cb477-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predictions)</span>
<span id="cb477-2"><a href="deep-learning-fundamentals.html#cb477-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(predictions[<span class="dv">1</span>,])</span></code></pre></div>
<pre><code>##                [,1]          [,2]          [,3]          [,4]         [,5]
## [1,] 0.000013018954 0.00003635166 0.00000459162 0.98119258881 0.0133803319
## [2,] 0.017108993605 0.15133123100 0.00384799275 0.00001253548 0.0033421055
## [3,] 0.003004374215 0.61370533705 0.00297529390 0.00402951660 0.0034054900
## [4,] 0.000070688649 0.00025290568 0.00016612237 0.03390027955 0.4744042754
## [5,] 0.000004567537 0.00267370627 0.00010470729 0.00056416728 0.0000742117
## [6,] 0.000010834472 0.00016048597 0.00006663371 0.99499785900 0.0006933536
##                [,6]          [,7]           [,8]         [,9]          [,10]
## [1,] 0.000002174748 0.00001351938 0.000016350714 0.0009854242 0.000000780304
## [2,] 0.000859998458 0.00119324971 0.000066683919 0.0000377787 0.007852645591
## [3,] 0.015549293719 0.00585722877 0.000355565804 0.0063044960 0.007743702736
## [4,] 0.000003331882 0.00038545718 0.000003834235 0.0001166838 0.000025974092
## [5,] 0.000573176309 0.00010614866 0.000002999967 0.0001086755 0.001020652824
## [6,] 0.000013807044 0.00000712571 0.000018944967 0.0003691049 0.000005835368
##              [,11]         [,12]         [,13]        [,14]          [,15]
## [1,] 0.00001627749 0.00144272321 0.00005226707 0.0001113787 0.000008367868
## [2,] 0.71173590422 0.00001761517 0.00229310174 0.0000823544 0.002526425058
## [3,] 0.01690869033 0.00456921617 0.00404314557 0.0144109605 0.000686369138
## [4,] 0.00021242636 0.00035038666 0.00007660327 0.0003188573 0.000009679169
## [5,] 0.00003916758 0.00037992132 0.00018144184 0.9705170989 0.000016562059
## [6,] 0.00009607588 0.00034397765 0.00007208740 0.0002452490 0.000017540240
##               [,16]        [,17]         [,18]         [,19]        [,20]
## [1,] 0.000005563867 0.0001168634 0.00006014727 0.00002126264 0.0001315648
## [2,] 0.006304908544 0.0011568516 0.00027936921 0.00094525702 0.0006402392
## [3,] 0.011111283675 0.0601866320 0.01218914427 0.00172124058 0.0034921216
## [4,] 0.000053068012 0.4723576307 0.00499939499 0.00031175319 0.0000356604
## [5,] 0.000016906184 0.0180149097 0.00067153946 0.00013839101 0.0007577202
## [6,] 0.000012497808 0.0002530222 0.00006632350 0.00006418266 0.0003244077
##              [,21]         [,22]         [,23]         [,24]          [,25]
## [1,] 0.00154624705 0.00001522659 0.00001452458 0.00015609854 0.000005694854
## [2,] 0.00991122238 0.00008329149 0.00046444789 0.00096701260 0.008080097847
## [3,] 0.03112313338 0.00282726460 0.00883400254 0.01436113752 0.010769764893
## [4,] 0.00395501684 0.00009052993 0.00004718580 0.00021245106 0.000098215154
## [5,] 0.00006127889 0.00138612452 0.00092710415 0.00012930980 0.000137188123
## [6,] 0.00106506119 0.00003067790 0.00002956227 0.00002641681 0.000021779171
##               [,26]          [,27]          [,28]          [,29]         [,30]
## [1,] 0.000007475229 0.000003610137 0.000025569634 0.000007711606 0.00001058070
## [2,] 0.005140288267 0.001466905000 0.000006258519 0.031494501978 0.00129509601
## [3,] 0.004628174007 0.006775671151 0.001770084840 0.013508649543 0.00498773530
## [4,] 0.003113465849 0.000021787917 0.000139587486 0.000027279219 0.00015037577
## [5,] 0.000072972369 0.000019424542 0.000089178124 0.000086252418 0.00007558661
## [6,] 0.000006921694 0.000008063514 0.000017873333 0.000015345366 0.00001107571
##              [,31]         [,32]          [,33]          [,34]          [,35]
## [1,] 0.00002891948 0.00006047298 0.000006979019 0.000003371635 0.000082295424
## [2,] 0.00018435616 0.00459399307 0.006161375903 0.000104838720 0.000249435805
## [3,] 0.00217565265 0.01508813165 0.022132785991 0.000507283199 0.010300055146
## [4,] 0.00022020194 0.00125578500 0.000036391946 0.000032738524 0.000006907357
## [5,] 0.00002557402 0.00046298656 0.000010422235 0.000009578448 0.000131571927
## [6,] 0.00001726519 0.00019951443 0.000002393808 0.000006617727 0.000122848942
##               [,36]         [,37]         [,38]          [,39]        [,40]
## [1,] 0.000003051214 0.00009624705 0.00002286687 0.000001139729 0.0002647820
## [2,] 0.000319024723 0.00030434411 0.00010024753 0.003061885247 0.0004960003
## [3,] 0.001531989663 0.01269211993 0.00062437792 0.003717431100 0.0084096389
## [4,] 0.000018493934 0.00017054942 0.00024764583 0.000009569342 0.0009295468
## [5,] 0.000023007917 0.00006461037 0.00002007023 0.000029851364 0.0000434920
## [6,] 0.000006693117 0.00034978276 0.00002527737 0.000003387122 0.0001212342
##               [,41]          [,42]          [,43]         [,44]          [,45]
## [1,] 0.000003940559 0.000004550842 0.000001754728 0.00001148353 0.000002792290
## [2,] 0.001860955032 0.011026582681 0.000076440148 0.00001685432 0.000039315157
## [3,] 0.002614533296 0.020181121305 0.001616727677 0.00104975479 0.000724369136
## [4,] 0.000092636496 0.000560513930 0.000005239623 0.00026017518 0.000016840333
## [5,] 0.000131443128 0.000004622811 0.000012548809 0.00004808808 0.000013714436
## [6,] 0.000004244224 0.000007864158 0.000026899817 0.00002179505 0.000008715462
##               [,46]
## [1,] 0.000001121492
## [2,] 0.000859980413
## [3,] 0.004799245857
## [4,] 0.000225814190
## [5,] 0.000017349159
## [6,] 0.000003341403
## [1] 1</code></pre>
<p>We see that each row is a sample and each coloumn is a unit in the last layer.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="deep-learning-fundamentals.html#cb479-1" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(predictions[<span class="dv">1</span>,],<span class="at">main =</span> <span class="st">&quot;Probabilities of class n&quot;</span>,<span class="at">xlab =</span> <span class="st">&quot;Probabilities&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-339"></span>
<img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-339-1.png" alt="Probabilities for classes for one observation" width="720" />
<p class="caption">
Figure 5.1: Probabilities for classes for one observation
</p>
</div>
<p>We see that the model is confident that the sample is either 4 or 5, where there is a greater probability for class 4.</p>
</div>
</div>
<div id="chapter-3---continous-prediction-a-regression-example---predicting-houseprices" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Chapter 3 - Continous prediction / a regression example - Predicting houseprices</h3>
<p>We are going to predict housing prices, hence we want to predict a continous variable and thus the last layer will end up with one unit.</p>
<div id="loading-the-data-1" class="section level4" number="5.3.3.1">
<h4><span class="header-section-number">5.3.3.1</span> Loading the data</h4>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="deep-learning-fundamentals.html#cb480-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb480-2"><a href="deep-learning-fundamentals.html#cb480-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb480-3"><a href="deep-learning-fundamentals.html#cb480-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">&lt;-</span> <span class="fu">dataset_boston_housing</span>()</span>
<span id="cb480-4"><a href="deep-learning-fundamentals.html#cb480-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(train_data, train_targets), <span class="fu">c</span>(test_data, test_targets)) <span class="sc">%&lt;-%</span> dataset</span></code></pre></div>
</div>
<div id="preparing-the-data-2" class="section level4" number="5.3.3.2">
<h4><span class="header-section-number">5.3.3.2</span> Preparing the data</h4>
<p>Notice that we scale the variables to make them comparable.</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="deep-learning-fundamentals.html#cb481-1" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_data, <span class="dv">2</span>, mean)</span>
<span id="cb481-2"><a href="deep-learning-fundamentals.html#cb481-2" aria-hidden="true" tabindex="-1"></a>std <span class="ot">&lt;-</span> <span class="fu">apply</span>(train_data, <span class="dv">2</span>, sd)</span>
<span id="cb481-3"><a href="deep-learning-fundamentals.html#cb481-3" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">scale</span>(train_data, <span class="at">center =</span> mean, <span class="at">scale =</span> std)</span>
<span id="cb481-4"><a href="deep-learning-fundamentals.html#cb481-4" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">scale</span>(test_data, <span class="at">center =</span> mean, <span class="at">scale =</span> std)</span></code></pre></div>
<p>We see that we did the following:</p>
<ol style="list-style-type: decimal">
<li>Subtract the mean, to demean the observations.</li>
<li>Divide by the standard deviation.</li>
</ol>
<p>It is a good idea (mostly a must) as it will make it easier for the network to learn. As if we did not scale them, then the model must first learn the spread in the each variable.</p>
</div>
<div id="building-the-model-2" class="section level4" number="5.3.3.3">
<h4><span class="header-section-number">5.3.3.3</span> Building the model</h4>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="deep-learning-fundamentals.html#cb482-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Because we will need to instantiate the same model multiple times,</span></span>
<span id="cb482-2"><a href="deep-learning-fundamentals.html#cb482-2" aria-hidden="true" tabindex="-1"></a><span class="co"># we use a function to construct it.</span></span>
<span id="cb482-3"><a href="deep-learning-fundamentals.html#cb482-3" aria-hidden="true" tabindex="-1"></a>build_model <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb482-4"><a href="deep-learning-fundamentals.html#cb482-4" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb482-5"><a href="deep-learning-fundamentals.html#cb482-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, </span>
<span id="cb482-6"><a href="deep-learning-fundamentals.html#cb482-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">input_shape =</span> <span class="fu">dim</span>(train_data)[[<span class="dv">2</span>]]) <span class="sc">%&gt;%</span> </span>
<span id="cb482-7"><a href="deep-learning-fundamentals.html#cb482-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb482-8"><a href="deep-learning-fundamentals.html#cb482-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>) </span>
<span id="cb482-9"><a href="deep-learning-fundamentals.html#cb482-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb482-10"><a href="deep-learning-fundamentals.html#cb482-10" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb482-11"><a href="deep-learning-fundamentals.html#cb482-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>, </span>
<span id="cb482-12"><a href="deep-learning-fundamentals.html#cb482-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;mse&quot;</span>, <span class="co">#MSE is good, if we want to punish the outliers.</span></span>
<span id="cb482-13"><a href="deep-learning-fundamentals.html#cb482-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;mae&quot;</span>) <span class="co">#This is nice, because absolute values will still be on the dollar scale</span></span>
<span id="cb482-14"><a href="deep-learning-fundamentals.html#cb482-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb482-15"><a href="deep-learning-fundamentals.html#cb482-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Note that we are not using any activation rule for the last layer, as we want it to be able to predict any value.</p>
</div>
<div id="validating-the-approach-using-k-fold-validation" class="section level4" number="5.3.3.4">
<h4><span class="header-section-number">5.3.3.4</span> Validating the approach using K-fold validation</h4>
<p>Notice that in this example we use 4 folds and then iterate through the folds.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="deep-learning-fundamentals.html#cb483-1" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb483-2"><a href="deep-learning-fundamentals.html#cb483-2" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(train_data))</span>
<span id="cb483-3"><a href="deep-learning-fundamentals.html#cb483-3" aria-hidden="true" tabindex="-1"></a>folds <span class="ot">&lt;-</span> <span class="fu">cut</span>(indices, <span class="at">breaks =</span> k, <span class="at">labels =</span> <span class="cn">FALSE</span>)</span>
<span id="cb483-4"><a href="deep-learning-fundamentals.html#cb483-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb483-5"><a href="deep-learning-fundamentals.html#cb483-5" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb483-6"><a href="deep-learning-fundamentals.html#cb483-6" aria-hidden="true" tabindex="-1"></a>all_scores <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb483-7"><a href="deep-learning-fundamentals.html#cb483-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb483-8"><a href="deep-learning-fundamentals.html#cb483-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;processing fold #&quot;</span>, i, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb483-9"><a href="deep-learning-fundamentals.html#cb483-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare the validation data: data from partition # k</span></span>
<span id="cb483-10"><a href="deep-learning-fundamentals.html#cb483-10" aria-hidden="true" tabindex="-1"></a>  val_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(folds <span class="sc">==</span> i, <span class="at">arr.ind =</span> <span class="cn">TRUE</span>) </span>
<span id="cb483-11"><a href="deep-learning-fundamentals.html#cb483-11" aria-hidden="true" tabindex="-1"></a>  val_data <span class="ot">&lt;-</span> train_data[val_indices,]</span>
<span id="cb483-12"><a href="deep-learning-fundamentals.html#cb483-12" aria-hidden="true" tabindex="-1"></a>  val_targets <span class="ot">&lt;-</span> train_targets[val_indices]</span>
<span id="cb483-13"><a href="deep-learning-fundamentals.html#cb483-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb483-14"><a href="deep-learning-fundamentals.html#cb483-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare the training data: data from all other partitions</span></span>
<span id="cb483-15"><a href="deep-learning-fundamentals.html#cb483-15" aria-hidden="true" tabindex="-1"></a>  partial_train_data <span class="ot">&lt;-</span> train_data[<span class="sc">-</span>val_indices,]</span>
<span id="cb483-16"><a href="deep-learning-fundamentals.html#cb483-16" aria-hidden="true" tabindex="-1"></a>  partial_train_targets <span class="ot">&lt;-</span> train_targets[<span class="sc">-</span>val_indices]</span>
<span id="cb483-17"><a href="deep-learning-fundamentals.html#cb483-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb483-18"><a href="deep-learning-fundamentals.html#cb483-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Build the Keras model (already compiled)</span></span>
<span id="cb483-19"><a href="deep-learning-fundamentals.html#cb483-19" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">build_model</span>()</span>
<span id="cb483-20"><a href="deep-learning-fundamentals.html#cb483-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb483-21"><a href="deep-learning-fundamentals.html#cb483-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Train the model (in silent mode, verbose=0)</span></span>
<span id="cb483-22"><a href="deep-learning-fundamentals.html#cb483-22" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(partial_train_data, partial_train_targets,</span>
<span id="cb483-23"><a href="deep-learning-fundamentals.html#cb483-23" aria-hidden="true" tabindex="-1"></a>                <span class="at">epochs =</span> num_epochs, <span class="at">batch_size =</span> <span class="dv">1</span>, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb483-24"><a href="deep-learning-fundamentals.html#cb483-24" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb483-25"><a href="deep-learning-fundamentals.html#cb483-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Evaluate the model on the validation data</span></span>
<span id="cb483-26"><a href="deep-learning-fundamentals.html#cb483-26" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(val_data, val_targets, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb483-27"><a href="deep-learning-fundamentals.html#cb483-27" aria-hidden="true" tabindex="-1"></a>  all_scores <span class="ot">&lt;-</span> <span class="fu">c</span>(all_scores,results[<span class="dv">2</span>]) <span class="co">#[2] for mean absolute error</span></span>
<span id="cb483-28"><a href="deep-learning-fundamentals.html#cb483-28" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="deep-learning-fundamentals.html#cb484-1" aria-hidden="true" tabindex="-1"></a>all_scores</span></code></pre></div>
<pre><code>##      mae      mae      mae      mae 
## 2.773674 2.073904 2.802147 2.657951</code></pre>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="deep-learning-fundamentals.html#cb486-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(all_scores)</span></code></pre></div>
<pre><code>## [1] 2.576919</code></pre>
<p>We see that on average we are off by 2,379 (notice that the variable is in 1000’s).</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="deep-learning-fundamentals.html#cb488-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Some memory clean-up</span></span>
<span id="cb488-2"><a href="deep-learning-fundamentals.html#cb488-2" aria-hidden="true" tabindex="-1"></a><span class="fu">k_clear_session</span>()</span></code></pre></div>
<div id="validation-with-more-iterations" class="section level5" number="5.3.3.4.1">
<h5><span class="header-section-number">5.3.3.4.1</span> Validation with more iterations</h5>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="deep-learning-fundamentals.html#cb489-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb489-2"><a href="deep-learning-fundamentals.html#cb489-2" aria-hidden="true" tabindex="-1"></a>all_mae_histories <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb489-3"><a href="deep-learning-fundamentals.html#cb489-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb489-4"><a href="deep-learning-fundamentals.html#cb489-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;processing fold #&quot;</span>, i, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb489-5"><a href="deep-learning-fundamentals.html#cb489-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb489-6"><a href="deep-learning-fundamentals.html#cb489-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare the validation data: data from partition # k</span></span>
<span id="cb489-7"><a href="deep-learning-fundamentals.html#cb489-7" aria-hidden="true" tabindex="-1"></a>  val_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(folds <span class="sc">==</span> i, <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span>
<span id="cb489-8"><a href="deep-learning-fundamentals.html#cb489-8" aria-hidden="true" tabindex="-1"></a>  val_data <span class="ot">&lt;-</span> train_data[val_indices,]</span>
<span id="cb489-9"><a href="deep-learning-fundamentals.html#cb489-9" aria-hidden="true" tabindex="-1"></a>  val_targets <span class="ot">&lt;-</span> train_targets[val_indices]</span>
<span id="cb489-10"><a href="deep-learning-fundamentals.html#cb489-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb489-11"><a href="deep-learning-fundamentals.html#cb489-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare the training data: data from all other partitions</span></span>
<span id="cb489-12"><a href="deep-learning-fundamentals.html#cb489-12" aria-hidden="true" tabindex="-1"></a>  partial_train_data <span class="ot">&lt;-</span> train_data[<span class="sc">-</span>val_indices,]</span>
<span id="cb489-13"><a href="deep-learning-fundamentals.html#cb489-13" aria-hidden="true" tabindex="-1"></a>  partial_train_targets <span class="ot">&lt;-</span> train_targets[<span class="sc">-</span>val_indices]</span>
<span id="cb489-14"><a href="deep-learning-fundamentals.html#cb489-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb489-15"><a href="deep-learning-fundamentals.html#cb489-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Build the Keras model (already compiled)</span></span>
<span id="cb489-16"><a href="deep-learning-fundamentals.html#cb489-16" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">build_model</span>()</span>
<span id="cb489-17"><a href="deep-learning-fundamentals.html#cb489-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb489-18"><a href="deep-learning-fundamentals.html#cb489-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Train the model (in silent mode, verbose=0)</span></span>
<span id="cb489-19"><a href="deep-learning-fundamentals.html#cb489-19" aria-hidden="true" tabindex="-1"></a>  history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb489-20"><a href="deep-learning-fundamentals.html#cb489-20" aria-hidden="true" tabindex="-1"></a>    partial_train_data, partial_train_targets,</span>
<span id="cb489-21"><a href="deep-learning-fundamentals.html#cb489-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(val_data, val_targets),</span>
<span id="cb489-22"><a href="deep-learning-fundamentals.html#cb489-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> num_epochs, <span class="at">batch_size =</span> <span class="dv">1</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb489-23"><a href="deep-learning-fundamentals.html#cb489-23" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb489-24"><a href="deep-learning-fundamentals.html#cb489-24" aria-hidden="true" tabindex="-1"></a>  mae_history <span class="ot">&lt;-</span> history<span class="sc">$</span>metrics<span class="sc">$</span>val_mean_absolute_error</span>
<span id="cb489-25"><a href="deep-learning-fundamentals.html#cb489-25" aria-hidden="true" tabindex="-1"></a>  all_mae_histories <span class="ot">&lt;-</span> <span class="fu">rbind</span>(all_mae_histories, mae_history)</span>
<span id="cb489-26"><a href="deep-learning-fundamentals.html#cb489-26" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We can then compute the average of the per-epoch MAE scores for all folds:</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="deep-learning-fundamentals.html#cb490-1" aria-hidden="true" tabindex="-1"></a><span class="co"># average_mae_history &lt;- data.frame(</span></span>
<span id="cb490-2"><a href="deep-learning-fundamentals.html#cb490-2" aria-hidden="true" tabindex="-1"></a><span class="co">#   epoch = seq(1:ncol(all_mae_histories)),</span></span>
<span id="cb490-3"><a href="deep-learning-fundamentals.html#cb490-3" aria-hidden="true" tabindex="-1"></a><span class="co">#   validation_mae = apply(all_mae_histories, 2, mean)</span></span>
<span id="cb490-4"><a href="deep-learning-fundamentals.html#cb490-4" aria-hidden="true" tabindex="-1"></a><span class="co"># )</span></span></code></pre></div>
<p>Let’s plot this:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="deep-learning-fundamentals.html#cb491-1" aria-hidden="true" tabindex="-1"></a><span class="co"># library(ggplot2)</span></span>
<span id="cb491-2"><a href="deep-learning-fundamentals.html#cb491-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + geom_line()</span></span></code></pre></div>
<p>We can use <code>geom_smooth</code> to see smooth it out a bit.</p>
<p>It may be a bit hard to see the plot due to scaling issues and relatively high variance. Let’s use <code>geom_smooth()</code> to try to get a clearer picture:</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="deep-learning-fundamentals.html#cb492-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + geom_smooth()</span></span></code></pre></div>
</div>
<div id="tuning-amount-fo-hidden-layers" class="section level5" number="5.3.3.4.2">
<h5><span class="header-section-number">5.3.3.4.2</span> Tuning amount fo hidden layers</h5>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="deep-learning-fundamentals.html#cb493-1" aria-hidden="true" tabindex="-1"></a><span class="co"># result</span></span></code></pre></div>
</div>
</div>
</div>
</div>
<div id="chapter-5---deep-learning-for-computer-vision" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Chapter 5 - Deep learning for computer vision</h2>
<p>This section deals with convoluted networks, its structure and some examples.</p>
<p>Computer vision = image recognition.</p>
<hr />
<p><strong>Summary</strong></p>
<p>We have seen the following so far:</p>
<ul>
<li>Densely connected network, that is where each neuron in each layer is tied to all neurons in adjacent layers.</li>
</ul>
<p>We are now going to look into another structure.</p>
<hr />
<div id="definition-of-convoluted-network" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Definition of convoluted network</h3>
<p>We also call the convoluted network <em>convnet</em>. As we have seen earlier the whole picture was used for analysis. That has it’s downsides that the model had to learn the patterns of the whole pictures. With convoluted networks, we specify a range (a patch) (equivilant to subset) of the pictures that are individually going to evaluated. This will not work with a densely connected network, hence one will connect neurons of inherited of different subsets of the pictures.</p>
<p>Note that each patch can be placed in any location as long as the borders are inside of the picture, but they will be overlapping, there is a solution to this in section <a href="deep-learning-fundamentals.html#padding">5.4.1.2.1</a>.</p>
<p><strong>Key take-aways</strong></p>
<ol style="list-style-type: decimal">
<li>convnets is the best tool for image classification</li>
<li>They work as a whole hierarchy of modular patterns</li>
<li>Each layer can be interpret and is not a black box as neural networks easily gets to be</li>
</ol>
<div id="tuning-parameters" class="section level4" number="5.4.1.1">
<h4><span class="header-section-number">5.4.1.1</span> Tuning parameters</h4>
<ol style="list-style-type: decimal">
<li>Size of the patches, typically 3 x 3 or 5 x 5.</li>
<li>Depth of the output feature map, see the picture below.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:convnet"></span>
<img src="Images/paste-334AE970.png" alt="How convnet works" width="386" />
<p class="caption">
Figure 5.2: How convnet works
</p>
</div>
</div>
<div id="data-modeling-techniques" class="section level4" number="5.4.1.2">
<h4><span class="header-section-number">5.4.1.2</span> Data modeling techniques</h4>
<p>The following presents different approaches options to the layers that will model with the data / affect the output.</p>
<div id="padding" class="section level5" number="5.4.1.2.1">
<h5><span class="header-section-number">5.4.1.2.1</span> Padding</h5>
<p>We see from the output feature map that we lost some of the data. This can be solved with padding, here we are adding some padding around the original feature map so we are able to have an equal amount of patches for each of the pixels in the input feature space.</p>
<p>To explain it more visually, we see that at the borders of the pictures we will not be able to center the patches at the borders.</p>
<p>To deal with this use <code>layer_conv_2d</code> where one can set <code>padding</code> to either <code>valid</code> or <code>same</code>, where same will get the output to be the same size as the input (hence adding padding) and valid not apply padding.</p>
</div>
<div id="strides" class="section level5" number="5.4.1.2.2">
<h5><span class="header-section-number">5.4.1.2.2</span> Strides</h5>
<p>This is referring to how much we move the patches around, the default is 1, but sometimes one will also use higher strides, for example 2, see the figure below. Where we see that we only are able to find 4 patches.</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="deep-learning-fundamentals.html#cb494-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="fu">rep</span>(<span class="st">&quot;Images/paste-72D86A41.png&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:Strides"></span>
<img src="Images/paste-72D86A41.png" alt="Example with strides, where stride = 2 x 2" width="468" />
<p class="caption">
Figure 5.3: Example with strides, where stride = 2 x 2
</p>
</div>
<p>This is in contrast to the normal scenario where we see that we are moving the patches around.</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="deep-learning-fundamentals.html#cb495-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="fu">rep</span>(<span class="st">&quot;Images/paste-DEF6C490.png&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:Strides2"></span>
<img src="Images/paste-DEF6C490.png" alt="Example with strides, where stride = 1 x 1" width="488" />
<p class="caption">
Figure 5.4: Example with strides, where stride = 1 x 1
</p>
</div>
<p><img src="Images/paste-DEF6C490.png" /></p>
<p><em>We say that this has no strides, but in fact the stride = 1 x 1.</em></p>
<p><strong>Why use the max-pooling operations?</strong></p>
<ul>
<li><p>It is a good approach to summarize the output layers and extract actual information.</p></li>
<li><p>Remember that the last layer will be a densely connected network</p></li>
</ul>
</div>
</div>
</div>
<div id="the-max-pooling-operation" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> The max-pooling operation</h3>
<p>It is similar to the strides in the way that you select a given patch of the output network, (see Section <a href="#convnet"><strong>??</strong></a>), often a size of 2 x 2 is chosen.</p>
<p>These selected areas are also called windows.</p>
<p><strong>Process:</strong></p>
<ol style="list-style-type: decimal">
<li>We are then moving the window around without overlapping with other patches.</li>
<li>We take the max value of each cell that are within the selected values</li>
</ol>
<p><strong><em>Example</em></strong></p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="deep-learning-fundamentals.html#cb496-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="fu">rep</span>(<span class="st">&quot;Images/paste-429DD86E.png&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:MaxPooling"></span>
<img src="Images/paste-429DD86E.png" alt="Max Polling example, source [@deeplizar2018]" width="1109" />
<p class="caption">
Figure 5.5: Max Polling example, source <span class="citation">(<a href="references.html#ref-deeplizar2018" role="doc-biblioref"><span>“Max Pooling in Convolutional Neural Networks Explained”</span> 2018</a>)</span>
</p>
</div>
<p>In the example above we see that we are pooling the max values and kinda summarizing in a smaller out, hence when we apply 2 x 2 pools, and extracting the max, then we will half the size of the output shape, e.g., in the Figure <a href="deep-learning-fundamentals.html#fig:MaxPooling">5.5</a> we see that output feature map of 26 x 26 is reduced to 13 x 13.</p>
<p><em>Notice, that the stride is 2</em>, <em>that is how much we are shifting the area for evaluation. Notice that the strides in a regular patching scenario, then we say that we don’t have a stride, although we do in fact have a stride = 1.</em></p>
</div>
</div>
<div id="lecture-notes-1" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Lecture notes</h2>
<div id="lecture-1" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Lecture 1</h3>
<hr />
<p>This notebook contains the code samples found in Chapter 2, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-r">Deep Learning with R</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p>
<hr />
<p>Let’s look at a concrete example of a neural network that uses the Keras R package to learn to classify hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this first example right away. You probably haven’t even installed Keras yet. Don’t worry, that is perfectly fine. In the next chapter, we will review each element in our example and explain them in detail. So don’t worry if some steps seem arbitrary or look like magic to you! We’ve got to start somewhere.</p>
<p>The problem we’re trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels) into their 10 categories (0 to 9). We’ll use the MNIST dataset, a classic dataset in the machine-learning community, which has been around almost as long as the field itself and has been intensively studied. It’s a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the “Hello World” of deep learning—it’s what you do to verify that your algorithms are working as expected. As you become a machine-learning practitioner, you’ll see MNIST come up over and over again, in scientific papers, blog posts, and so on.</p>
<p>The MNIST dataset comes preloaded in Keras, in the form of <code>train</code> and <code>test</code> lists, each of which includes a set of images (<code>x</code>) and associated labels (<code>y</code>):</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="deep-learning-fundamentals.html#cb497-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb497-2"><a href="deep-learning-fundamentals.html#cb497-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb497-3"><a href="deep-learning-fundamentals.html#cb497-3" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb497-4"><a href="deep-learning-fundamentals.html#cb497-4" aria-hidden="true" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb497-5"><a href="deep-learning-fundamentals.html#cb497-5" aria-hidden="true" tabindex="-1"></a>train_labels <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb497-6"><a href="deep-learning-fundamentals.html#cb497-6" aria-hidden="true" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb497-7"><a href="deep-learning-fundamentals.html#cb497-7" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span></code></pre></div>
<p><code>train_images</code> and <code>train_labels</code> form the <em>training set</em>, the data that the model will learn from. The model will then be tested on the <em>test set</em>, <code>test_images</code> and <code>test_labels</code>. The images are encoded as as 3D arrays, and the labels are a 1D array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.</p>
<p>The R <code>str()</code> function is a convenient way to get a quick glimpse at the structure of an array. Let’s use it to have a look at the training data:</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="deep-learning-fundamentals.html#cb498-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(train_images)</span></code></pre></div>
<pre><code>##  int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="deep-learning-fundamentals.html#cb500-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(train_labels)</span></code></pre></div>
<pre><code>##  int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...</code></pre>
<p>Let’s have a look at the test data:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="deep-learning-fundamentals.html#cb502-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(test_images)</span></code></pre></div>
<pre><code>##  int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="deep-learning-fundamentals.html#cb504-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(test_labels)</span></code></pre></div>
<pre><code>##  int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...</code></pre>
<p>The workflow will be as follows: first we’ll feed the neural network the training data, <code>train_images</code> and <code>train_labels</code>. The network will then learn to associate images and labels. Finally, we’ll ask the network to produce predictions for <code>test_images</code>, and we’ll verify whether these predictions match the labels from <code>test_labels</code>.</p>
<p>Let’s build the network – again, remember that you aren’t supposed to understand everything about this example yet.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="deep-learning-fundamentals.html#cb506-1" aria-hidden="true" tabindex="-1"></a>network <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb506-2"><a href="deep-learning-fundamentals.html#cb506-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">512</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">28</span> <span class="sc">*</span> <span class="dv">28</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb506-3"><a href="deep-learning-fundamentals.html#cb506-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">10</span>, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span></code></pre></div>
<p>The core building block of neural networks is the <em>layer</em>, a data-processing module that you can think of as a filter for data. Some data comes in, and it comes out in a more useful form. Specifically, layers extract <em>representations</em> out of the data fed into them—hopefully representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of progressive <em>data distillation</em>. A deep-learning model is like a sieve for data processing, made of a succession of increasingly refined data filters—the layers.</p>
<p>Here our network consists of a sequence of two layers, which are densely connected (also called <em>fully connected</em>) neural layers. The second (and last) layer is a 10-way <em>softmax</em> layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.</p>
<p>To make the network ready for training, we need to pick three more things, as part of the <em>compilation</em> step:</p>
<ul>
<li><em>A loss function</em>—How the network will be able to measure how good a job it’s doing on its training data, and thus how it will be able to steer itself in the right direction.</li>
<li><em>An optimizer</em>—The mechanism through which the network will update itself based on the data it sees and its loss function.</li>
<li><em>Metrics to monitor during training and testing</em>—Here we’ll only care about accuracy (the fraction of the images that were correctly classified).</li>
</ul>
<p>The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters.</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="deep-learning-fundamentals.html#cb507-1" aria-hidden="true" tabindex="-1"></a>network <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb507-2"><a href="deep-learning-fundamentals.html#cb507-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb507-3"><a href="deep-learning-fundamentals.html#cb507-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb507-4"><a href="deep-learning-fundamentals.html#cb507-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb507-5"><a href="deep-learning-fundamentals.html#cb507-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Before training, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the <code>[0, 1]</code> interval. Previously, our training images, for instance, were stored in an array of shape <code>(60000, 28, 28)</code> of type integer with values in the <code>[0, 255]</code> interval. We transform it into a double array of shape <code>(60000, 28 * 28)</code> with values between 0 and 1.</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="deep-learning-fundamentals.html#cb508-1" aria-hidden="true" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(train_images, <span class="fu">c</span>(<span class="dv">60000</span>, <span class="dv">28</span> <span class="sc">*</span> <span class="dv">28</span>))</span>
<span id="cb508-2"><a href="deep-learning-fundamentals.html#cb508-2" aria-hidden="true" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> train_images <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb508-3"><a href="deep-learning-fundamentals.html#cb508-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb508-4"><a href="deep-learning-fundamentals.html#cb508-4" aria-hidden="true" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(test_images, <span class="fu">c</span>(<span class="dv">10000</span>, <span class="dv">28</span> <span class="sc">*</span> <span class="dv">28</span>))</span>
<span id="cb508-5"><a href="deep-learning-fundamentals.html#cb508-5" aria-hidden="true" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> test_images <span class="sc">/</span> <span class="dv">255</span></span></code></pre></div>
<p>We also need to categorically encode the labels, a step which we explain in chapter 3:</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="deep-learning-fundamentals.html#cb509-1" aria-hidden="true" tabindex="-1"></a>train_labels <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(train_labels)</span>
<span id="cb509-2"><a href="deep-learning-fundamentals.html#cb509-2" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(test_labels)</span></code></pre></div>
<p>We are now ready to train our network, which in Keras is done via a call to the <code>fit</code> method of the network: we “fit” the model to its training data.</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="deep-learning-fundamentals.html#cb510-1" aria-hidden="true" tabindex="-1"></a>network <span class="sc">%&gt;%</span> <span class="fu">fit</span>(train_images, train_labels, <span class="at">epochs =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">128</span>)</span></code></pre></div>
<p>Two quantities are being displayed during training: the “loss” of the network over the training data, and the accuracy of the network over the training data.</p>
<p>We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let’s check that our model performs well on the test set too:</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="deep-learning-fundamentals.html#cb511-1" aria-hidden="true" tabindex="-1"></a>metrics <span class="ot">&lt;-</span> network <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(test_images, test_labels, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb511-2"><a href="deep-learning-fundamentals.html#cb511-2" aria-hidden="true" tabindex="-1"></a>metrics</span></code></pre></div>
<pre><code>##       loss   accuracy 
## 0.07090375 0.97860003</code></pre>
<p>Our test set accuracy turns out to be 98.1% – that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of “overfitting,” the fact that machine learning models tend to perform worse on new data than on their training data. Overfitting will be a central topic in chapter 3.</p>
<p>This concludes our first example – you just saw how you can build and a train a neural network to classify handwritten digits in less than 20 lines of R code. In the next chapter, we’ll go into detail about every moving piece we just previewed and clarify what’s going on behind the scenes. You’ll learn about tensors, the data-storing objects going into the network; about tensor operations, which layers are made of; and about gradient descent, which allows your network to learn from its training examples.</p>
</div>
<div id="lecture-2" class="section level3" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Lecture 2</h3>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Rules of thumb for how many dimensions to get, see <span class="citation">(<a href="references.html#ref-chollet2018" role="doc-biblioref">Chollet and Allaire 2018</a>, p 32)</span><a href="deep-learning-fundamentals.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="support-vector-machines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="structuring-data-transformation-and-model-assessments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
