<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Moving Beyond Linearity | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="2 Moving Beyond Linearity | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Moving Beyond Linearity | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Moving Beyond Linearity | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="tree-based-methods.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#models-beyond-linearity"><i class="fa fa-check"></i><b>2.1</b> Models Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#degrees-of-freedom"><i class="fa fa-check"></i><b>2.1.3.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="2.1.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#basis-splines-vs.-natural-splines"><i class="fa fa-check"></i><b>2.1.3.5</b> Basis splines vs.Â natural splines</a></li>
<li class="chapter" data-level="2.1.3.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.6</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#feature-selection"><i class="fa fa-check"></i><b>2.1.6.1</b> Feature Selection</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for regression problems</a>
<ul>
<li class="chapter" data-level="2.1.6.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#interpretation-of-output"><i class="fa fa-check"></i><b>2.1.6.2.1</b> Interpretation of output</a></li>
</ul></li>
<li class="chapter" data-level="2.1.6.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.3</b> GAM for classification problems</a></li>
</ul></li>
<li class="chapter" data-level="2.1.7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#model-assessment"><i class="fa fa-check"></i><b>2.1.7</b> Model assessment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lecture-notes"><i class="fa fa-check"></i><b>2.2</b> Lecture notes</a></li>
<li class="chapter" data-level="2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#lab-section"><i class="fa fa-check"></i><b>2.3</b> Lab section</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#polynomial-regression-and-step-functions"><i class="fa fa-check"></i><b>2.3.1</b> Polynomial Regression and Step Functions</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#continous-model"><i class="fa fa-check"></i><b>2.3.1.1</b> Continous model</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logarithmic-model"><i class="fa fa-check"></i><b>2.3.1.2</b> Logarithmic model</a></li>
<li class="chapter" data-level="2.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#step-function"><i class="fa fa-check"></i><b>2.3.1.3</b> Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#splines"><i class="fa fa-check"></i><b>2.3.2</b> Splines</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#basis-function-splines"><i class="fa fa-check"></i><b>2.3.2.1</b> Basis Function Splines</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#natural-splines"><i class="fa fa-check"></i><b>2.3.2.2</b> Natural Splines</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#smooth-splines"><i class="fa fa-check"></i><b>2.3.2.3</b> Smooth Splines</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#local-regression-1"><i class="fa fa-check"></i><b>2.3.2.4</b> Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gams"><i class="fa fa-check"></i><b>2.3.3</b> GAMs</a>
<ul>
<li class="chapter" data-level="2.3.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-only-natural-splines"><i class="fa fa-check"></i><b>2.3.3.1</b> With only natural splines</a></li>
<li class="chapter" data-level="2.3.3.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#with-different-splines"><i class="fa fa-check"></i><b>2.3.3.2</b> With different splines</a></li>
<li class="chapter" data-level="2.3.3.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#but-what-variables-to-include"><i class="fa fa-check"></i><b>2.3.3.3</b> But what variables to include?</a></li>
<li class="chapter" data-level="2.3.3.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#gam-with-local-regression"><i class="fa fa-check"></i><b>2.3.3.4</b> GAM with local regression</a></li>
<li class="chapter" data-level="2.3.3.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#logistic-regression"><i class="fa fa-check"></i><b>2.3.3.5</b> Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a>
<ul>
<li class="chapter" data-level="2.4.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-polynomial-regression"><i class="fa fa-check"></i><b>2.4.1.1</b> 6.a Polynomial Regression</a></li>
<li class="chapter" data-level="2.4.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-step-function"><i class="fa fa-check"></i><b>2.4.1.2</b> 6.b Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a>
<ul>
<li class="chapter" data-level="2.4.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-using-poly-function-to-fit-cubic-polynomial-regression"><i class="fa fa-check"></i><b>2.4.4.1</b> (a) using poly function to fit cubic polynomial regression</a></li>
<li class="chapter" data-level="2.4.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-plotting-polynomial-fits-for-a-range-of-polynomials"><i class="fa fa-check"></i><b>2.4.4.2</b> (b) Plotting polynomial fits for a range of polynomials</a></li>
<li class="chapter" data-level="2.4.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-using-cv-to-select-best-degree-of-d"><i class="fa fa-check"></i><b>2.4.4.3</b> (c) Using CV to select best degree of d</a></li>
<li class="chapter" data-level="2.4.4.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-use-bs-to-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.4</b> (d) Use <code>bs()</code> to fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#e-now-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.5</b> (e) Now fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#f-perform-cross-validation-to-select-degrees"><i class="fa fa-check"></i><b>2.4.4.6</b> (f) Perform cross-validation, to select degrees</a></li>
</ul></li>
<li class="chapter" data-level="2.4.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a>
<ul>
<li class="chapter" data-level="2.4.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-partitioning-the-data"><i class="fa fa-check"></i><b>2.4.5.1</b> (a) Partitioning the data</a></li>
<li class="chapter" data-level="2.4.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#b-fitting-a-gam"><i class="fa fa-check"></i><b>2.4.5.2</b> (b) Fitting a GAM</a></li>
<li class="chapter" data-level="2.4.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c-evaluating-on-the-test-set"><i class="fa fa-check"></i><b>2.4.5.3</b> (c) Evaluating on the test set</a></li>
<li class="chapter" data-level="2.4.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#d-which-variables-appear-to-have-a-non-linear-relationship"><i class="fa fa-check"></i><b>2.4.5.4</b> (d) Which variables appear to have a non linear relationship?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#FacebookCasestudy"><i class="fa fa-check"></i><b>2.5</b> Casestudy - Predicting the Return on Advertising Spent</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#background"><i class="fa fa-check"></i><b>2.5.1</b> 1. Background</a></li>
<li class="chapter" data-level="2.5.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#case-study-business-understanding-phase"><i class="fa fa-check"></i><b>2.5.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#the-data-data-understanding-phase"><i class="fa fa-check"></i><b>2.5.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#specific-requirements"><i class="fa fa-check"></i><b>2.5.4</b> 4. Specific requirements:</a>
<ul>
<li class="chapter" data-level="2.5.4.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-1---import-and-overview"><i class="fa fa-check"></i><b>2.5.4.1</b> 4.1 Task 1 - Import and overview</a></li>
<li class="chapter" data-level="2.5.4.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-2---data-inspection"><i class="fa fa-check"></i><b>2.5.4.2</b> 4.2 Task 2 - Data inspection</a></li>
<li class="chapter" data-level="2.5.4.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#task-3---building-different-models"><i class="fa fa-check"></i><b>2.5.4.3</b> 4.3 Task 3 - Building different models</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#a-generalized-additive-model-gam-to-predict-roas"><i class="fa fa-check"></i><b>2.5.4.3.1</b> A Generalized Additive Model (GAM) to predict ROAS</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c1-feature-selection-using-regsubsets"><i class="fa fa-check"></i><b>2.5.4.3.1.1</b> c1) Feature selection using regsubsets()</a></li>
<li class="chapter" data-level="2.5.4.3.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c2-feature-selection-using-step.gam"><i class="fa fa-check"></i><b>2.5.4.3.1.2</b> c2) Feature selection using step.GAM</a></li>
<li class="chapter" data-level="2.5.4.3.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#c3-fetaure-selection-using-random-forest"><i class="fa fa-check"></i><b>2.5.4.3.1.3</b> c3) Fetaure selection using random forest</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>3.1</b> The Basics of Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-trees"><i class="fa fa-check"></i><b>3.1.1</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#how-to-make-the-decision-trees"><i class="fa fa-check"></i><b>3.1.1.1</b> How to make the decision trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-goal-of-regression"><i class="fa fa-check"></i><b>3.1.1.1.1</b> The goal of regression</a></li>
<li class="chapter" data-level="3.1.1.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning-algoritm"><i class="fa fa-check"></i><b>3.1.1.1.2</b> Tree Pruning &amp; Algoritm</a></li>
<li class="chapter" data-level="3.1.1.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#setting-contrains-of-the-tree-sise"><i class="fa fa-check"></i><b>3.1.1.1.3</b> Setting contrains of the tree sise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#classification-trees"><i class="fa fa-check"></i><b>3.1.2</b> Classification Trees</a></li>
<li class="chapter" data-level="3.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-vs.-lienar-models"><i class="fa fa-check"></i><b>3.1.3</b> Tree vs.Â Lienar Models</a></li>
<li class="chapter" data-level="3.1.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#advantages-and-disadvantages-of-trees"><i class="fa fa-check"></i><b>3.1.4</b> Advantages and Disadvantages of Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-random-forests-boosting"><i class="fa fa-check"></i><b>3.2</b> Bagging, Random Forests, Boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-bootstrap-aggregation"><i class="fa fa-check"></i><b>3.2.1</b> Bagging (Bootstrap Aggregation)</a></li>
<li class="chapter" data-level="3.2.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>3.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="3.2.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-i.e.-gradient-boosting"><i class="fa fa-check"></i><b>3.2.3</b> Boosting (i.e.Â Gradient Boosting)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost"><i class="fa fa-check"></i><b>3.3</b> XGBoost</a></li>
<li class="chapter" data-level="3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#application-in-r"><i class="fa fa-check"></i><b>3.4</b> Application in R</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#decision-trees"><i class="fa fa-check"></i><b>3.4.1</b> Decision trees</a></li>
<li class="chapter" data-level="3.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>3.4.2</b> Bagging</a></li>
<li class="chapter" data-level="3.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-1"><i class="fa fa-check"></i><b>3.4.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.4.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting"><i class="fa fa-check"></i><b>3.4.4</b> Boosting</a></li>
<li class="chapter" data-level="3.4.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost-1"><i class="fa fa-check"></i><b>3.4.5</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#lab-section-1"><i class="fa fa-check"></i><b>3.5</b> Lab section</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#LabClassification"><i class="fa fa-check"></i><b>3.5.1</b> Fitting Classification Trees</a></li>
<li class="chapter" data-level="3.5.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#fitting-regression-trees"><i class="fa fa-check"></i><b>3.5.2</b> Fitting Regression Trees</a></li>
<li class="chapter" data-level="3.5.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-and-random-forests"><i class="fa fa-check"></i><b>3.5.3</b> Bagging and Random Forests</a>
<ul>
<li class="chapter" data-level="3.5.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-1"><i class="fa fa-check"></i><b>3.5.3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.5.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forest"><i class="fa fa-check"></i><b>3.5.3.2</b> Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="3.5.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#boosting-1"><i class="fa fa-check"></i><b>3.5.4</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercies"><i class="fa fa-check"></i><b>3.6</b> Exercies</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-7---decision-tree-assessment"><i class="fa fa-check"></i><b>3.6.1</b> Exercise 7 - Decision Tree Assessment</a></li>
<li class="chapter" data-level="3.6.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#but-what-does-it-really-show-ntrees-are-fixed-at-500"><i class="fa fa-check"></i><b>3.6.2</b> But what does it really show?, ntrees are fixed at 500</a></li>
<li class="chapter" data-level="3.6.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-8---dtrfboosting"><i class="fa fa-check"></i><b>3.6.3</b> Exercise 8 - DT/RF/Boosting</a></li>
<li class="chapter" data-level="3.6.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-9---decision-tree-pruning"><i class="fa fa-check"></i><b>3.6.4</b> Exercise 9 - Decision Tree / Pruning</a></li>
<li class="chapter" data-level="3.6.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-10---boostinggamlinearregbagging---comparison"><i class="fa fa-check"></i><b>3.6.5</b> Exercise 10 - Boosting/GAM/LinearReg/Bagging - Comparison</a></li>
<li class="chapter" data-level="3.6.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-11---boosting"><i class="fa fa-check"></i><b>3.6.6</b> Exercise 11 - Boosting</a></li>
<li class="chapter" data-level="3.6.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#exercise-12"><i class="fa fa-check"></i><b>3.6.7</b> Exercise 12</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#casestudy---predicting-algae-blooms"><i class="fa fa-check"></i><b>3.7</b> Casestudy - Predicting Algae Blooms</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#background-1"><i class="fa fa-check"></i><b>3.7.1</b> 1. Background</a></li>
<li class="chapter" data-level="3.7.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#case-study-business-understanding-phase-1"><i class="fa fa-check"></i><b>3.7.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="3.7.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#the-data-data-understanding-phase-1"><i class="fa fa-check"></i><b>3.7.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="3.7.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#requirements"><i class="fa fa-check"></i><b>3.7.4</b> 4. Requirements:</a>
<ul>
<li class="chapter" data-level="3.7.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-1-understand-and-import-data-properly"><i class="fa fa-check"></i><b>3.7.4.1</b> 4.1 Task 1: Understand and import data properly</a></li>
<li class="chapter" data-level="3.7.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-2-inspect-your-data-and-do-the-required-variable-adaptations-and-transformations"><i class="fa fa-check"></i><b>3.7.4.2</b> 4.2 Task 2: Inspect your data and do the required variable adaptations and transformations</a></li>
<li class="chapter" data-level="3.7.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#task-3-build-one-or-several-predictive-models-and-evaluate-their-performance."><i class="fa fa-check"></i><b>3.7.4.3</b> 4.3 Task 3: Build one or several predictive models and evaluate their performance.</a>
<ul>
<li class="chapter" data-level="3.7.4.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#growing-a-regression-tree"><i class="fa fa-check"></i><b>3.7.4.3.1</b> 4.3.1. Growing a regression tree</a></li>
<li class="chapter" data-level="3.7.4.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#pruning-the-tree"><i class="fa fa-check"></i><b>3.7.4.3.2</b> 4.3.2 Pruning the tree</a></li>
<li class="chapter" data-level="3.7.4.3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#model-evaluation-and-selection"><i class="fa fa-check"></i><b>3.7.4.3.3</b> 4.3.3.Model evaluation and selection</a></li>
<li class="chapter" data-level="3.7.4.3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#ensemble-methods-random-forests"><i class="fa fa-check"></i><b>3.7.4.3.4</b> 4.3.4. Ensemble methods: Random Forests</a></li>
<li class="chapter" data-level="3.7.4.3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#predicting-in-the-test-sample"><i class="fa fa-check"></i><b>3.7.4.3.5</b> 4.3.5. Predicting in the test sample</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="4.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-vector-machines"><i class="fa fa-check"></i><b>4.1</b> Maximal Vector Machines</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#what-is-a-hyperplane"><i class="fa fa-check"></i><b>4.1.1</b> What is a hyperplane?</a></li>
<li class="chapter" data-level="4.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#classification-using-a-separating-hyperplane"><i class="fa fa-check"></i><b>4.1.2</b> Classification Using a Separating Hyperplane</a></li>
<li class="chapter" data-level="4.1.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-maximal-margin-classifier"><i class="fa fa-check"></i><b>4.1.3</b> The Maximal Margin Classifier</a></li>
<li class="chapter" data-level="4.1.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#ConstructMMC"><i class="fa fa-check"></i><b>4.1.4</b> Construction of the Maximal Margin Classifer</a></li>
<li class="chapter" data-level="4.1.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>4.1.5</b> The Non-separable Case</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifiers"><i class="fa fa-check"></i><b>4.2</b> Support Vector Classifiers</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#overview-of-the-support-vector-classifier"><i class="fa fa-check"></i><b>4.2.1</b> Overview of the Support Vector Classifier</a></li>
<li class="chapter" data-level="4.2.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#details-of-the-support-vector-classifer"><i class="fa fa-check"></i><b>4.2.2</b> Details of the Support Vector Classifer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>4.3</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#classification-with-non-linear-decision-boundary"><i class="fa fa-check"></i><b>4.3.1</b> Classification with non linear decision boundary</a></li>
<li class="chapter" data-level="4.3.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-support-vector-machine"><i class="fa fa-check"></i><b>4.3.2</b> The Support Vector Machine</a></li>
<li class="chapter" data-level="4.3.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#MoreThanTwoClasses"><i class="fa fa-check"></i><b>4.3.3</b> SVMs with More than Two Classes</a></li>
<li class="chapter" data-level="4.3.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#relationship-to-logistic-regression"><i class="fa fa-check"></i><b>4.3.4</b> Relationship to logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#process-of-kernels-methods"><i class="fa fa-check"></i><b>4.4</b> Process of kernels methods</a></li>
<li class="chapter" data-level="4.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#lab"><i class="fa fa-check"></i><b>4.5</b> Lab</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>4.5.1</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="4.5.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machine"><i class="fa fa-check"></i><b>4.5.2</b> Support Vector Machine</a></li>
<li class="chapter" data-level="4.5.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#roc-curves"><i class="fa fa-check"></i><b>4.5.3</b> ROC Curves</a></li>
<li class="chapter" data-level="4.5.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-with-multiple-classes"><i class="fa fa-check"></i><b>4.5.4</b> SVM with Multiple Classes</a></li>
<li class="chapter" data-level="4.5.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-to-gene-expression-data"><i class="fa fa-check"></i><b>4.5.5</b> Application to Gene Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#exercises-1"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Deep Learning Fundamentals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#basic-deep-learning"><i class="fa fa-check"></i><b>5.1</b> Basic Deep Learning</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#terms"><i class="fa fa-check"></i><b>5.1.1</b> Terms</a></li>
<li class="chapter" data-level="5.1.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#optimizers-loss-metrics-and-activation-rules"><i class="fa fa-check"></i><b>5.1.2</b> Optimizers, Loss, Metrics and Activation rules</a>
<ul>
<li class="chapter" data-level="5.1.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#gradient-descents"><i class="fa fa-check"></i><b>5.1.2.1</b> Gradient Descents</a></li>
</ul></li>
<li class="chapter" data-level="5.1.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#example-with-image-recognizion"><i class="fa fa-check"></i><b>5.1.3</b> Example with image recognizion</a></li>
<li class="chapter" data-level="5.1.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#model-building"><i class="fa fa-check"></i><b>5.1.4</b> Model building</a></li>
<li class="chapter" data-level="5.1.5" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#validating-the-model"><i class="fa fa-check"></i><b>5.1.5</b> Validating the model</a></li>
<li class="chapter" data-level="5.1.6" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#overfitting-underfitting"><i class="fa fa-check"></i><b>5.1.6</b> Overfitting / Underfitting</a>
<ul>
<li class="chapter" data-level="5.1.6.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#hyperparameters"><i class="fa fa-check"></i><b>5.1.6.1</b> Hyperparameters:</a></li>
<li class="chapter" data-level="5.1.6.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#regularization"><i class="fa fa-check"></i><b>5.1.6.2</b> Regularization:</a></li>
<li class="chapter" data-level="5.1.6.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#dropout"><i class="fa fa-check"></i><b>5.1.6.3</b> Dropout</a></li>
<li class="chapter" data-level="5.1.6.4" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#then-how-do-we-control-for-a-good-fit"><i class="fa fa-check"></i><b>5.1.6.4</b> Then how do we control for a good fit?</a></li>
</ul></li>
<li class="chapter" data-level="5.1.7" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#dealing-with-missing-data"><i class="fa fa-check"></i><b>5.1.7</b> Dealing with missing data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#the-workflow-for-building-the-neural-network"><i class="fa fa-check"></i><b>5.2</b> The workflow for building the neural network</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#being-aware-of-the-process"><i class="fa fa-check"></i><b>5.2.1</b> Being aware of the process</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#when-neural-networks-will-fail"><i class="fa fa-check"></i><b>5.3</b> When Neural Networks will fail</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="deep-learning-fundamentals.html"><a href="deep-learning-fundamentals.html#hyper-parameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyper parameter tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Chapter 3 - Getting Started With Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#positive-negative-imdb-reviews"><i class="fa fa-check"></i><b>6.1</b> Positive / Negative IMDB reviews</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#extracting-the-data"><i class="fa fa-check"></i><b>6.1.1</b> Extracting the data</a></li>
<li class="chapter" data-level="6.1.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#preparing-the-data"><i class="fa fa-check"></i><b>6.1.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.1.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#building-the-model"><i class="fa fa-check"></i><b>6.1.3</b> Building the model</a></li>
<li class="chapter" data-level="6.1.4" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#assessing-model-performance"><i class="fa fa-check"></i><b>6.1.4</b> Assessing model performance</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#multiclass-classification---classifying-newswires"><i class="fa fa-check"></i><b>6.2</b> Multiclass classification - Classifying newswires</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#loading-the-data"><i class="fa fa-check"></i><b>6.2.1</b> Loading the data</a></li>
<li class="chapter" data-level="6.2.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#preparing-the-data-1"><i class="fa fa-check"></i><b>6.2.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.2.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#building-the-model-1"><i class="fa fa-check"></i><b>6.2.3</b> Building the model</a></li>
<li class="chapter" data-level="6.2.4" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#validating-the-model-model-assessment"><i class="fa fa-check"></i><b>6.2.4</b> Validating the model + model assessment</a></li>
<li class="chapter" data-level="6.2.5" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#training-model-with-optimal-epochs"><i class="fa fa-check"></i><b>6.2.5</b> Training model with optimal epochs</a></li>
<li class="chapter" data-level="6.2.6" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#predictions-on-new-data"><i class="fa fa-check"></i><b>6.2.6</b> Predictions on new data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#continous-prediction-a-regression-example---predicting-houseprices"><i class="fa fa-check"></i><b>6.3</b> Continous prediction / a regression example - Predicting houseprices</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#loading-the-data-1"><i class="fa fa-check"></i><b>6.3.1</b> Loading the data</a></li>
<li class="chapter" data-level="6.3.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#preparing-the-data-2"><i class="fa fa-check"></i><b>6.3.2</b> Preparing the data</a></li>
<li class="chapter" data-level="6.3.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#building-the-model-2"><i class="fa fa-check"></i><b>6.3.3</b> Building the model</a>
<ul>
<li class="chapter" data-level="6.3.3.1" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#validating-the-approach-using-k-fold-validation"><i class="fa fa-check"></i><b>6.3.3.1</b> Validating the approach using K-fold validation</a></li>
<li class="chapter" data-level="6.3.3.2" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#validation-with-more-iterations"><i class="fa fa-check"></i><b>6.3.3.2</b> Validation with more iterations</a></li>
<li class="chapter" data-level="6.3.3.3" data-path="chapter-3-getting-started-with-neural-networks.html"><a href="chapter-3-getting-started-with-neural-networks.html#tuning-amount-fo-hidden-layers"><i class="fa fa-check"></i><b>6.3.3.3</b> Tuning amount fo hidden layers</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html"><i class="fa fa-check"></i><b>7</b> Chapter 5 - Deep learning for computer vision</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#definition-of-convoluted-network"><i class="fa fa-check"></i><b>7.1</b> Definition of convoluted network</a></li>
<li class="chapter" data-level="7.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#spetial-heirarchical"><i class="fa fa-check"></i><b>7.2</b> Spetial heirarchical</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#tuning-parameters"><i class="fa fa-check"></i><b>7.2.1</b> Tuning parameters</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#data-modeling-techniques"><i class="fa fa-check"></i><b>7.2.2</b> Data modeling techniques</a>
<ul>
<li class="chapter" data-level="7.2.2.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#padding"><i class="fa fa-check"></i><b>7.2.2.1</b> Padding</a></li>
<li class="chapter" data-level="7.2.2.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#strides"><i class="fa fa-check"></i><b>7.2.2.2</b> Strides</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#the-max-pooling-operation"><i class="fa fa-check"></i><b>7.3</b> The max-pooling operation</a></li>
<li class="chapter" data-level="7.4" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#examples"><i class="fa fa-check"></i><b>7.4</b> Examples</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#training-a-cats-and-dogs-classifier-from-scratch."><i class="fa fa-check"></i><b>7.4.1</b> Training a cats and dogs classifier from scratch.</a>
<ul>
<li class="chapter" data-level="7.4.1.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#loading-data"><i class="fa fa-check"></i><b>7.4.1.1</b> Loading data</a></li>
<li class="chapter" data-level="7.4.1.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#building-the-model-3"><i class="fa fa-check"></i><b>7.4.1.2</b> Building the model</a></li>
<li class="chapter" data-level="7.4.1.3" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#data-preprocessing"><i class="fa fa-check"></i><b>7.4.1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="7.4.1.4" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#fitting-the-model"><i class="fa fa-check"></i><b>7.4.1.4</b> Fitting the model</a></li>
<li class="chapter" data-level="7.4.1.5" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#dealing-with-overfitting"><i class="fa fa-check"></i><b>7.4.1.5</b> Dealing with overfitting</a>
<ul>
<li class="chapter" data-level="7.4.1.5.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#randomized-transformations-data-augmentation"><i class="fa fa-check"></i><b>7.4.1.5.1</b> Randomized transformations / Data augmentation</a></li>
<li class="chapter" data-level="7.4.1.5.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#adding-dropout"><i class="fa fa-check"></i><b>7.4.1.5.2</b> Adding dropout</a></li>
<li class="chapter" data-level="7.4.1.5.3" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#max-pooling"><i class="fa fa-check"></i><b>7.4.1.5.3</b> Max pooling</a></li>
</ul></li>
<li class="chapter" data-level="7.4.1.6" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#training-with-dropout-and-random-image-transformations"><i class="fa fa-check"></i><b>7.4.1.6</b> Training with dropout and random image transformations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#using-a-pretrained-convnet"><i class="fa fa-check"></i><b>7.5</b> Using a pretrained convnet</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#feature-extraction"><i class="fa fa-check"></i><b>7.5.1</b> Feature extraction</a>
<ul>
<li class="chapter" data-level="7.5.1.1" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#feature-extraction-without-data-augmentation"><i class="fa fa-check"></i><b>7.5.1.1</b> Feature extraction without data augmentation</a></li>
<li class="chapter" data-level="7.5.1.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#feature-extraction-with-data-augmentation"><i class="fa fa-check"></i><b>7.5.1.2</b> Feature extraction with data augmentation</a></li>
</ul></li>
<li class="chapter" data-level="7.5.2" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#fine-tuning"><i class="fa fa-check"></i><b>7.5.2</b> Fine-tuning</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chapter-5-deep-learning-for-computer-vision.html"><a href="chapter-5-deep-learning-for-computer-vision.html#visualizing-what-convnets-learn"><i class="fa fa-check"></i><b>7.6</b> Visualizing what convnets learn</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html"><i class="fa fa-check"></i><b>8</b> Deep Learning for Text and Sequences</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#working-with-text-data"><i class="fa fa-check"></i><b>8.1</b> Working with Text Data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#one-hot-encoding-of-words-and-characters"><i class="fa fa-check"></i><b>8.1.1</b> One-hot encoding of words and characters</a></li>
<li class="chapter" data-level="8.1.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-word-embeddings"><i class="fa fa-check"></i><b>8.1.2</b> Using word embeddings</a>
<ul>
<li class="chapter" data-level="8.1.2.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#the-first-approach"><i class="fa fa-check"></i><b>8.1.2.1</b> The first approach</a></li>
<li class="chapter" data-level="8.1.2.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#the-second-approach---using-pretrained-word-embeddings"><i class="fa fa-check"></i><b>8.1.2.2</b> The second approach - using pretrained word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="8.1.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#putting-it-all-together-from-raw-text-to-word-embeddings"><i class="fa fa-check"></i><b>8.1.3</b> Putting it all together: from raw text to word embeddings</a>
<ul>
<li class="chapter" data-level="8.1.3.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#preprocessing-the-embeddings"><i class="fa fa-check"></i><b>8.1.3.1</b> Preprocessing the embeddings</a></li>
<li class="chapter" data-level="8.1.3.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#defining-a-model"><i class="fa fa-check"></i><b>8.1.3.2</b> Defining a model</a></li>
<li class="chapter" data-level="8.1.3.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#loading-glove-embeddings-in-the-model"><i class="fa fa-check"></i><b>8.1.3.3</b> Loading GloVe embeddings in the model</a></li>
<li class="chapter" data-level="8.1.3.4" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#training-and-evaluating-the-model"><i class="fa fa-check"></i><b>8.1.3.4</b> Training and evaluating the model</a></li>
<li class="chapter" data-level="8.1.3.5" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#training-and-evaluating-without-glove"><i class="fa fa-check"></i><b>8.1.3.5</b> Training and evaluating without GloVe</a></li>
<li class="chapter" data-level="8.1.3.6" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-test-data"><i class="fa fa-check"></i><b>8.1.3.6</b> Using test data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#understanding-recurrent-neural-networks-rnn"><i class="fa fa-check"></i><b>8.2</b> Understanding Recurrent Neural Networks (RNN)</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-recurrent-layer-in-keras"><i class="fa fa-check"></i><b>8.2.1</b> A recurrent layer in Keras</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#understanding-the-lstm-and-gru-layers"><i class="fa fa-check"></i><b>8.2.2</b> Understanding the LSTM and GRU layers</a>
<ul>
<li class="chapter" data-level="8.2.2.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#units-inside-gru-and-lstm"><i class="fa fa-check"></i><b>8.2.2.1</b> Units inside GRU and LSTM</a></li>
</ul></li>
<li class="chapter" data-level="8.2.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-concrete-lstm-example-in-keras"><i class="fa fa-check"></i><b>8.2.3</b> A concrete LSTM example in Keras</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#advanced-use-of-recurrent-neural-networks"><i class="fa fa-check"></i><b>8.3</b> Advanced use of Recurrent neural networks</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-temperature-forecasting-problem"><i class="fa fa-check"></i><b>8.3.1</b> A temperature-forecasting problem</a></li>
<li class="chapter" data-level="8.3.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#preparing-the-data-3"><i class="fa fa-check"></i><b>8.3.2</b> Preparing the data</a></li>
<li class="chapter" data-level="8.3.3" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-common-sense-non-machine-learning-baseline"><i class="fa fa-check"></i><b>8.3.3</b> A common-sense, non-machine-learning baseline</a></li>
<li class="chapter" data-level="8.3.4" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-basic-machine-learning-approach"><i class="fa fa-check"></i><b>8.3.4</b> A basic machine-learning approach</a></li>
<li class="chapter" data-level="8.3.5" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#a-first-recurrent-baseline"><i class="fa fa-check"></i><b>8.3.5</b> A first recurrent baseline</a></li>
<li class="chapter" data-level="8.3.6" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-recurrent-dropout-to-fight-overfitting"><i class="fa fa-check"></i><b>8.3.6</b> using recurrent dropout to fight overfitting</a></li>
<li class="chapter" data-level="8.3.7" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#stacking-recurrent-layers"><i class="fa fa-check"></i><b>8.3.7</b> Stacking recurrent layers</a></li>
<li class="chapter" data-level="8.3.8" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#using-bidirectional-rnns"><i class="fa fa-check"></i><b>8.3.8</b> Using bidirectional RNNs</a></li>
<li class="chapter" data-level="8.3.9" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#going-even-further"><i class="fa fa-check"></i><b>8.3.9</b> Going even further</a></li>
<li class="chapter" data-level="8.3.10" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#wrap-up"><i class="fa fa-check"></i><b>8.3.10</b> Wrap up</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#sequence-processing-with-convnets"><i class="fa fa-check"></i><b>8.4</b> Sequence processing with convnets</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#implementing-a-1d-convnet"><i class="fa fa-check"></i><b>8.4.1</b> Implementing a 1D convnet</a></li>
<li class="chapter" data-level="8.4.2" data-path="deep-learning-for-text-and-sequences.html"><a href="deep-learning-for-text-and-sequences.html#combining-cnns-and-rnns-to-process-long-sequences"><i class="fa fa-check"></i><b>8.4.2</b> Combining CNNs and RNNs to process long sequences</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html"><i class="fa fa-check"></i><b>9</b> Advanced Deep-Learning Best Practices</a>
<ul>
<li class="chapter" data-level="9.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#going-beyond-the-sequential-model-the-keras-functino-api"><i class="fa fa-check"></i><b>9.1</b> Going beyond the sequential model: the Keras functino API</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#introduction-to-the-functional-api"><i class="fa fa-check"></i><b>9.1.1</b> Introduction to the functional API</a></li>
<li class="chapter" data-level="9.1.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#multi-input-models"><i class="fa fa-check"></i><b>9.1.2</b> Multi-input models</a></li>
<li class="chapter" data-level="9.1.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#multi-output"><i class="fa fa-check"></i><b>9.1.3</b> Multi-output</a></li>
<li class="chapter" data-level="9.1.4" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#directed-acyclic-graphs-of-layers-dag"><i class="fa fa-check"></i><b>9.1.4</b> Directed acyclic graphs of layers (DAG)</a>
<ul>
<li class="chapter" data-level="9.1.4.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#inception-modules"><i class="fa fa-check"></i><b>9.1.4.1</b> Inception modules</a></li>
<li class="chapter" data-level="9.1.4.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#residual-connection"><i class="fa fa-check"></i><b>9.1.4.2</b> Residual Connection</a></li>
</ul></li>
<li class="chapter" data-level="9.1.5" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#layer-weight-sharing"><i class="fa fa-check"></i><b>9.1.5</b> Layer weight sharing</a></li>
<li class="chapter" data-level="9.1.6" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#models-as-layers"><i class="fa fa-check"></i><b>9.1.6</b> Models as layers</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#inspecting-and-monitoring-deep-learning-models-using-keras-callba--acks-and-tensorboard"><i class="fa fa-check"></i><b>9.2</b> Inspecting and monitoring deep-learning models using Keras callba- acks and TensorBoard</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#using-callbacks-to-act-on-a-model-during-training"><i class="fa fa-check"></i><b>9.2.1</b> Using callbacks to act on a model during training</a>
<ul>
<li class="chapter" data-level="9.2.1.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#the-model-checkpoint-and-early-stopping-callbacks"><i class="fa fa-check"></i><b>9.2.1.1</b> The model-checkpoint and early-stopping callbacks</a></li>
<li class="chapter" data-level="9.2.1.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#the-reduce-learning-rate-on-plateau-callback"><i class="fa fa-check"></i><b>9.2.1.2</b> The reduce-learning-rate-on-plateau callback</a></li>
<li class="chapter" data-level="9.2.1.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#writing-your-own-callback-functions"><i class="fa fa-check"></i><b>9.2.1.3</b> Writing your own callback functions</a></li>
</ul></li>
<li class="chapter" data-level="9.2.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#introduction-to-tensorboard-the-tensorflow-visualization-framework"><i class="fa fa-check"></i><b>9.2.2</b> Introduction to tensorBoard: the TensorFlow visualization framework</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#getting-the-most-of-your-models"><i class="fa fa-check"></i><b>9.3</b> Getting the most of your models</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#advanced-architecture-patterns"><i class="fa fa-check"></i><b>9.3.1</b> Advanced architecture patterns</a>
<ul>
<li class="chapter" data-level="9.3.1.1" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#batch-normalization"><i class="fa fa-check"></i><b>9.3.1.1</b> Batch normalization</a></li>
<li class="chapter" data-level="9.3.1.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#depthwise-separable-convolution"><i class="fa fa-check"></i><b>9.3.1.2</b> Depthwise separable convolution</a></li>
</ul></li>
<li class="chapter" data-level="9.3.2" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#hyperparameter-optimization"><i class="fa fa-check"></i><b>9.3.2</b> Hyperparameter optimization</a></li>
<li class="chapter" data-level="9.3.3" data-path="advanced-deep-learning-best-practices.html"><a href="advanced-deep-learning-best-practices.html#model-ensembling"><i class="fa fa-check"></i><b>9.3.3</b> Model ensembling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="structuring-data-transformation-and-model-assessments.html"><a href="structuring-data-transformation-and-model-assessments.html"><i class="fa fa-check"></i><b>10</b> Structuring data transformation and model assessments</a></li>
<li class="chapter" data-level="11" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html"><i class="fa fa-check"></i><b>11</b> Github and CSS styling</a>
<ul>
<li class="chapter" data-level="11.1" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html#managing-github"><i class="fa fa-check"></i><b>11.1</b> Managing GitHub</a></li>
<li class="chapter" data-level="11.2" data-path="github-and-css-styling.html"><a href="github-and-css-styling.html#css-styling"><i class="fa fa-check"></i><b>11.2</b> CSS Styling</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="moving-beyond-linearity" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Moving Beyond Linearity</h1>
<p>Literature:</p>
<ul>
<li>Moving Beyond Linearity (ISL CH7)</li>
</ul>
<p>Recall that complexity = also means lower interpretibility. This subject extents the linear models with the following:</p>
<ol style="list-style-type: decimal">
<li>Polunomial Regression - where polynomials of the variables are added.</li>
<li>Step Functions - where the x range is cut into k distinct regions to produce a qualitative variable. Hence also the name, piecewise constant function.</li>
<li>Regression Splines - a combination / extensions of number one and two. Where polynomials functions are applied in specified regions of an X range.</li>
<li>Smoothing Splines - Similar to the one above, but slightly different in the fitting process.</li>
<li>Local Regression - Similar to regression splines, but these are able to overlap.</li>
<li>Generalized Additive Models - allows to extent the model with several predictors.</li>
</ol>
<div id="models-beyond-linearity" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Models Beyond Linearity</h2>
<p>Notice that all approaches despite GAM are extensions of simple linear regression, as it only takes on one predictor variable.</p>
<div id="polynomial-regression" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Polynomial Regression</h3>
<p>Can be defined by the following</p>
<p><span class="math display" id="eq:PolynomialRegression">\[\begin{equation}
y_{i\ }=\ \beta_0+\beta_1x_i+\beta_2x_i^2+...+\ \beta_dx_i^d\ +\ \epsilon_i
\tag{2.1}
\end{equation}\]</span></p>
<p>Rules of thumb:</p>
<ul>
<li>We donât take on more than 3 or 4 degrees of d, as that yields strange lines</li>
</ul>
<p>Note that we can still use standard errors for coefficient estimates.</p>
<div id="beta-coefficients-and-variance" class="section level4" number="2.1.1.1">
<h4><span class="header-section-number">2.1.1.1</span> Beta coefficients and variance</h4>
<p>Each beta coefficient has its own variance (just as in linear regression).</p>
<p>It can be defined by a matrix of j dimensions, e.g., if you have 5 betas (including beta 0) we can construct the correlation matrix.</p>
<p>Covariance matrix can be identified by <span class="math inline">\(\hat{C}\)</span>.</p>
<p>Generally we get point estimates, but it is also interesting to show the confidence intervals (using 2 standard errors).</p>
<p>Notice, that we cant really interprete beta coefficients as we do with linear regression, hence we dont have the same ability to do inference as the coefficients are missleading.</p>
</div>
<div id="application-procedure" class="section level4" number="2.1.1.2">
<h4><span class="header-section-number">2.1.1.2</span> Application procedure</h4>
<ol style="list-style-type: decimal">
<li>Use <code>lm()</code> or <code>glm()</code></li>
<li>Use DV~poly(IV,degree)</li>
<li>Perform CV with <code>cv.glment()</code> / aonva F-test to select degree
<ul>
<li>This is basically either visually selecting the degrees that are the best using CV or using an ANOVA to assess if the MSE are significantly different from each other, hence an ANOVA test. Regarding ANOVA, if there stop being significance, then significant changes, e.g., in a poly 8, where the previous polynomials was not significant, then we can also disregard the 8âth polynomial.</li>
</ul></li>
<li>Fit the selected model</li>
<li>Look at <code>coef(summary(fit))</code></li>
<li>Plot data and predictions with <code>predict()</code></li>
<li>Check residuals</li>
<li>Interpret</li>
</ol>
<p>The lecture shows exercise number 6 in chapter 7.</p>
</div>
</div>
<div id="step-functions" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Step Functions</h3>
<p>This is literally just fitting a constant in different bins, see example on page 269. This is also called discretizing x. Thus it is not polynoomial, but it is non linear.</p>
<p>It is often applied when we see, e.g., five year age bins, e.g., a 20-25 year old is expected to earn so and so much etc. But we can only really use it, when there are natural cuts, hence one must be considerate using the model.</p>
<p><strong>Major disadvantage:</strong> If there are no natural breakpoints, then the model is likely to miss variance and also generalize too much.</p>
<p>Remember, that the steps reflect the average increase in Y for each step. Hence the first bin (range) is defined by <span class="math inline">\(\beta_0\)</span>, and can be regarded as the average of that range of x. Thus each coefficient of the ranges of x is to be understood as the average increase of response.</p>
<p>In other words, <span class="math inline">\(\beta_0\)</span> is the reference level, where the following cuts reflect the average increase or decrease hereon.</p>
<p>Note that we can still use standard errors for coefficient estimates.</p>
</div>
<div id="regression-splines" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Regression Splines</h3>
<div id="piecewise-polynomials" class="section level4" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Piecewise Polynomials</h4>
<p>This is basically polynomial regression, where the coefficients apply to specified ranges of X. The points where the coefficients changes are called <strong><em>knots</em></strong>. Hence a cubic function will look like the following.</p>
<p>Notice, that a piecewise polynomial function with no knots, is merely a standard polynomial function.</p>
<p><span class="math display" id="eq:PiecewisePolynomials">\[\begin{equation}
y_{i\ }=\ \beta_0+\beta_1x_i+\beta_2x_i^2+\ \beta_3x_i^3\ +\ \epsilon_i
\tag{2.2}
\end{equation}\]</span></p>
<p>Where it can be extended to be written for each range.</p>
<p>The coefficients can then be written with: <span class="math inline">\(\beta_{01},\beta_{11},\beta_{21}\)</span> etc. and for the second set: <span class="math inline">\(\beta_{02},\beta_{12},\beta_{22}\)</span></p>
<p>And for each of the betas in all of the cuts, you add one degree of freedom.</p>
<p>Rule of thumb:</p>
<ul>
<li>The more knots, the more complex = the more variance capture and noise trailed, hence low model bias but large model variance.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="Images/Piecewise%20regression.png" alt="Piecewise Polynomials" width="488" />
<p class="caption">
Figure 2.1: Piecewise Polynomials
</p>
</div>
</div>
<div id="constraints-and-splines" class="section level4" number="2.1.3.2">
<h4><span class="header-section-number">2.1.3.2</span> Constraints and Splines</h4>
<p><strong>It is a spline when the piecewise polynomials have been imposed with restrictions for continouity and derivatives, so the knot donât break or so to say, hence the knot will not be visible.</strong></p>
<p>Figure 7.3 show how the splines look, as the top left window tell, the jump is rather odd. Hence, we can force the fit to be continuous, by imposing a constraint.</p>
<p>Notice, that it is a piecewise polynomial regression, when you merely fit polynomials onto bins of data. If you want to have the ends tied together, you impose contraints and thus created a spline.</p>
<p>We can further constrain the model, with adding derivatives of the functions, hence the first derivative and the second derivative (where it in this instance created linear splines, that is because we have added <span class="math inline">\(d-1\)</span> i.e.Â 2 derivatives, if the function was to the power of 4, then we should have imposed 3 derivatives to achieve linearity in the splines.)</p>
<p>Hence, the <em>linear spline</em> can be defined by: It is piecewise splines of degree-d polynomials, with continuity in derivatives up to degree <span class="math inline">\(d-1\)</span> at each knot.</p>
<p>Hence we have the following constraints:</p>
<ol style="list-style-type: decimal">
<li>Continuity</li>
<li>Derivatives</li>
</ol>
</div>
<div id="choosing-the-number-and-location-of-the-knots" class="section level4" number="2.1.3.3">
<h4><span class="header-section-number">2.1.3.3</span> Choosing the number and location of the Knots</h4>
<p>Choosing amount of knots? One may ask themself, how many degrees of freedom do you want to include in the model?</p>
<p>Amount of knots is therefore corresponding to amount of degrees of freedom.</p>
<p>We can let software estimate the best amount and the best locations. Here one can use:</p>
<ol style="list-style-type: decimal">
<li>In-sample performance</li>
<li>Out-of-sample performance, e.g.Â with CV, perhaps extent to K folds with K tests, to ensure, that each variable has been held out once.</li>
</ol>
<p>This can be followed by visualizing the MSE for the different simulations with different amount of knots.</p>
</div>
<div id="degrees-of-freedom" class="section level4" number="2.1.3.4">
<h4><span class="header-section-number">2.1.3.4</span> Degrees of freedom</h4>
<p>You count the amount of coefficients in the piecewise polynomial. Then you deduct df as you impose restrictions.</p>
<p>e.g.Â with one knot, you impose continuity, then you deduct one 1 df. for each derivative that we impose we can subtract one df.</p>
<p>Therefore the example above: 8 df in the beginning less 1 for the cut and two for the two derivatives. Hence, we end up with 5 df.</p>
</div>
<div id="basis-splines-vs.-natural-splines" class="section level4" number="2.1.3.5">
<h4><span class="header-section-number">2.1.3.5</span> Basis splines vs.Â natural splines</h4>
<p>We see that in polynomial regression and splines we are able to apply the finction <code>bs()</code> and <code>ns()</code>, this stands for <em>basis splines</em> and <em>natural splines</em>. In principle they are very similar, although the natural spline just have an additional constraint in the regions below and above the first and last knot, where it is linear.</p>
<p><em>Notice, that a polynomial and step functions are basically basis functions</em></p>
<p>Natiral splies are often beneficiary as you often donât have much information in these regions, hence a normal polynomial (basis functions) will become very wiggly, hence we seek for linearity here.</p>
</div>
<div id="comparison-with-polynomial-regression" class="section level4" number="2.1.3.6">
<h4><span class="header-section-number">2.1.3.6</span> Comparison with Polynomial Regression</h4>
<p>With regression splines we are able to introduce knots that account for variance as it slightly resets the model in each section, hence we can fit the model to the data without having to impose as much complexity as we would in normal polynomial regression.</p>
<p>Hence one often observes that regression splines have more stability than polynomial regression.</p>
</div>
</div>
<div id="smoothing-splines" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Smoothing Splines</h3>
<p>In prediction this is slightly better than basic functions and natural splines.</p>
<p>This is basically attempting to find a model that captures the variance by a smoothing line. Doing so, we fit a very flexible model and impose restrictions upon this, to achieve a shrunken model, just as with Lasso and Ridge Regression. Thus, the smoothing (imposing restrictions) deals with overfitting.</p>
<p>Also as we havee discovered previously, that degrees of freedom is equivilant with amount of knots, e.g., in polynomial splines, then three knots in a cubic function leads to 6 degrees of freedom, hence an smooth spline with df = 6.8 can be said to approximately have 3 knots, but we will never really know.</p>
<p>This can be defined as a cubic spline with knot at every unique value of <span class="math inline">\(x_i\)</span></p>
<p>Hence we have the following model:</p>
<p><span class="math display" id="eq:SmoothingSplines">\[\begin{equation}
RSS=\sum_{i=1}^n\left(y_i-g\left(x_i\right)\right)^{^2}+\lambda\int g&#39;&#39;\left(t\right)^{^2}dt
\tag{2.3}
\end{equation}\]</span></p>
<p><em>i.e.Â Loss + Penalty</em></p>
<p><em>Recall that the penalty controls the curvature of the function</em></p>
<p>Where:</p>
<ul>
<li>We define model g(x)</li>
<li><span class="math inline">\((y_i-g(x_i))^{^2}\)</span> = the loss, meaning the difference between the fitted model and the actual yâs</li>
<li><span class="math inline">\(\lambda\)</span> = the tuning parameter, hence the restriction that we want to impose. If lambda is low, then much flexibility, if lambda is high, then low flexibility. Hence, controls the bias variance tradeoff.</li>
<li><span class="math inline">\(\int g&#39;&#39;\left(t\right)^{^2}dt\)</span> = a measure of how much <span class="math inline">\(g&#39;(x)\)</span> changes oer time. Hence, the higher we set <span class="math inline">\(\lambda\)</span> the more imposed restrictions, meaning the smoother the model, as lambda gets closer to infinity, the model becomes linear.</li>
</ul>
<div id="choosing-optimal-tuning-parameter" class="section level4" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> Choosing optimal tuning parameter</h4>
<p>The analytic LOOCV can be calculated, the procedure appears to be the same as for lasso and ridge regression. The book (page 279) describes this a bit in details. However it says that software is able to do this.</p>
<p>Basically what is done, is LOOCV and simulating different tuning parameters to assess what model that performs the best.</p>
<p>Notice, that in R we are not working with <span class="math inline">\(\lambda\)</span>, but we can control the df associated with lambda. Or we can just let the model choose.</p>
<p>With this, degrees of freedom is not the same as we are used to. This creates sparsity as we know from regularization.</p>
<p><em>We are not expected to explain this, but we should be able to interpret the results</em></p>
</div>
</div>
<div id="local-regression" class="section level3" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Local Regression</h3>
<p>This i basically fitting a linear regression to each x, where s observations are included in the fitting procedure. Thus, one creates several fits, that are based on the observations weighted, where observations close to <span class="math inline">\(x_0\)</span> (the center of the regression) are given the highest weight and then the weight is gradually decreasing.</p>
<p>This is often really good when you have outliers, as you define how big a neighborhood you want to evaluate (also called the span, e.g.Â span of 0.5 = 50% of the observations).</p>
<p>This can be visualized with:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="Images/Local%20Regression.png" alt="Local regression" width="494" />
<p class="caption">
Figure 2.2: Local regression
</p>
</div>
<p>Doing local regression has the following procedure (algorithm)</p>
<ol style="list-style-type: decimal">
<li>Gather the fraction <span class="math inline">\(s = k/n\)</span> of training points whose <span class="math inline">\(x_i\)</span> are closest to <span class="math inline">\(x_0\)</span>.</li>
<li>Assign a weight <span class="math inline">\(K_{i0} = K(x_i, x_0)\)</span> to each point in this neighborhood, so that the point furthest from x0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.</li>
<li>Fit a weighted least squares regression of the <span class="math inline">\(y_i\)</span> on the <span class="math inline">\(x_i\)</span> using the aforementioned weights, by finding <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> that minimize</li>
</ol>
<p><span class="math display" id="eq:LocalRegression">\[\begin{equation}
\sum_{i=1}^nK_{i0}\left(y_i-\beta_0-\beta_1x_i\right)^{^2}
\tag{2.4}
\end{equation}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>The fitted value at <span class="math inline">\(x_0\)</span> is given by <span class="math inline">\(\hat{f}(x_0)=\hat\beta_0+\hat\beta_1x_0\)</span></li>
</ol>
<p>Where we see how the model is</p>
</div>
<div id="generalized-additive-models" class="section level3" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> Generalized Additive Models</h3>
<p>This can naturally both be applied in regression and classification problems, futher elaborated in the following.</p>
<p>It is called generalized, as the dependent variable can be both continuous (e.g.Â Gaussian) and categorical (e.g., binomial, Poisson, or other distributions) distributed</p>
<p>Additive = the model is adding different polynomials of the IDV toghether. Notice, as the model is additive, it does not account for interactions, then you have to specify the interactions.</p>
<p>Thus GAM is merely an approach to make a model, where we include the posibility of having non linear components. Hence we include more complex model (with the ability to trail the observations more than linear models).</p>
<p>But the advantage of linear regressions, are that we are able to quickly deduct the effects the variables. Although we dont always have a linear relationship, hence you can be forced to choose a more complex model. (see the R file âGAMs with discussion Râ).</p>
<p>We have previously worked with non parametric models (e.g., KNN regression). GAM is in between linear regression and non parametric models.</p>
<p>That is the beuty of GAMs, as we preserve the ability of having transparancy in the model, despite it coming at a cost of worse prediction power than neural networks, but at such complex models, you are not able to deduct how the variables are interrelated, you can only say which are important and which are not.</p>
<p><strong><em>What are the assumptions for GAMS??</em></strong></p>
<ul>
<li></li>
</ul>
<div id="feature-selection" class="section level4" number="2.1.6.1">
<h4><span class="header-section-number">2.1.6.1</span> Feature Selection</h4>
<p>There are naturally a bunch of different approaches, we either manually assess this, running an automatic feature selection algorithm or both, see examples below. Also the casestudy in section <a href="moving-beyond-linearity.html#FacebookCasestudy">2.5</a> provides an example of all approaches to assess the features.</p>
<p>Feature selection approaches includes:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Generally looking at the variables âone by one,â to understand what features are important and to figure out how they contribute towards solving the problem.</p></li>
<li><p>Looking at the correlation matrix: If we are working with a model which assumes a linear relationship between the dependent and the independent variables, corr matrix can help us come up with an initial list of variable importance. However, corr matrix also works as a ârough informative toolâ for nonlinear modelling.</p></li>
<li><p>Running automatic feature selection algorithms. Functions in R include, among others:</p>
<ul>
<li>c1. regsubsets() function in âleapsâ library (presented in ISL, p.Â 244); used to select the best size model that contains a given number of predictors, where best is quantified using Residual Sum of Squares (RSS). Although regsubsets() is based on testing linear models, it works as a âroughâ list for nonlinear models.</li>
<li>c2. step.Gam() function in âgamâ library for stepwise selection of variables in GAM models. This is useful when the number of predictors is not very high.</li>
<li>c3. advanced feature selection methods based on other data-mining techniques, including but not only: random forests, Bayesian Networks, Neural Networks, or other. Notice, that we can use very complex methods of feature selection, and then construct a model that is more transparant, for instance GAMs</li>
</ul></li>
</ol>
<p>We will end up with candidate models, during the case study a new approach was introduced, which was quite nice, this has the following procedure:</p>
<ol style="list-style-type: decimal">
<li>Train the candidate model</li>
<li>Do cross validation of k partitions on the train data.</li>
<li>Select the best model.</li>
<li>Run this on the test data. <em>NB: Ideally we donât want to run more than one model on the test data, as that will lead to data leakage and we may choose the best model based on the best fit to the test data. That is wrong.</em></li>
</ol>
</div>
<div id="gam-for-regression-problems" class="section level4" number="2.1.6.2">
<h4><span class="header-section-number">2.1.6.2</span> GAM for regression problems</h4>
<p>Now we move beyond being constrained to only one predictor variable, hence GAM can be seen more as an extension of multiple linear regression. Hence GAM is a combination of different functions, where they are each fitted while holding the other variables fixed. GAM can consist of any different non-linear model, e.g., we can just as well use local regression, polynomial regression, or any combination of the approaches seen above in this subsection.</p>
<p>See section @ref(fig:GAMPlotLab7.8.3) for explanation of interpretation of the plots.</p>
<p><strong>Disadvantages of GAM:</strong></p>
<ol style="list-style-type: decimal">
<li>The fitting procedure holds the other variables fixed, hence it does not count for interactions. Therefore, one may manually construct interaction variables to account for this, just like in mulitple linear regression.</li>
<li>Prediction wise it is not competitive with Neural Networks and Support Vector Machines.</li>
</ol>
<p><strong>Advantages of GAM:</strong></p>
<ol style="list-style-type: decimal">
<li>Allowing to fit non linear function for j variables (<span class="math inline">\(f_j\)</span>)</li>
<li>Has potential of making more accurate predictions</li>
<li>As the model is additive (meaning that each function is fitted holding the other variables fixed) we are still able to make inference, e.g., assessing how one variable affects the y variable.</li>
<li>Smoothness of function <span class="math inline">\(f_j\)</span> can be summarized with degrees of freedom.</li>
<li>Often applied when aiming for explanatory analysis (instead of prediction)</li>
</ol>
<div id="interpretation-of-output" class="section level5" number="2.1.6.2.1">
<h5><span class="header-section-number">2.1.6.2.1</span> Interpretation of output</h5>
<p>Remember GAM is a construct of M different models, hence there are different areas one must be aware of.</p>
<ul>
<li><p>With smooth variables: <code>summary()</code> will output <em>approximate signifcance of smooth terms</em>,</p>
<ul>
<li>Here we can assess if there is statistical evidence for including the smoothing on the variable. But notice, that this is merely the p-value.</li>
<li>We can also assess the <code>edf</code> = estimated degrees of freedom, hence the flexibility of each smoothed variable.</li>
</ul></li>
<li><p>With linear variables we get <em>Parametric coefficients</em> (from <code>summary()</code>): These are what we know from the linear scenario. It assigns a p-value and coefficients to the linear variables. Interpretation wise, we should not pay too much attention to this, although it is always good to have the p-values in mind.</p></li>
<li><p>With <code>summary()</code> we also get the traditional information, such as <span class="math inline">\(R^2\)</span> and adjusted. This can also be found in the documentation.</p></li>
</ul>
</div>
</div>
<div id="gam-for-classification-problems" class="section level4" number="2.1.6.3">
<h4><span class="header-section-number">2.1.6.3</span> GAM for classification problems</h4>
<p>When y is qualitative (categorical), GAM can also be applied in the logistical form.</p>
<p>As discovered in the classification section, we can apply logits (log of odds) and odds, see material from first semester.</p>
<p>The same advantages and disadvantages as in the prior section applies.</p>
</div>
</div>
<div id="model-assessment" class="section level3" number="2.1.7">
<h3><span class="header-section-number">2.1.7</span> Model assessment</h3>
<p>The same applies as in ML1, thus I refer to ML1.</p>
<p>Also one must always assess the residuals, e.g., for normality.</p>
</div>
</div>
<div id="lecture-notes" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Lecture notes</h2>
<ul>
<li><p>Talking about polynomials and what they are, e.g., can be parabula, etc.</p></li>
<li><p>With splines we set polynomials in each of the X regions. Where just polynomial regression is fitted to the whole dataset, and not just in regions.</p></li>
<li><p>By default poly() will make orthogonal polynomials. Meaning that it tries to create orthogonal terms, where the polynomials are orthogonal (not related to each other). As that is default, then we have to define, that we want to use raw data, hence raw = TRUE, hence we will get the regular polynomials of the dataa.</p></li>
<li><p>See notes in her R file.</p></li>
</ul>
<p>General Wrap-Up</p>
<ul>
<li>The approaches are similar</li>
<li>Must be aware of why the models are used</li>
<li>Try them out</li>
<li>Assess how they look</li>
</ul>
</div>
<div id="lab-section" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Lab section</h2>
<p>Loading the data that will be used throughout the lab section.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="moving-beyond-linearity.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb4-2"><a href="moving-beyond-linearity.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(Wage)</span>
<span id="cb4-3"><a href="moving-beyond-linearity.html#cb4-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Wage</span></code></pre></div>
<p><br />
</p>
<div id="polynomial-regression-and-step-functions" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Polynomial Regression and Step Functions</h3>
<div id="continous-model" class="section level4" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Continous model</h4>
<p>Fitting the model:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="moving-beyond-linearity.html#cb5-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">4</span>) <span class="co">#Orthogonal polynomials</span></span>
<span id="cb5-2"><a href="moving-beyond-linearity.html#cb5-2" aria-hidden="true" tabindex="-1"></a>          ,<span class="at">data =</span> df)</span>
<span id="cb5-3"><a href="moving-beyond-linearity.html#cb5-3" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">4</span>,<span class="at">raw =</span> <span class="cn">TRUE</span>) <span class="co">#Orthogonal polynomials</span></span>
<span id="cb5-4"><a href="moving-beyond-linearity.html#cb5-4" aria-hidden="true" tabindex="-1"></a>          ,<span class="at">data =</span> df)</span></code></pre></div>
<p>Note: <code>poly()</code> returns orthogonal polynomials, which is some linear combination of the variables to the d power. See the following two examples when using orthogonal and normal polynomials:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="moving-beyond-linearity.html#cb6-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb6-2"><a href="moving-beyond-linearity.html#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">&quot;Orthogonal&quot;</span>)</span>
<span id="cb6-3"><a href="moving-beyond-linearity.html#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(df<span class="sc">$</span>age,<span class="fu">poly</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">degree =</span> <span class="dv">4</span>))[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,]  <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb6-4"><a href="moving-beyond-linearity.html#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">&quot;Regular&quot;</span>)</span>
<span id="cb6-5"><a href="moving-beyond-linearity.html#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(df<span class="sc">$</span>age,<span class="fu">poly</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">degree =</span> <span class="dv">4</span>,<span class="at">raw =</span> <span class="cn">TRUE</span>))[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,] <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb6-6"><a href="moving-beyond-linearity.html#cb6-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] &quot;Orthogonal&quot;
##                     1            2             3           4
## [1,] 18 -0.0386247992  0.055908727 -0.0717405794  0.08672985
## [2,] 24 -0.0291326034  0.026298066 -0.0145499511 -0.00259928
## [3,] 45  0.0040900817 -0.014506548 -0.0001331835  0.01448009
## [4,] 43  0.0009260164 -0.014831404  0.0045136682  0.01265751
## [5,] 50  0.0120002448 -0.009815846 -0.0111366263  0.01021146
## [1] &quot;Regular&quot;
##          1    2      3       4
## [1,] 18 18  324   5832  104976
## [2,] 24 24  576  13824  331776
## [3,] 45 45 2025  91125 4100625
## [4,] 43 43 1849  79507 3418801
## [5,] 50 50 2500 125000 6250000</code></pre>
<p>In the end, it does not have a noticeable effect.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="moving-beyond-linearity.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">scipen =</span> <span class="dv">5</span>)</span>
<span id="cb8-2"><a href="moving-beyond-linearity.html#cb8-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb8-3"><a href="moving-beyond-linearity.html#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(<span class="fu">summary</span>(fit)) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb8-4"><a href="moving-beyond-linearity.html#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(<span class="fu">summary</span>(fit2)) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb8-5"><a href="moving-beyond-linearity.html#cb8-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>##                 Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)    111.70361  0.7287409 153.283015 0.000000e+00
## poly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28
## poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32
## poly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03
## poly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02
##                                  Estimate     Std. Error   t value     Pr(&gt;|t|)
## (Intercept)               -184.1541797743 60.04037718327 -3.067172 0.0021802539
## poly(age, 4, raw = TRUE)1   21.2455205321  5.88674824448  3.609042 0.0003123618
## poly(age, 4, raw = TRUE)2   -0.5638593126  0.20610825640 -2.735743 0.0062606446
## poly(age, 4, raw = TRUE)3    0.0068106877  0.00306593115  2.221409 0.0263977518
## poly(age, 4, raw = TRUE)4   -0.0000320383  0.00001641359 -1.951938 0.0510386498</code></pre>
<p>Even though the coefficients are different and the p-values hereof, the fitted values will be indistinguishable <span class="citation">(<a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al. 2013, 288</a>)</span>. This is also shown later.</p>
<p><em>Alternatives to using <code>poly()</code>??</em></p>
<p>We have two alternatives:</p>
<ol style="list-style-type: decimal">
<li><p>Using <code>I()</code></p></li>
<li><p>Using <code>cbind()</code></p></li>
<li><p>Using <code>I()</code></p></li>
</ol>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="moving-beyond-linearity.html#cb10-1" aria-hidden="true" tabindex="-1"></a>fit2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> age <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">4</span>) <span class="co">#Note that &#39;I()&#39; is added</span></span>
<span id="cb10-2"><a href="moving-beyond-linearity.html#cb10-2" aria-hidden="true" tabindex="-1"></a>            ,<span class="at">data =</span> df)</span>
<span id="cb10-3"><a href="moving-beyond-linearity.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit2a)</span></code></pre></div>
<pre><code>##     (Intercept)             age        I(age^2)        I(age^3)        I(age^4) 
## -184.1541797743   21.2455205321   -0.5638593126    0.0068106877   -0.0000320383</code></pre>
<p><em>Notice <code>I()</code> as â^â has another special meaning in formulas</em></p>
<p>Hence we see that the coefficients are the same.</p>
<ol start="2" style="list-style-type: decimal">
<li>Using <code>cbind()</code></li>
</ol>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="moving-beyond-linearity.html#cb12-1" aria-hidden="true" tabindex="-1"></a>fit2b <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">cbind</span>(age,age<span class="sc">^</span><span class="dv">2</span>,age<span class="sc">^</span><span class="dv">3</span>,age<span class="sc">^</span><span class="dv">4</span>)</span>
<span id="cb12-2"><a href="moving-beyond-linearity.html#cb12-2" aria-hidden="true" tabindex="-1"></a>            ,<span class="at">data =</span> df)</span>
<span id="cb12-3"><a href="moving-beyond-linearity.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit2b)</span></code></pre></div>
<pre><code>##                        (Intercept) cbind(age, age^2, age^3, age^4)age 
##                    -184.1541797743                      21.2455205321 
##    cbind(age, age^2, age^3, age^4)    cbind(age, age^2, age^3, age^4) 
##                      -0.5638593126                       0.0068106877 
##    cbind(age, age^2, age^3, age^4) 
##                      -0.0000320383</code></pre>
<p>We see that we are now able to use â^â within the <code>cbind()</code>.</p>
<p><br />
</p>
<p>proceeding with the lab sections. We can now present a grid of values for age, at which we want predictions and then call the <code>predict()</code> and also plot the standard errors.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="moving-beyond-linearity.html#cb14-1" aria-hidden="true" tabindex="-1"></a>agelims <span class="ot">&lt;-</span> <span class="fu">range</span>(df<span class="sc">$</span>age) <span class="co">#The min and max</span></span>
<span id="cb14-2"><a href="moving-beyond-linearity.html#cb14-2" aria-hidden="true" tabindex="-1"></a>age.grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> agelims[<span class="dv">1</span>],<span class="at">to =</span> agelims[<span class="dv">2</span>]) <span class="co">#Creating a counter within the range</span></span>
<span id="cb14-3"><a href="moving-beyond-linearity.html#cb14-3" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit</span>
<span id="cb14-4"><a href="moving-beyond-linearity.html#cb14-4" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid) <span class="co">#Creating a list with the counter named age, so it fits the IV naming</span></span>
<span id="cb14-5"><a href="moving-beyond-linearity.html#cb14-5" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-6"><a href="moving-beyond-linearity.html#cb14-6" aria-hidden="true" tabindex="-1"></a>se.bands <span class="ot">&lt;-</span> <span class="fu">cbind</span>(preds<span class="sc">$</span>fit <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>preds<span class="sc">$</span>se.fit <span class="co">#Upper band</span></span>
<span id="cb14-7"><a href="moving-beyond-linearity.html#cb14-7" aria-hidden="true" tabindex="-1"></a>                  ,preds<span class="sc">$</span>fit<span class="dv">-2</span><span class="sc">*</span>preds<span class="sc">$</span>se.fit) <span class="co">#Lower band</span></span></code></pre></div>
<p>Notice that 2 SE (2 sd) = 95%, hence we expect to contain 95% of the data within confidence levels.</p>
<p>Now we can plot the data</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="moving-beyond-linearity.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">y =</span> df<span class="sc">$</span>wage</span>
<span id="cb15-2"><a href="moving-beyond-linearity.html#cb15-2" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlim =</span> agelims</span>
<span id="cb15-3"><a href="moving-beyond-linearity.html#cb15-3" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">cex =</span> <span class="fl">0.5</span> <span class="co">#Size of dots</span></span>
<span id="cb15-4"><a href="moving-beyond-linearity.html#cb15-4" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb15-5"><a href="moving-beyond-linearity.html#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Degree-4 Polynomial&quot;</span>,<span class="at">outer =</span> <span class="cn">TRUE</span>)</span>
<span id="cb15-6"><a href="moving-beyond-linearity.html#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> age.grid,<span class="at">y =</span> preds<span class="sc">$</span>fit</span>
<span id="cb15-7"><a href="moving-beyond-linearity.html#cb15-7" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb15-8"><a href="moving-beyond-linearity.html#cb15-8" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-17-1.png" width="720" style="display: block; margin: auto;" /></p>
<p><strong><em>Comparison between raw polynomials and orthogonal polynomials</em></strong></p>
<p>With the following we see that the difference of the fitted values are practically 0.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="moving-beyond-linearity.html#cb16-1" aria-hidden="true" tabindex="-1"></a>preds2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit2</span>
<span id="cb16-2"><a href="moving-beyond-linearity.html#cb16-2" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid)</span>
<span id="cb16-3"><a href="moving-beyond-linearity.html#cb16-3" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb16-4"><a href="moving-beyond-linearity.html#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(<span class="fu">abs</span>(preds<span class="sc">$</span>fit<span class="sc">-</span>preds2<span class="sc">$</span>fit))</span></code></pre></div>
<pre><code>## [1] 7.81597e-11</code></pre>
<p>In terms of predictions, the two approaches are more or less the same, although the orthogonal polynomials removes some effect of collinearity.</p>
<p><br />
</p>
<hr />
<p><strong>Assessing what polynomial to include</strong></p>
<p>Now we can compare models with different orthogonal polynomials. Using ANOVA, which compare the RSS to see if the decrease in RSS is significant.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="moving-beyond-linearity.html#cb18-1" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span>age,<span class="at">data=</span>df)</span>
<span id="cb18-2"><a href="moving-beyond-linearity.html#cb18-2" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span><span class="fu">poly</span>(df<span class="sc">$</span>age,<span class="dv">2</span>),<span class="at">data=</span>df)</span>
<span id="cb18-3"><a href="moving-beyond-linearity.html#cb18-3" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span><span class="fu">poly</span>(df<span class="sc">$</span>age,<span class="dv">3</span>),<span class="at">data=</span>df)</span>
<span id="cb18-4"><a href="moving-beyond-linearity.html#cb18-4" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span><span class="fu">poly</span>(df<span class="sc">$</span>age,<span class="dv">4</span>),<span class="at">data=</span>df)</span>
<span id="cb18-5"><a href="moving-beyond-linearity.html#cb18-5" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage<span class="sc">~</span><span class="fu">poly</span>(df<span class="sc">$</span>age,<span class="dv">5</span>),<span class="at">data=</span>df)</span>
<span id="cb18-6"><a href="moving-beyond-linearity.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fit<span class="fl">.1</span>,fit<span class="fl">.2</span>,fit<span class="fl">.3</span>,fit<span class="fl">.4</span>,fit<span class="fl">.5</span>)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">Res.Df</th>
<th align="right">RSS</th>
<th align="right">Df</th>
<th align="right">Sum of Sq</th>
<th align="right">F</th>
<th align="right">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2998</td>
<td align="right">5022216</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">2997</td>
<td align="right">4793430</td>
<td align="right">1</td>
<td align="right">228786.010</td>
<td align="right">143.5931074</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="right">2996</td>
<td align="right">4777674</td>
<td align="right">1</td>
<td align="right">15755.694</td>
<td align="right">9.8887559</td>
<td align="right">0.0016792</td>
</tr>
<tr class="even">
<td align="right">2995</td>
<td align="right">4771604</td>
<td align="right">1</td>
<td align="right">6070.152</td>
<td align="right">3.8098134</td>
<td align="right">0.0510462</td>
</tr>
<tr class="odd">
<td align="right">2994</td>
<td align="right">4770322</td>
<td align="right">1</td>
<td align="right">1282.563</td>
<td align="right">0.8049758</td>
<td align="right">0.3696820</td>
</tr>
</tbody>
</table>
</div>
<p><em>Note, </em></p>
<ul>
<li><em>the anova compares the sum of resduals squared.</em></li>
<li><em>the anova follows an F distribution, hence we could apple the critical values</em></li>
</ul>
<p>based on the anova we see that the errors change significantly until the 5th degree, hence the decision should be to take the model with order 4 of polynomials.</p>
<p>Notice, that the model will never become worse in sample when complexity is added, as we fit the model more to the data.</p>
<p><em>Alternative</em></p>
<p>We could also have obtained the same output using <code>coef()</code> instead of the anove, where we see that teh p-values are the same, also the squared value of t (<span class="math inline">\(t^2=F\)</span>).</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="moving-beyond-linearity.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit<span class="fl">.5</span>))</span></code></pre></div>
<pre><code>##                    Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)       111.70361  0.7287647 153.2780243 0.000000e+00
## poly(df$age, 5)1  447.06785 39.9160847  11.2001930 1.491111e-28
## poly(df$age, 5)2 -478.31581 39.9160847 -11.9830341 2.367734e-32
## poly(df$age, 5)3  125.52169 39.9160847   3.1446392 1.679213e-03
## poly(df$age, 5)4  -77.91118 39.9160847  -1.9518743 5.104623e-02
## poly(df$age, 5)5  -35.81289 39.9160847  -0.8972045 3.696820e-01</code></pre>
<p><em>Notice: this is only an alternative when we exclusively have polynomials in the model!</em></p>
<hr />
<p><br />
</p>
<p><strong>Using ANOVA to assess for best models</strong></p>
<p>The following is another example of using ANOVA where different variables are used:</p>
<p>And recall, that we should never apply p-values of variables in a model, to decide which that should be included.</p>
<p><strong>NOTE; this approach only works when the models are nested, meanin that the overall variables are the same, hence M2 could not have regian for instance, they all need to have the same overall variable</strong></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="moving-beyond-linearity.html#cb21-1" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.1</span> <span class="ot">=</span> <span class="fu">lm</span>(wage<span class="sc">~</span>education <span class="sc">+</span>age ,<span class="at">data=</span>df)</span>
<span id="cb21-2"><a href="moving-beyond-linearity.html#cb21-2" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">lm</span>(wage<span class="sc">~</span>education <span class="sc">+</span><span class="fu">poly</span>(age ,<span class="dv">2</span>) ,<span class="at">data=</span>df)</span>
<span id="cb21-3"><a href="moving-beyond-linearity.html#cb21-3" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.3</span> <span class="ot">=</span> <span class="fu">lm</span>(wage<span class="sc">~</span>education <span class="sc">+</span><span class="fu">poly</span>(age ,<span class="dv">3</span>) ,<span class="at">data=</span>df)</span>
<span id="cb21-4"><a href="moving-beyond-linearity.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fit<span class="fl">.1</span>,fit<span class="fl">.2</span>,fit<span class="fl">.3</span>)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">Res.Df</th>
<th align="right">RSS</th>
<th align="right">Df</th>
<th align="right">Sum of Sq</th>
<th align="right">F</th>
<th align="right">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2994</td>
<td align="right">3867992</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">2993</td>
<td align="right">3725395</td>
<td align="right">1</td>
<td align="right">142597.10</td>
<td align="right">114.696898</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="right">2992</td>
<td align="right">3719809</td>
<td align="right">1</td>
<td align="right">5586.66</td>
<td align="right">4.493588</td>
<td align="right">0.0341043</td>
</tr>
</tbody>
</table>
</div>
<p>What are we looking for?</p>
<ol style="list-style-type: decimal">
<li>What model lowers the RSS significantly.</li>
</ol>
<p><strong>Using CV</strong></p>
<p>We could also have chosen the order of polynomials using cross validation.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="moving-beyond-linearity.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb22-2"><a href="moving-beyond-linearity.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">19</span>)</span>
<span id="cb22-3"><a href="moving-beyond-linearity.html#cb22-3" aria-hidden="true" tabindex="-1"></a>cv.error <span class="ot">=</span> <span class="fu">rep</span> (<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb22-4"><a href="moving-beyond-linearity.html#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span>
<span id="cb22-5"><a href="moving-beyond-linearity.html#cb22-5" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb22-6"><a href="moving-beyond-linearity.html#cb22-6" aria-hidden="true" tabindex="-1"></a>  fit.i<span class="ot">=</span><span class="fu">glm</span>(wage<span class="sc">~</span><span class="fu">poly</span>(age,i),<span class="at">data=</span>Wage)  <span class="co"># notice glm here in conjunction with cv.glm function</span></span>
<span id="cb22-7"><a href="moving-beyond-linearity.html#cb22-7" aria-hidden="true" tabindex="-1"></a>  cv.error[i]<span class="ot">=</span><span class="fu">cv.glm</span>(Wage, fit.i, <span class="at">K=</span><span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">1</span>] <span class="co">#K fold CV</span></span>
<span id="cb22-8"><a href="moving-beyond-linearity.html#cb22-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-9"><a href="moving-beyond-linearity.html#cb22-9" aria-hidden="true" tabindex="-1"></a>cv.error <span class="co"># the CV errors of the five polynomials models</span></span></code></pre></div>
<pre><code>## [1] 1674.979 1600.176 1595.913 1594.003 1596.304</code></pre>
<p>Concl: A 5 order model is not justified as the it starts increasing. Also we see that the 4th order d, is the best, which corresponds with previous findings.</p>
</div>
<div id="logarithmic-model" class="section level4" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Logarithmic model</h4>
<p>The procedure is per se the same, but now we are working with a probabilistic model instead of. Hence the outcome must be binary. Thus, it is decided to predict whether a persons wage is higher or lower than 250.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="moving-beyond-linearity.html#cb24-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">4</span>) <span class="co">#Note the use of I()</span></span>
<span id="cb24-2"><a href="moving-beyond-linearity.html#cb24-2" aria-hidden="true" tabindex="-1"></a>           ,<span class="at">data =</span> df</span>
<span id="cb24-3"><a href="moving-beyond-linearity.html#cb24-3" aria-hidden="true" tabindex="-1"></a>           ,<span class="at">family =</span> binomial)</span></code></pre></div>
<p>Note, that again <code>I()</code> is used, where the expression is evaluated on the fly, one could naturally also had made a vector of the classes.</p>
<p><em>Note, by default <code>glm()</code> will transform TRUE and FALSE to respectively 1 and 0.</em></p>
<p>Now we can make predictions, which are logits, these can be used for much, hence later we will transform them into probabilities.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="moving-beyond-linearity.html#cb25-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(fit</span>
<span id="cb25-2"><a href="moving-beyond-linearity.html#cb25-2" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age=</span>age.grid)</span>
<span id="cb25-3"><a href="moving-beyond-linearity.html#cb25-3" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb25-4"><a href="moving-beyond-linearity.html#cb25-4" aria-hidden="true" tabindex="-1"></a>                <span class="co"># We could have added type = response to get probabilities</span></span>
<span id="cb25-5"><a href="moving-beyond-linearity.html#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="moving-beyond-linearity.html#cb25-6" aria-hidden="true" tabindex="-1"></a>preds<span class="sc">$</span>fit[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]<span class="co">#First 10 logits</span></span></code></pre></div>
<pre><code>##          1          2          3          4          5          6          7 
## -18.438190 -16.395452 -14.560646 -12.919746 -11.459196 -10.165904  -9.027249 
##          8          9         10 
##  -8.031077  -7.165700  -6.419899</code></pre>
<p>To make confidence intevals for Pr(Y = 1|X), i.e.</p>
<p><span class="math display" id="eq:LogConfidenceinterval">\[\begin{equation}
Pr(Y=1|X)= \frac{exp(X\beta)}{1+exp(X\beta)}
\tag{2.5}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(X\beta\)</span> can be explained by:</p>
<p><span class="math display" id="eq:XBeta">\[\begin{equation}
log(\frac{Pr(Y=1|X)}{1-Pr(Y=1|X)})=X\beta
\tag{2.6}
\end{equation}\]</span></p>
<p>Hence we must first calculate <span class="math inline">\(X\beta\)</span> to find Pr(Y=1|X).</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="moving-beyond-linearity.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Making Prbabilities</span></span>
<span id="cb27-2"><a href="moving-beyond-linearity.html#cb27-2" aria-hidden="true" tabindex="-1"></a>pfit <span class="ot">=</span> <span class="fu">exp</span>(preds<span class="sc">$</span>fit)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(preds<span class="sc">$</span>fit)) <span class="co">#See equation above</span></span>
<span id="cb27-3"><a href="moving-beyond-linearity.html#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="moving-beyond-linearity.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">#X beta</span></span>
<span id="cb27-5"><a href="moving-beyond-linearity.html#cb27-5" aria-hidden="true" tabindex="-1"></a>se.bands.logit <span class="ot">=</span> <span class="fu">cbind</span>(preds<span class="sc">$</span>fit<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>preds<span class="sc">$</span>se.fit <span class="co">#Upper level</span></span>
<span id="cb27-6"><a href="moving-beyond-linearity.html#cb27-6" aria-hidden="true" tabindex="-1"></a>                       ,preds<span class="sc">$</span>fit<span class="dv">-2</span><span class="sc">*</span>preds<span class="sc">$</span>se.fit) <span class="co">#Lower level</span></span>
<span id="cb27-7"><a href="moving-beyond-linearity.html#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="moving-beyond-linearity.html#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Pr(Y = 1|X)</span></span>
<span id="cb27-9"><a href="moving-beyond-linearity.html#cb27-9" aria-hidden="true" tabindex="-1"></a>se.bands <span class="ot">=</span> <span class="fu">exp</span>(se.bands.logit)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(se.bands.logit))</span></code></pre></div>
<p>Remember that 2 SE = 95%, thus with the confidence levels we expect to contain 95% of the data.</p>
<p>Notice, that the posterior probabilities could also have been found by using <code>predict()</code>, see the following:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="moving-beyond-linearity.html#cb28-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span> (fit</span>
<span id="cb28-2"><a href="moving-beyond-linearity.html#cb28-2" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid)</span>
<span id="cb28-3"><a href="moving-beyond-linearity.html#cb28-3" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">type =</span> <span class="st">&quot;response&quot;</span> <span class="co">#Getting probabilities instead of logits</span></span>
<span id="cb28-4"><a href="moving-beyond-linearity.html#cb28-4" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><strong>NOTICE: for some reason this will lead to wrong confidence intervals <span class="citation">(<a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al. 2013, 292</a>)</span>, thus we prefer the regular approach, as shown before</strong></p>
<p>Now we can make the right hand plot, so we can compare with continous result.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="moving-beyond-linearity.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb29-2"><a href="moving-beyond-linearity.html#cb29-2" aria-hidden="true" tabindex="-1"></a>    ,<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="fl">4.5</span>,<span class="fl">4.5</span>,<span class="fl">1.1</span>) <span class="co">#Controls the margins</span></span>
<span id="cb29-3"><a href="moving-beyond-linearity.html#cb29-3" aria-hidden="true" tabindex="-1"></a>    ,<span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">0</span>)) <span class="co">#Controls the margins</span></span>
<span id="cb29-4"><a href="moving-beyond-linearity.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Copy from earlier to combine plots</span></span>
<span id="cb29-5"><a href="moving-beyond-linearity.html#cb29-5" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">4</span>) <span class="co">#Orthogonal polynomials</span></span>
<span id="cb29-6"><a href="moving-beyond-linearity.html#cb29-6" aria-hidden="true" tabindex="-1"></a>          ,<span class="at">data =</span> df)</span>
<span id="cb29-7"><a href="moving-beyond-linearity.html#cb29-7" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit</span>
<span id="cb29-8"><a href="moving-beyond-linearity.html#cb29-8" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid)</span>
<span id="cb29-9"><a href="moving-beyond-linearity.html#cb29-9" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-10"><a href="moving-beyond-linearity.html#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">y =</span> df<span class="sc">$</span>wage</span>
<span id="cb29-11"><a href="moving-beyond-linearity.html#cb29-11" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlim =</span> agelims</span>
<span id="cb29-12"><a href="moving-beyond-linearity.html#cb29-12" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">cex =</span> <span class="fl">0.5</span> <span class="co">#Size of dots</span></span>
<span id="cb29-13"><a href="moving-beyond-linearity.html#cb29-13" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb29-14"><a href="moving-beyond-linearity.html#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Degree-4 Polynomial&quot;</span>,<span class="at">outer =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-15"><a href="moving-beyond-linearity.html#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> age.grid,<span class="at">y =</span> preds<span class="sc">$</span>fit</span>
<span id="cb29-16"><a href="moving-beyond-linearity.html#cb29-16" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb29-17"><a href="moving-beyond-linearity.html#cb29-17" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb29-18"><a href="moving-beyond-linearity.html#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="moving-beyond-linearity.html#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="co">#The new plot</span></span>
<span id="cb29-20"><a href="moving-beyond-linearity.html#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> age,<span class="at">y =</span> <span class="fu">I</span>(wage <span class="sc">&gt;</span><span class="dv">250</span>)</span>
<span id="cb29-21"><a href="moving-beyond-linearity.html#cb29-21" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlim =</span> agelims</span>
<span id="cb29-22"><a href="moving-beyond-linearity.html#cb29-22" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">type =</span><span class="st">&quot;n&quot;</span></span>
<span id="cb29-23"><a href="moving-beyond-linearity.html#cb29-23" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,.<span class="dv">2</span>))</span>
<span id="cb29-24"><a href="moving-beyond-linearity.html#cb29-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-25"><a href="moving-beyond-linearity.html#cb29-25" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">jitter</span>(age)</span>
<span id="cb29-26"><a href="moving-beyond-linearity.html#cb29-26" aria-hidden="true" tabindex="-1"></a>       ,<span class="fu">I</span>((wage<span class="sc">&gt;</span><span class="dv">250</span>)<span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb29-27"><a href="moving-beyond-linearity.html#cb29-27" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">cex =</span> .<span class="dv">5</span></span>
<span id="cb29-28"><a href="moving-beyond-linearity.html#cb29-28" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">pch =</span> <span class="st">&quot;|&quot;</span></span>
<span id="cb29-29"><a href="moving-beyond-linearity.html#cb29-29" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb29-30"><a href="moving-beyond-linearity.html#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="moving-beyond-linearity.html#cb29-31" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> age.grid,<span class="at">y =</span> pfit</span>
<span id="cb29-32"><a href="moving-beyond-linearity.html#cb29-32" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb29-33"><a href="moving-beyond-linearity.html#cb29-33" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col=</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb29-34"><a href="moving-beyond-linearity.html#cb29-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-35"><a href="moving-beyond-linearity.html#cb29-35" aria-hidden="true" tabindex="-1"></a><span class="fu">matlines</span>(<span class="at">x =</span> age.grid</span>
<span id="cb29-36"><a href="moving-beyond-linearity.html#cb29-36" aria-hidden="true" tabindex="-1"></a>         ,<span class="at">y =</span> se.bands</span>
<span id="cb29-37"><a href="moving-beyond-linearity.html#cb29-37" aria-hidden="true" tabindex="-1"></a>         ,<span class="at">lwd =</span> <span class="dv">1</span></span>
<span id="cb29-38"><a href="moving-beyond-linearity.html#cb29-38" aria-hidden="true" tabindex="-1"></a>         ,<span class="at">col =</span> <span class="st">&quot;blue&quot;</span></span>
<span id="cb29-39"><a href="moving-beyond-linearity.html#cb29-39" aria-hidden="true" tabindex="-1"></a>         ,<span class="at">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-27-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see on the right hand panel that the all the observations that have a wage above 250 is in the top and all those below hare in the bottom of the visualization. Although at the tail, we arenât able to conclude much, as confidence interval is really high, hence it can both be high and low earners.</p>
<p><code>jitter()</code> is merely an approach to avoid observations to overlap each other.</p>
</div>
<div id="step-function" class="section level4" number="2.3.1.3">
<h4><span class="header-section-number">2.3.1.3</span> Step function</h4>
<p>To fit the step function we must do:</p>
<ol style="list-style-type: decimal">
<li>Define the cuts, <code>cut()</code> is able to automatically pick cutpoints. One could also use <code>break()</code> to define where the cuts should be.</li>
<li>Train the model. Notice, that <code>lm()</code> will automatically create dummy variables for the ranges.</li>
</ol>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="moving-beyond-linearity.html#cb30-1" aria-hidden="true" tabindex="-1"></a>{<span class="fu">table</span>(<span class="fu">cut</span>(df<span class="sc">$</span>age,<span class="dv">4</span>)) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb30-2"><a href="moving-beyond-linearity.html#cb30-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">cut</span>(df<span class="sc">$</span>age,<span class="dv">4</span>)</span>
<span id="cb30-3"><a href="moving-beyond-linearity.html#cb30-3" aria-hidden="true" tabindex="-1"></a>          ,<span class="at">data =</span> df)</span>
<span id="cb30-4"><a href="moving-beyond-linearity.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit)) <span class="sc">%&gt;%</span> <span class="fu">print</span>()}</span></code></pre></div>
<pre><code>## 
## (17.9,33.5]   (33.5,49]   (49,64.5] (64.5,80.1] 
##         750        1399         779          72 
##                            Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)               94.158392   1.476069 63.789970 0.000000e+00
## cut(df$age, 4)(33.5,49]   24.053491   1.829431 13.148074 1.982315e-38
## cut(df$age, 4)(49,64.5]   23.664559   2.067958 11.443444 1.040750e-29
## cut(df$age, 4)(64.5,80.1]  7.640592   4.987424  1.531972 1.256350e-01</code></pre>
<p>We see that the p value of the cuts are significant, not that we can use the p-values for much.</p>
<p>Notice, that the first range is the base level, thus it is also left out. We can then use the intercept as the average wage for all in the range of up to 33.5 years.</p>
<p>Hence for a 40 year old person, the model will say that he has an wage of 94 + 24 = 118</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="moving-beyond-linearity.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
</div>
</div>
<div id="splines" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Splines</h3>
<p>The different approaches to splines are presented in the following.</p>
<div id="basis-function-splines" class="section level4" number="2.3.2.1">
<h4><span class="header-section-number">2.3.2.1</span> Basis Function Splines</h4>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="moving-beyond-linearity.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb33-2"><a href="moving-beyond-linearity.html#cb33-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Wage</span>
<span id="cb33-3"><a href="moving-beyond-linearity.html#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb33-4"><a href="moving-beyond-linearity.html#cb33-4" aria-hidden="true" tabindex="-1"></a>agelims <span class="ot">&lt;-</span> <span class="fu">range</span>(df<span class="sc">$</span>age) <span class="co">#The min and max</span></span>
<span id="cb33-5"><a href="moving-beyond-linearity.html#cb33-5" aria-hidden="true" tabindex="-1"></a>age.grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> agelims[<span class="dv">1</span>],<span class="at">to =</span> agelims[<span class="dv">2</span>]) <span class="co">#Creating a counter within the range</span></span></code></pre></div>
<p>The splines library contain what we need. We introduce the following functions:</p>
<ul>
<li><code>bs()</code>: Basis functions for splines. Generates entire matrix of basis functions for splines with the specified set of knots.</li>
<li><code>ns()</code>: Natural splines.</li>
<li><code>smooth.spline()</code>: Used when fitting smoothing splines.</li>
<li><code>loess()</code>: When fitting local regression.</li>
</ul>
<p>Note, that by default the splines will be choosen to be 3, this can also be found in the function documentation.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="moving-beyond-linearity.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb34-2"><a href="moving-beyond-linearity.html#cb34-2" aria-hidden="true" tabindex="-1"></a>fit.bs <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">bs</span>(age,<span class="at">knots =</span> <span class="fu">c</span>(<span class="dv">25</span>,<span class="dv">40</span>,<span class="dv">60</span>)) <span class="co">#we just chose the knots randomly</span></span>
<span id="cb34-3"><a href="moving-beyond-linearity.html#cb34-3" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">data =</span> df)</span>
<span id="cb34-4"><a href="moving-beyond-linearity.html#cb34-4" aria-hidden="true" tabindex="-1"></a>pred.bs <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.bs</span>
<span id="cb34-5"><a href="moving-beyond-linearity.html#cb34-5" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid)</span>
<span id="cb34-6"><a href="moving-beyond-linearity.html#cb34-6" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb34-7"><a href="moving-beyond-linearity.html#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df<span class="sc">$</span>age</span>
<span id="cb34-8"><a href="moving-beyond-linearity.html#cb34-8" aria-hidden="true" tabindex="-1"></a>     ,df<span class="sc">$</span>wage</span>
<span id="cb34-9"><a href="moving-beyond-linearity.html#cb34-9" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb34-10"><a href="moving-beyond-linearity.html#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="moving-beyond-linearity.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(age.grid</span>
<span id="cb34-12"><a href="moving-beyond-linearity.html#cb34-12" aria-hidden="true" tabindex="-1"></a>      ,pred.bs<span class="sc">$</span>fit</span>
<span id="cb34-13"><a href="moving-beyond-linearity.html#cb34-13" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb34-14"><a href="moving-beyond-linearity.html#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="moving-beyond-linearity.html#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(age.grid</span>
<span id="cb34-16"><a href="moving-beyond-linearity.html#cb34-16" aria-hidden="true" tabindex="-1"></a>      ,pred.bs<span class="sc">$</span>fit<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>pred.bs<span class="sc">$</span>se</span>
<span id="cb34-17"><a href="moving-beyond-linearity.html#cb34-17" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb34-18"><a href="moving-beyond-linearity.html#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="moving-beyond-linearity.html#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(age.grid</span>
<span id="cb34-20"><a href="moving-beyond-linearity.html#cb34-20" aria-hidden="true" tabindex="-1"></a>      ,pred.bs<span class="sc">$</span>fit<span class="dv">-2</span><span class="sc">*</span>pred.bs<span class="sc">$</span>se</span>
<span id="cb34-21"><a href="moving-beyond-linearity.html#cb34-21" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb34-22"><a href="moving-beyond-linearity.html#cb34-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-23"><a href="moving-beyond-linearity.html#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Splines - Basis Functions&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-31-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the splines have been fitted to the data and notice that the tails have wider confidence intervals.</p>
<p>We can get the amount of degrees of freedom by calling the <code>dim()</code>function.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="moving-beyond-linearity.html#cb35-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb35-2"><a href="moving-beyond-linearity.html#cb35-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Specifying the knots</span></span>
<span id="cb35-3"><a href="moving-beyond-linearity.html#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dim</span>(<span class="fu">bs</span>(age,<span class="at">knots =</span> <span class="fu">c</span>(<span class="dv">25</span>,<span class="dv">40</span>,<span class="dv">60</span>))) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb35-4"><a href="moving-beyond-linearity.html#cb35-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-5"><a href="moving-beyond-linearity.html#cb35-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#df can be specified instead of knots</span></span>
<span id="cb35-6"><a href="moving-beyond-linearity.html#cb35-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dim</span>(<span class="fu">bs</span>(age,<span class="at">df =</span> <span class="dv">6</span>)) <span class="sc">%&gt;%</span> <span class="fu">print</span>()</span>
<span id="cb35-7"><a href="moving-beyond-linearity.html#cb35-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] 3000    6
## [1] 3000    6</code></pre>
<p>We see that the two alternatives produce the same results.</p>
<p>Notice, that there are packages that will optimize the amount of knots.</p>
<p>We can assess where the <code>bs()</code> placed the knots, by calling the <code>attr()</code>.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="moving-beyond-linearity.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(<span class="fu">bs</span>(age,<span class="at">df=</span><span class="dv">6</span>),<span class="st">&quot;knots&quot;</span>)</span></code></pre></div>
<pre><code>##   25%   50%   75% 
## 33.75 42.00 51.00</code></pre>
<p>In this case, R chose the 25%, 50% and 75% quantiles.</p>
</div>
<div id="natural-splines" class="section level4" number="2.3.2.2">
<h4><span class="header-section-number">2.3.2.2</span> Natural Splines</h4>
<p>It similar to bs(), but it has an additional condition. I did not really get it.</p>
<p>The fitting procedure is the same, but now we just use <code>ns()</code> instead of <code>bs()</code>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="moving-beyond-linearity.html#cb39-1" aria-hidden="true" tabindex="-1"></a>fit.ns <span class="ot">=</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">ns</span>(age</span>
<span id="cb39-2"><a href="moving-beyond-linearity.html#cb39-2" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">df =</span> <span class="dv">4</span> <span class="co">#Note, as with bs() we could have specified the knots instead of.</span></span>
<span id="cb39-3"><a href="moving-beyond-linearity.html#cb39-3" aria-hidden="true" tabindex="-1"></a>                      )</span>
<span id="cb39-4"><a href="moving-beyond-linearity.html#cb39-4" aria-hidden="true" tabindex="-1"></a>            ,<span class="at">data =</span> df)</span>
<span id="cb39-5"><a href="moving-beyond-linearity.html#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="moving-beyond-linearity.html#cb39-6" aria-hidden="true" tabindex="-1"></a>pred.ns <span class="ot">=</span> <span class="fu">predict</span>(fit.ns</span>
<span id="cb39-7"><a href="moving-beyond-linearity.html#cb39-7" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age=</span>age.grid)</span>
<span id="cb39-8"><a href="moving-beyond-linearity.html#cb39-8" aria-hidden="true" tabindex="-1"></a>                  ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb39-9"><a href="moving-beyond-linearity.html#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="moving-beyond-linearity.html#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Copy of old plot</span></span>
<span id="cb39-11"><a href="moving-beyond-linearity.html#cb39-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(df<span class="sc">$</span>age</span>
<span id="cb39-12"><a href="moving-beyond-linearity.html#cb39-12" aria-hidden="true" tabindex="-1"></a>       ,df<span class="sc">$</span>wage</span>
<span id="cb39-13"><a href="moving-beyond-linearity.html#cb39-13" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb39-14"><a href="moving-beyond-linearity.html#cb39-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb39-15"><a href="moving-beyond-linearity.html#cb39-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(age.grid</span>
<span id="cb39-16"><a href="moving-beyond-linearity.html#cb39-16" aria-hidden="true" tabindex="-1"></a>        ,pred.bs<span class="sc">$</span>fit</span>
<span id="cb39-17"><a href="moving-beyond-linearity.html#cb39-17" aria-hidden="true" tabindex="-1"></a>        ,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb39-18"><a href="moving-beyond-linearity.html#cb39-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb39-19"><a href="moving-beyond-linearity.html#cb39-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(age.grid</span>
<span id="cb39-20"><a href="moving-beyond-linearity.html#cb39-20" aria-hidden="true" tabindex="-1"></a>        ,pred.bs<span class="sc">$</span>fit<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>pred.bs<span class="sc">$</span>se</span>
<span id="cb39-21"><a href="moving-beyond-linearity.html#cb39-21" aria-hidden="true" tabindex="-1"></a>        ,<span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb39-22"><a href="moving-beyond-linearity.html#cb39-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb39-23"><a href="moving-beyond-linearity.html#cb39-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(age.grid</span>
<span id="cb39-24"><a href="moving-beyond-linearity.html#cb39-24" aria-hidden="true" tabindex="-1"></a>        ,pred.bs<span class="sc">$</span>fit<span class="dv">-2</span><span class="sc">*</span>pred.bs<span class="sc">$</span>se</span>
<span id="cb39-25"><a href="moving-beyond-linearity.html#cb39-25" aria-hidden="true" tabindex="-1"></a>        ,<span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb39-26"><a href="moving-beyond-linearity.html#cb39-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-27"><a href="moving-beyond-linearity.html#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="co">#Adding natural splines</span></span>
<span id="cb39-28"><a href="moving-beyond-linearity.html#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(age.grid</span>
<span id="cb39-29"><a href="moving-beyond-linearity.html#cb39-29" aria-hidden="true" tabindex="-1"></a>      ,pred.ns<span class="sc">$</span>fit</span>
<span id="cb39-30"><a href="moving-beyond-linearity.html#cb39-30" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col =</span><span class="st">&quot;red&quot;</span></span>
<span id="cb39-31"><a href="moving-beyond-linearity.html#cb39-31" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span><span class="dv">2</span>)</span>
<span id="cb39-32"><a href="moving-beyond-linearity.html#cb39-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-33"><a href="moving-beyond-linearity.html#cb39-33" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Splines - Basis Functions + Natural Splines&quot;</span>)</span>
<span id="cb39-34"><a href="moving-beyond-linearity.html#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="fu">c</span>(<span class="st">&quot;Basis&quot;</span>,<span class="st">&quot;Natural&quot;</span>),<span class="at">lty =</span> <span class="dv">1</span>,<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;Black&quot;</span>,<span class="st">&quot;Red&quot;</span>),<span class="at">cex =</span> <span class="fl">0.6</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-34-1.png" width="720" style="display: block; margin: auto;" /></p>
</div>
<div id="smooth-splines" class="section level4" number="2.3.2.3">
<h4><span class="header-section-number">2.3.2.3</span> Smooth Splines</h4>
<p>As we discovered in the first part of the chapter, it sets a knot at each observation, and then we will penalize the function with a lamda (<span class="math inline">\(\lambda\)</span>), to avoid overfitting.</p>
<p>The code show the procedure.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="moving-beyond-linearity.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Hardcoding degrees of freedom</span></span>
<span id="cb40-2"><a href="moving-beyond-linearity.html#cb40-2" aria-hidden="true" tabindex="-1"></a>fit.ss <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">y =</span> df<span class="sc">$</span>wage</span>
<span id="cb40-3"><a href="moving-beyond-linearity.html#cb40-3" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">df =</span> <span class="dv">16</span>) <span class="co">#Remember that we must impose constraints</span></span>
<span id="cb40-4"><a href="moving-beyond-linearity.html#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="moving-beyond-linearity.html#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Choosing smoothing param with CV</span></span>
<span id="cb40-6"><a href="moving-beyond-linearity.html#cb40-6" aria-hidden="true" tabindex="-1"></a>fit.ss2 <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span> (df<span class="sc">$</span>age</span>
<span id="cb40-7"><a href="moving-beyond-linearity.html#cb40-7" aria-hidden="true" tabindex="-1"></a>                          ,df<span class="sc">$</span>wage</span>
<span id="cb40-8"><a href="moving-beyond-linearity.html#cb40-8" aria-hidden="true" tabindex="-1"></a>                          ,<span class="at">cv =</span> <span class="cn">TRUE</span>) <span class="co">#we choose cv instead of fixed amount of df</span></span>
<span id="cb40-9"><a href="moving-beyond-linearity.html#cb40-9" aria-hidden="true" tabindex="-1"></a>fit.ss2<span class="sc">$</span>df</span></code></pre></div>
<pre><code>## [1] 6.794596</code></pre>
<p>We get sparsity hence we have degrees of freedom of 6.8. That is due to the tuning parameter which was found by the cross validation proces. We can find the specific lambda value with the following:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="moving-beyond-linearity.html#cb42-1" aria-hidden="true" tabindex="-1"></a>fit.ss2<span class="sc">$</span>lambda</span></code></pre></div>
<pre><code>## [1] 0.02792303</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="moving-beyond-linearity.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(age,wage</span>
<span id="cb44-2"><a href="moving-beyond-linearity.html#cb44-2" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlim =</span> agelims</span>
<span id="cb44-3"><a href="moving-beyond-linearity.html#cb44-3" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">cex =</span> .<span class="dv">5</span></span>
<span id="cb44-4"><a href="moving-beyond-linearity.html#cb44-4" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb44-5"><a href="moving-beyond-linearity.html#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Smoothing Spline&quot;</span>)</span>
<span id="cb44-6"><a href="moving-beyond-linearity.html#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="moving-beyond-linearity.html#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(fit.ss,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb44-8"><a href="moving-beyond-linearity.html#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(fit.ss2,<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lwd =</span><span class="dv">2</span>)</span>
<span id="cb44-9"><a href="moving-beyond-linearity.html#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="moving-beyond-linearity.html#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;16 DF&quot;</span>,<span class="st">&quot;6.8 DF&quot;</span>)</span>
<span id="cb44-11"><a href="moving-beyond-linearity.html#cb44-11" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>)</span>
<span id="cb44-12"><a href="moving-beyond-linearity.html#cb44-12" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">lty =</span> <span class="dv">1</span></span>
<span id="cb44-13"><a href="moving-beyond-linearity.html#cb44-13" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb44-14"><a href="moving-beyond-linearity.html#cb44-14" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">cex =</span> .<span class="dv">8</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-37-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>As expected, we see that the more complex model (highest amount of df) is the more flexible model.</p>
<p>Note: tuning parameter = <span class="math inline">\(\lambda\)</span>, where the CV seeks to choose the parameter that leads to the lowest error and return the df that leads to this level.</p>
</div>
<div id="local-regression-1" class="section level4" number="2.3.2.4">
<h4><span class="header-section-number">2.3.2.4</span> Local Regression</h4>
<p>Recall that local regression makes a linear regression for the observations that are close to the observation under evaluation (<span class="math inline">\(x_0\)</span>).</p>
<p>Thus we have to specify the span, the larger the span the smoother the fit, as we will include more observations.</p>
<p>NB: <code>locfit</code> library can also be used for fitting local regress</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="moving-beyond-linearity.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">y =</span> df<span class="sc">$</span>wage</span>
<span id="cb45-2"><a href="moving-beyond-linearity.html#cb45-2" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">xlim =</span> agelims</span>
<span id="cb45-3"><a href="moving-beyond-linearity.html#cb45-3" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">cex =</span> .<span class="dv">5</span></span>
<span id="cb45-4"><a href="moving-beyond-linearity.html#cb45-4" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb45-5"><a href="moving-beyond-linearity.html#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span> (<span class="st">&quot;Local Regression&quot;</span>)</span>
<span id="cb45-6"><a href="moving-beyond-linearity.html#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="moving-beyond-linearity.html#cb45-7" aria-hidden="true" tabindex="-1"></a>fit.lr <span class="ot">&lt;-</span> <span class="fu">loess</span>(wage <span class="sc">~</span> age</span>
<span id="cb45-8"><a href="moving-beyond-linearity.html#cb45-8" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">span =</span> .<span class="dv">2</span> <span class="co">#Degree of smoothing / neighborhood to be included</span></span>
<span id="cb45-9"><a href="moving-beyond-linearity.html#cb45-9" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">data =</span> df)</span>
<span id="cb45-10"><a href="moving-beyond-linearity.html#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="moving-beyond-linearity.html#cb45-11" aria-hidden="true" tabindex="-1"></a>fit.lr2 <span class="ot">&lt;-</span> <span class="fu">loess</span>(wage <span class="sc">~</span> age</span>
<span id="cb45-12"><a href="moving-beyond-linearity.html#cb45-12" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">span =</span> .<span class="dv">5</span> <span class="co">#Degree of smoothing / neighborhood to be included</span></span>
<span id="cb45-13"><a href="moving-beyond-linearity.html#cb45-13" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">data =</span> df)</span>
<span id="cb45-14"><a href="moving-beyond-linearity.html#cb45-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-15"><a href="moving-beyond-linearity.html#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> age.grid,<span class="at">y =</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.lr,<span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">age=</span>age.grid))</span>
<span id="cb45-16"><a href="moving-beyond-linearity.html#cb45-16" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb45-17"><a href="moving-beyond-linearity.html#cb45-17" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-18"><a href="moving-beyond-linearity.html#cb45-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-19"><a href="moving-beyond-linearity.html#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> age.grid,<span class="at">y =</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.lr2,<span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">age=</span>age.grid))</span>
<span id="cb45-20"><a href="moving-beyond-linearity.html#cb45-20" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col =</span><span class="st">&quot; blue&quot;</span></span>
<span id="cb45-21"><a href="moving-beyond-linearity.html#cb45-21" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb45-22"><a href="moving-beyond-linearity.html#cb45-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-23"><a href="moving-beyond-linearity.html#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span></span>
<span id="cb45-24"><a href="moving-beyond-linearity.html#cb45-24" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Span = 0.2&quot;</span>,<span class="st">&quot;Span = 0.5&quot;</span>)</span>
<span id="cb45-25"><a href="moving-beyond-linearity.html#cb45-25" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>)</span>
<span id="cb45-26"><a href="moving-beyond-linearity.html#cb45-26" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">lty =</span> <span class="dv">1</span></span>
<span id="cb45-27"><a href="moving-beyond-linearity.html#cb45-27" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb45-28"><a href="moving-beyond-linearity.html#cb45-28" aria-hidden="true" tabindex="-1"></a>       ,<span class="at">cex =</span> .<span class="dv">8</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-38-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>From the plot we also see that the model with the largest span has the smoothest fit.</p>
</div>
</div>
<div id="gams" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> GAMs</h3>
<p>We want to predict wage, where year, age and education (as categorical) as predictors.</p>
<div id="with-only-natural-splines" class="section level4" number="2.3.3.1">
<h4><span class="header-section-number">2.3.3.1</span> With only natural splines</h4>
<p>According to the <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>, 294, this is just a bunch of linear functions, hence we can merely apply <code>lm()</code>, see the following.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="moving-beyond-linearity.html#cb46-1" aria-hidden="true" tabindex="-1"></a>gam.m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">ns</span>(year,<span class="at">df =</span> <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">ns</span>(age,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education <span class="co">#NOTICE, that we just use lm()</span></span>
<span id="cb46-2"><a href="moving-beyond-linearity.html#cb46-2" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">data =</span> df)</span>
<span id="cb46-3"><a href="moving-beyond-linearity.html#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wage ~ ns(year, df = 4) + ns(age, df = 5) + education, 
##     data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -120.513  -19.608   -3.583   14.112  214.535 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                   46.949      4.704   9.980  &lt; 2e-16 ***
## ns(year, df = 4)1              8.625      3.466   2.488  0.01289 *  
## ns(year, df = 4)2              3.762      2.959   1.271  0.20369    
## ns(year, df = 4)3              8.127      4.211   1.930  0.05375 .  
## ns(year, df = 4)4              6.806      2.397   2.840  0.00455 ** 
## ns(age, df = 5)1              45.170      4.193  10.771  &lt; 2e-16 ***
## ns(age, df = 5)2              38.450      5.076   7.575 4.78e-14 ***
## ns(age, df = 5)3              34.239      4.383   7.813 7.69e-15 ***
## ns(age, df = 5)4              48.678     10.572   4.605 4.31e-06 ***
## ns(age, df = 5)5               6.557      8.367   0.784  0.43328    
## education2. HS Grad           10.983      2.430   4.520 6.43e-06 ***
## education3. Some College      23.473      2.562   9.163  &lt; 2e-16 ***
## education4. College Grad      38.314      2.547  15.042  &lt; 2e-16 ***
## education5. Advanced Degree   62.554      2.761  22.654  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 35.16 on 2986 degrees of freedom
## Multiple R-squared:  0.293,  Adjusted R-squared:  0.2899 
## F-statistic:  95.2 on 13 and 2986 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>From the summary we see the variables that have been created and also the factor levels for education.</p>
<p>Again, we donât have to interprete the coefficients, we just need to look at the shape.</p>
</div>
<div id="with-different-splines" class="section level4" number="2.3.3.2">
<h4><span class="header-section-number">2.3.3.2</span> With different splines</h4>
<p>Now we have to apply the package <code>gam</code>.</p>
<p><strong>This is the best approach.</strong></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="moving-beyond-linearity.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gam)</span></code></pre></div>
<p>We can also construct a GAM model, that contains smoothing splines, that is done by calling <code>s()</code>. Where year and age will be included with up to 4 and 5 degrees of freedom.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="moving-beyond-linearity.html#cb49-1" aria-hidden="true" tabindex="-1"></a>gam.m3 <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">s</span>(year,<span class="at">df =</span> <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(age,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education</span>
<span id="cb49-2"><a href="moving-beyond-linearity.html#cb49-2" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">data =</span> df)</span></code></pre></div>
<p>Remember, that GAM fits each variable while holding all other variables fixed. The actual fitting procedure is called backfitting, and fits variables by repeatedly updating the fit for each predictor <span class="citation">(<a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al. 2013, 284â85</a>)</span>. Hence, we create plots to interprete how.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="moving-beyond-linearity.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb50-2"><a href="moving-beyond-linearity.html#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.m3 <span class="co">#Note, automatically identifies the GAM object, hence plots for each variable</span></span>
<span id="cb50-3"><a href="moving-beyond-linearity.html#cb50-3" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">se =</span> <span class="cn">TRUE</span></span>
<span id="cb50-4"><a href="moving-beyond-linearity.html#cb50-4" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span><span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="Machine-Learning-Notes_files/figure-html/GAMPlotLab7.8.3-1.png" alt="GAM plot and intepretation" width="720" />
<p class="caption">
(#fig:GAMPlotLab7.8.3)GAM plot and intepretation
</p>
</div>
<p>Interpreting the plot: Recall that the plots assumes that we hold the other variables fixed, hence we see the following:</p>
<ul>
<li>Left: We see that holding education and age fixed, the wage tends to increase over the years, that is quite natural, e.g., because of inflation.</li>
<li>Center: Holding year and education fixed, we see that the wage tends to be highest in the middle region around 40-45 years of age. That is also quite intuitive that the wage first increase and then decreasing as the person gets closer to the retirement age.</li>
<li>Right: Holding year and age fixed, we see that the higher education you have, the higher will your wage be.</li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="moving-beyond-linearity.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb51-2"><a href="moving-beyond-linearity.html#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.Gam</span>(gam.m1</span>
<span id="cb51-3"><a href="moving-beyond-linearity.html#cb51-3" aria-hidden="true" tabindex="-1"></a>         ,<span class="at">se =</span> <span class="cn">TRUE</span></span>
<span id="cb51-4"><a href="moving-beyond-linearity.html#cb51-4" aria-hidden="true" tabindex="-1"></a>         ,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-42"></span>
<img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-42-1.png" alt="GAM of natural splines" width="720" />
<p class="caption">
Figure 2.3: GAM of natural splines
</p>
</div>
<p>Notice, that this plot looks very similar to @(fig:GAMPlotLab7.8.3).</p>
<p>This command could naturally also be used for the other GAM object, it is just that <code>plot()</code> does not automatically identify, that it is in fact intended to be interpretet as a GAM.</p>
</div>
<div id="but-what-variables-to-include" class="section level4" number="2.3.3.3">
<h4><span class="header-section-number">2.3.3.3</span> But what variables to include?</h4>
<p>It looks as is year is rather linear. To make this assessment, we can apply an ANOVA test of the different combinations. Hence:</p>
<p><em>Note, the first model is nested in the second model (has the same variables), hence we can use ANOVA</em></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="moving-beyond-linearity.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Excluding year</span></span>
<span id="cb52-2"><a href="moving-beyond-linearity.html#cb52-2" aria-hidden="true" tabindex="-1"></a>gam.m1 <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">s</span>(age,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education</span>
<span id="cb52-3"><a href="moving-beyond-linearity.html#cb52-3" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">data =</span> df)</span>
<span id="cb52-4"><a href="moving-beyond-linearity.html#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="moving-beyond-linearity.html#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Including year, but as a linear</span></span>
<span id="cb52-6"><a href="moving-beyond-linearity.html#cb52-6" aria-hidden="true" tabindex="-1"></a>gam.m2 <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> year <span class="sc">+</span> <span class="fu">s</span>(age,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education</span>
<span id="cb52-7"><a href="moving-beyond-linearity.html#cb52-7" aria-hidden="true" tabindex="-1"></a>             ,<span class="at">data =</span> df)</span>
<span id="cb52-8"><a href="moving-beyond-linearity.html#cb52-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-9"><a href="moving-beyond-linearity.html#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(gam.m1,gam.m2,gam.m3,<span class="at">test =</span> <span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">Resid. Df</th>
<th align="right">Resid. Dev</th>
<th align="right">Df</th>
<th align="right">Deviance</th>
<th align="right">F</th>
<th align="right">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2990</td>
<td align="right">3711731</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">2989</td>
<td align="right">3693842</td>
<td align="right">1.000000</td>
<td align="right">17889.243</td>
<td align="right">14.477130</td>
<td align="right">0.0001447</td>
</tr>
<tr class="odd">
<td align="right">2986</td>
<td align="right">3689770</td>
<td align="right">2.999989</td>
<td align="right">4071.134</td>
<td align="right">1.098212</td>
<td align="right">0.3485661</td>
</tr>
</tbody>
</table>
</div>
<p>We see that performance is significantly better going from model 1 to model 2, but on a five percent level, we are able to say, that we donât gain anything with the third model, which is most complex model.</p>
<p>Thus, the linear constellation of year, with polynomials on age + education as factors, appear to be the best performing model.</p>
<p>With this in mind, it is interesting to assess the summary of the complex model:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="moving-beyond-linearity.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.m3)</span></code></pre></div>
<pre><code>## 
## Call: gam(formula = wage ~ s(year, df = 4) + s(age, df = 5) + education, 
##     data = df)
## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -119.43  -19.70   -3.33   14.17  213.48 
## 
## (Dispersion Parameter for gaussian family taken to be 1235.69)
## 
##     Null Deviance: 5222086 on 2999 degrees of freedom
## Residual Deviance: 3689770 on 2986 degrees of freedom
## AIC: 29887.75 
## 
## Number of Local Scoring Iterations: NA 
## 
## Anova for Parametric Effects
##                   Df  Sum Sq Mean Sq F value      Pr(&gt;F)    
## s(year, df = 4)    1   27162   27162  21.981 0.000002877 ***
## s(age, df = 5)     1  195338  195338 158.081   &lt; 2.2e-16 ***
## education          4 1069726  267432 216.423   &lt; 2.2e-16 ***
## Residuals       2986 3689770    1236                        
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Anova for Nonparametric Effects
##                 Npar Df Npar F  Pr(F)    
## (Intercept)                              
## s(year, df = 4)       3  1.086 0.3537    
## s(age, df = 5)        4 32.380 &lt;2e-16 ***
## education                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Looking at the section: âAnova for Nonparametric Effects,â we see that the smoothing spline on year, is not significant, hence it supports the conclusion from above, that we are better off, including the year as a linear variable.</p>
<p>Now we can make predictions.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="moving-beyond-linearity.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Predictions with linear year, non linear age and factors of education</span></span>
<span id="cb55-2"><a href="moving-beyond-linearity.html#cb55-2" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span><span class="fu">predict</span>(gam.m2</span>
<span id="cb55-3"><a href="moving-beyond-linearity.html#cb55-3" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">newdata =</span> df)</span></code></pre></div>
</div>
<div id="gam-with-local-regression" class="section level4" number="2.3.3.4">
<h4><span class="header-section-number">2.3.3.4</span> GAM with local regression</h4>
<p>We are also able to make GAM on other building blocks, for instance local regression, that will be shown in the following</p>
<p>For some reason the following cant be run.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="moving-beyond-linearity.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">#GAM with local regression</span></span>
<span id="cb56-2"><a href="moving-beyond-linearity.html#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="moving-beyond-linearity.html#cb56-3" aria-hidden="true" tabindex="-1"></a>gam.lo <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">s</span>(df<span class="sc">$</span>year,<span class="at">df =</span> <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">lo</span>(df<span class="sc">$</span>age,<span class="at">span =</span> <span class="fl">0.7</span>) <span class="sc">+</span> education</span>
<span id="cb56-4"><a href="moving-beyond-linearity.html#cb56-4" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">data =</span> df)</span>
<span id="cb56-5"><a href="moving-beyond-linearity.html#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="moving-beyond-linearity.html#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot.Gam(gam.lo #For some reason it cant be plotted</span></span>
<span id="cb56-7"><a href="moving-beyond-linearity.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="co">#          ,se = TRUE</span></span>
<span id="cb56-8"><a href="moving-beyond-linearity.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co">#          ,col = &quot;green&quot;)</span></span></code></pre></div>
<p>Making interactions in the local regression:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="moving-beyond-linearity.html#cb57-1" aria-hidden="true" tabindex="-1"></a>gam.lo.i <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">lo</span>(year,age,<span class="at">span =</span> <span class="fl">0.5</span>) <span class="sc">+</span> education</span>
<span id="cb57-2"><a href="moving-beyond-linearity.html#cb57-2" aria-hidden="true" tabindex="-1"></a>                ,<span class="at">data =</span> df)</span>
<span id="cb57-3"><a href="moving-beyond-linearity.html#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="moving-beyond-linearity.html#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(akima) </span>
<span id="cb57-5"><a href="moving-beyond-linearity.html#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.lo.i)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-47-1.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-47-2.png" width="720" style="display: block; margin: auto;" /></p>
</div>
<div id="logistic-regression" class="section level4" number="2.3.3.5">
<h4><span class="header-section-number">2.3.3.5</span> Logistic Regression</h4>
<p>Plotting logistic regression GAM, here we can apply <code>I()</code> as previous used, to make the expression on the fly.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="moving-beyond-linearity.html#cb58-1" aria-hidden="true" tabindex="-1"></a>gam.lr <span class="ot">&lt;-</span> <span class="fu">gam</span>(<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">~</span> year <span class="sc">+</span> <span class="fu">s</span>(age,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education</span>
<span id="cb58-2"><a href="moving-beyond-linearity.html#cb58-2" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">family =</span> binomial</span>
<span id="cb58-3"><a href="moving-beyond-linearity.html#cb58-3" aria-hidden="true" tabindex="-1"></a>              ,<span class="at">data =</span> df)</span>
<span id="cb58-4"><a href="moving-beyond-linearity.html#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="moving-beyond-linearity.html#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb58-6"><a href="moving-beyond-linearity.html#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.lr,<span class="at">se=</span>T,<span class="at">col =</span><span class="st">&quot; green &quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-48-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>One could interprete the plot and assess each window to see how the variable influence the decision wether the observation is above or below. Remember that the outcome can be seen as probabilities, these can also be plotted to be shown the spread:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="moving-beyond-linearity.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb59-2"><a href="moving-beyond-linearity.html#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.lr<span class="sc">$</span>fitted.values)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-49-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>From th plot, we see that there is a tendency that the lower the education the lower the wage, the following table show how the high earners are distributed.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="moving-beyond-linearity.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(education,<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>))</span></code></pre></div>
<pre><code>##                     
## education            FALSE TRUE
##   1. &lt; HS Grad         268    0
##   2. HS Grad           966    5
##   3. Some College      643    7
##   4. College Grad      663   22
##   5. Advanced Degree   381   45</code></pre>
<p>We see that there are no people with less than high school degree that earns more than 250.</p>
<p>To get more sensible result, we can remove the observations with a low degree, this will also show a more sensible result for the other degrees, see the following.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="moving-beyond-linearity.html#cb62-1" aria-hidden="true" tabindex="-1"></a>gam.lr.s <span class="ot">=</span> <span class="fu">gam</span>(<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">~</span> year <span class="sc">+</span> <span class="fu">s</span>(age,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education</span>
<span id="cb62-2"><a href="moving-beyond-linearity.html#cb62-2" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">family =</span> binomial</span>
<span id="cb62-3"><a href="moving-beyond-linearity.html#cb62-3" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">data =</span> df</span>
<span id="cb62-4"><a href="moving-beyond-linearity.html#cb62-4" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">subset =</span> (education <span class="sc">!=</span> <span class="st">&quot;1. &lt; HS Grad&quot;</span>)) <span class="co">#removing people in the lowest group of education.</span></span>
<span id="cb62-5"><a href="moving-beyond-linearity.html#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb62-6"><a href="moving-beyond-linearity.html#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.lr.s</span>
<span id="cb62-7"><a href="moving-beyond-linearity.html#cb62-7" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">se =</span> <span class="cn">TRUE</span> <span class="co">#Standard errors</span></span>
<span id="cb62-8"><a href="moving-beyond-linearity.html#cb62-8" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span><span class="st">&quot; green &quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-51-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Do we need a nonlinear term for year?
Use anova for comparing the previous model with a model that includes a smooth spline of year with df=4</p>
<p>We can do an ANOVA, but please notice, we use Chi Square now.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="moving-beyond-linearity.html#cb63-1" aria-hidden="true" tabindex="-1"></a>gam.y.s <span class="ot">=</span> <span class="fu">gam</span>(<span class="fu">I</span>(wage<span class="sc">&gt;</span><span class="dv">250</span>) <span class="sc">~</span> <span class="fu">s</span>(year, <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(age,<span class="dv">5</span>) <span class="sc">+</span> education,<span class="at">family=</span>binomial,<span class="at">data =</span> df,<span class="at">subset=</span>(education<span class="sc">!=</span><span class="st">&quot;1. &lt; HS Grad&quot;</span>)) </span>
<span id="cb63-2"><a href="moving-beyond-linearity.html#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(gam.lr.s,gam.y.s, <span class="at">test=</span><span class="st">&quot;Chisq&quot;</span>) <span class="co">#  Chi-square test as Dep variable is categorical</span></span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">Resid. Df</th>
<th align="right">Resid. Dev</th>
<th align="right">Df</th>
<th align="right">Deviance</th>
<th align="right">Pr(&gt;Chi)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2722</td>
<td align="right">602.4588</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">2719</td>
<td align="right">601.5718</td>
<td align="right">2.999982</td>
<td align="right">0.8869514</td>
<td align="right">0.8285731</td>
</tr>
</tbody>
</table>
</div>
<p>We do not need a non-linear term for year.</p>
</div>
</div>
</div>
<div id="exercises" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Exercises</h2>
<div id="exercise-6" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Exercise 6</h3>
<p><strong>Purpose, to practice polynomial regression and step functions</strong></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="moving-beyond-linearity.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb64-2"><a href="moving-beyond-linearity.html#cb64-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Wage</span></code></pre></div>
<div id="a-polynomial-regression" class="section level4" number="2.4.1.1">
<h4><span class="header-section-number">2.4.1.1</span> 6.a Polynomial Regression</h4>
<p>We use orthogonal polynomials in the modeling process as we know that these are slightly better than raw polynomials due to the fact that this tend to avoid collinearity.</p>
<p>Training the model</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="moving-beyond-linearity.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot) <span class="co">#For the cv.glm() function</span></span>
<span id="cb65-2"><a href="moving-beyond-linearity.html#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb65-3"><a href="moving-beyond-linearity.html#cb65-3" aria-hidden="true" tabindex="-1"></a>cv.error <span class="ot">=</span> <span class="fu">rep</span> (<span class="dv">0</span>,<span class="dv">10</span>)</span>
<span id="cb65-4"><a href="moving-beyond-linearity.html#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">1</span>,<span class="at">to =</span> <span class="fu">length</span>(cv.error),<span class="at">by =</span> <span class="dv">1</span>)) {</span>
<span id="cb65-5"><a href="moving-beyond-linearity.html#cb65-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Training</span></span>
<span id="cb65-6"><a href="moving-beyond-linearity.html#cb65-6" aria-hidden="true" tabindex="-1"></a>  fit.i <span class="ot">&lt;-</span> <span class="fu">glm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,i),<span class="at">data =</span> df)  <span class="co"># notice glm here in conjunction with cv.glm function</span></span>
<span id="cb65-7"><a href="moving-beyond-linearity.html#cb65-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb65-8"><a href="moving-beyond-linearity.html#cb65-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Performing cross validation</span></span>
<span id="cb65-9"><a href="moving-beyond-linearity.html#cb65-9" aria-hidden="true" tabindex="-1"></a>  cv.error[i] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(<span class="at">data =</span> df,<span class="at">glmfit =</span> fit.i,<span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">1</span>] <span class="co">#K fold CV, delta = prediction errer i.e. MSE</span></span>
<span id="cb65-10"><a href="moving-beyond-linearity.html#cb65-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb65-11"><a href="moving-beyond-linearity.html#cb65-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-12"><a href="moving-beyond-linearity.html#cb65-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Printing the </span></span>
<span id="cb65-13"><a href="moving-beyond-linearity.html#cb65-13" aria-hidden="true" tabindex="-1"></a>cv.error <span class="co"># MSE the CV errors of the five polynomials models</span></span></code></pre></div>
<pre><code>##  [1] 1675.056 1600.832 1594.505 1594.872 1594.608 1593.053 1594.069 1596.428
##  [9] 1593.284 1595.530</code></pre>
<p>The vector above are all of the prediction errors computed in the loop.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="moving-beyond-linearity.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(cv.error)</span></code></pre></div>
<pre><code>## [1] 6</code></pre>
<p>We see that the fifth prediction appear to yield the lowest MSE, but is it significantly different than e.g.Â forth or third order polynomial?</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="moving-beyond-linearity.html#cb69-1" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">1</span>),<span class="at">data =</span> df) </span>
<span id="cb69-2"><a href="moving-beyond-linearity.html#cb69-2" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">2</span>),<span class="at">data =</span> df) </span>
<span id="cb69-3"><a href="moving-beyond-linearity.html#cb69-3" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">3</span>),<span class="at">data =</span> df) </span>
<span id="cb69-4"><a href="moving-beyond-linearity.html#cb69-4" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">4</span>),<span class="at">data =</span> df) </span>
<span id="cb69-5"><a href="moving-beyond-linearity.html#cb69-5" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">glm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age,<span class="dv">5</span>),<span class="at">data =</span> df) </span>
<span id="cb69-6"><a href="moving-beyond-linearity.html#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fit<span class="fl">.1</span>,fit<span class="fl">.2</span>,fit<span class="fl">.3</span>,fit<span class="fl">.4</span>,fit<span class="fl">.5</span>,<span class="at">test =</span> <span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">Resid. Df</th>
<th align="right">Resid. Dev</th>
<th align="right">Df</th>
<th align="right">Deviance</th>
<th align="right">F</th>
<th align="right">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2998</td>
<td align="right">5022216</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">2997</td>
<td align="right">4793430</td>
<td align="right">1</td>
<td align="right">228786.010</td>
<td align="right">143.5931074</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="right">2996</td>
<td align="right">4777674</td>
<td align="right">1</td>
<td align="right">15755.694</td>
<td align="right">9.8887559</td>
<td align="right">0.0016792</td>
</tr>
<tr class="even">
<td align="right">2995</td>
<td align="right">4771604</td>
<td align="right">1</td>
<td align="right">6070.152</td>
<td align="right">3.8098134</td>
<td align="right">0.0510462</td>
</tr>
<tr class="odd">
<td align="right">2994</td>
<td align="right">4770322</td>
<td align="right">1</td>
<td align="right">1282.563</td>
<td align="right">0.8049758</td>
<td align="right">0.3696820</td>
</tr>
</tbody>
</table>
</div>
<p><strong><em>We can only use these as the models are nested as the variables are the same</em></strong></p>
<p>Using the F test, we see that on a five percent level the 4th polynomial is not justified, but close to. This argues that we should select the third order of polynomials as that is the last where there is statistical evidence for lowering the residuals.</p>
<p>Thus we select a model with three polynomials. Plotting the errors, we also see that there does not happen much after the third polynomial. We also plotted the standard errors and thus we are able to select based on this.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="moving-beyond-linearity.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.error,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb70-2"><a href="moving-beyond-linearity.html#cb70-2" aria-hidden="true" tabindex="-1"></a>min.point <span class="ot">=</span> <span class="fu">min</span>(cv.error)</span>
<span id="cb70-3"><a href="moving-beyond-linearity.html#cb70-3" aria-hidden="true" tabindex="-1"></a>sd.points <span class="ot">=</span> <span class="fu">sd</span>(cv.error)</span>
<span id="cb70-4"><a href="moving-beyond-linearity.html#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>min.point <span class="sc">+</span> <span class="fl">0.2</span> <span class="sc">*</span> sd.points, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="st">&quot;dashed&quot;</span>) <span class="co">#0.2 is just a rule of thumb, could be anything</span></span>
<span id="cb70-5"><a href="moving-beyond-linearity.html#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>min.point <span class="sc">-</span> <span class="fl">0.2</span> <span class="sc">*</span> sd.points, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb70-6"><a href="moving-beyond-linearity.html#cb70-6" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="st">&quot;0.2-standard deviation lines&quot;</span>, <span class="at">lty=</span><span class="st">&quot;dashed&quot;</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-57-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Thus, there is even more information supporting selecting three degrees of freedom.</p>
<p><strong>Plotting the polynomial regression</strong></p>
<p>This is done with the following procedure:</p>
<ol style="list-style-type: decimal">
<li>Make a grid counting IDV (Age)</li>
<li>Make predictions</li>
<li>Make a plot with the variables</li>
<li>Fit a line onto the predictions</li>
<li>Perhaps calculate confidence levels and plot these</li>
</ol>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="moving-beyond-linearity.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Grid of X</span></span>
<span id="cb71-2"><a href="moving-beyond-linearity.html#cb71-2" aria-hidden="true" tabindex="-1"></a>age.grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fu">min</span>(df<span class="sc">$</span>age),<span class="at">to =</span> <span class="fu">max</span>(df<span class="sc">$</span>age),<span class="at">by =</span> <span class="dv">1</span>)</span>
<span id="cb71-3"><a href="moving-beyond-linearity.html#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="moving-beyond-linearity.html#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Predictions</span></span>
<span id="cb71-5"><a href="moving-beyond-linearity.html#cb71-5" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit<span class="fl">.3</span></span>
<span id="cb71-6"><a href="moving-beyond-linearity.html#cb71-6" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid) <span class="co">#Renaming age.grid to age</span></span>
<span id="cb71-7"><a href="moving-beyond-linearity.html#cb71-7" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">se.fit =</span> <span class="cn">TRUE</span>) <span class="co">#We want to produce confidence levels</span></span>
<span id="cb71-8"><a href="moving-beyond-linearity.html#cb71-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-9"><a href="moving-beyond-linearity.html#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting</span></span>
<span id="cb71-10"><a href="moving-beyond-linearity.html#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> df<span class="sc">$</span>age,<span class="at">y =</span> df<span class="sc">$</span>wage,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>,<span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb71-11"><a href="moving-beyond-linearity.html#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb71-12"><a href="moving-beyond-linearity.html#cb71-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> age.grid <span class="co">#We need to define the grid, otherwise the fit will not be alligned with the data</span></span>
<span id="cb71-13"><a href="moving-beyond-linearity.html#cb71-13" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">y =</span> preds<span class="sc">$</span>fit</span>
<span id="cb71-14"><a href="moving-beyond-linearity.html#cb71-14" aria-hidden="true" tabindex="-1"></a>      ,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb71-15"><a href="moving-beyond-linearity.html#cb71-15" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Polynomial of 3rd order&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-58-1.png" width="720" style="display: block; margin: auto;" /></p>
</div>
<div id="b-step-function" class="section level4" number="2.4.1.2">
<h4><span class="header-section-number">2.4.1.2</span> 6.b Step function</h4>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="moving-beyond-linearity.html#cb72-1" aria-hidden="true" tabindex="-1"></a>cuts <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb72-2"><a href="moving-beyond-linearity.html#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="moving-beyond-linearity.html#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Cutting the x variable</span></span>
<span id="cb72-4"><a href="moving-beyond-linearity.html#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">cut</span>(df<span class="sc">$</span>age</span>
<span id="cb72-5"><a href="moving-beyond-linearity.html#cb72-5" aria-hidden="true" tabindex="-1"></a>          ,<span class="at">breaks =</span> cuts))</span></code></pre></div>
<pre><code>## 
## (17.9,33.5]   (33.5,49]   (49,64.5] (64.5,80.1] 
##         750        1399         779          72</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="moving-beyond-linearity.html#cb74-1" aria-hidden="true" tabindex="-1"></a>  <span class="co">#&#39; Note, this only shows where the cuts lie and how many there are in each</span></span>
<span id="cb74-2"><a href="moving-beyond-linearity.html#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="moving-beyond-linearity.html#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="moving-beyond-linearity.html#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Fitting the step function</span></span>
<span id="cb74-5"><a href="moving-beyond-linearity.html#cb74-5" aria-hidden="true" tabindex="-1"></a>fit.step <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">cut</span>(df<span class="sc">$</span>age,<span class="dv">4</span>)</span>
<span id="cb74-6"><a href="moving-beyond-linearity.html#cb74-6" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">data =</span> df)</span>
<span id="cb74-7"><a href="moving-beyond-linearity.html#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="moving-beyond-linearity.html#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit.step))</span></code></pre></div>
<pre><code>##                            Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)               94.158392   1.476069 63.789970 0.000000e+00
## cut(df$age, 4)(33.5,49]   24.053491   1.829431 13.148074 1.982315e-38
## cut(df$age, 4)(49,64.5]   23.664559   2.067958 11.443444 1.040750e-29
## cut(df$age, 4)(64.5,80.1]  7.640592   4.987424  1.531972 1.256350e-01</code></pre>
<p>We see that the the first cut (bin with people up to 33,5) have been left out. That is because they are contained in the intercept.</p>
<p>Now we can fit the step function</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="moving-beyond-linearity.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stats)</span>
<span id="cb76-2"><a href="moving-beyond-linearity.html#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="moving-beyond-linearity.html#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Predictions</span></span>
<span id="cb76-4"><a href="moving-beyond-linearity.html#cb76-4" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.step</span>
<span id="cb76-5"><a href="moving-beyond-linearity.html#cb76-5" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid)) <span class="co">#Renaming age.grid to age</span></span>
<span id="cb76-6"><a href="moving-beyond-linearity.html#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="moving-beyond-linearity.html#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting</span></span>
<span id="cb76-8"><a href="moving-beyond-linearity.html#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(x = df$age,y = df$wage,col = &quot;darkgrey&quot;,cex = 0.8)</span></span>
<span id="cb76-9"><a href="moving-beyond-linearity.html#cb76-9" aria-hidden="true" tabindex="-1"></a><span class="co"># grid()</span></span>
<span id="cb76-10"><a href="moving-beyond-linearity.html#cb76-10" aria-hidden="true" tabindex="-1"></a><span class="co"># lines(age.grid</span></span>
<span id="cb76-11"><a href="moving-beyond-linearity.html#cb76-11" aria-hidden="true" tabindex="-1"></a><span class="co">#       ,preds</span></span>
<span id="cb76-12"><a href="moving-beyond-linearity.html#cb76-12" aria-hidden="true" tabindex="-1"></a><span class="co">#       ,col = &quot;red&quot;)</span></span>
<span id="cb76-13"><a href="moving-beyond-linearity.html#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="co"># title(&quot;Step function of 3rd order&quot;)</span></span></code></pre></div>
<p>I need to check what she is doing, one could perhaps manually order the</p>
</div>
</div>
<div id="exercise-7" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Exercise 7</h3>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="moving-beyond-linearity.html#cb77-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Wage</span></code></pre></div>
<p>Evaluating features other features to see how age respond hereon.</p>
<p>We can plot the variables agains each other, to see how they interact.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="moving-beyond-linearity.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb78-2"><a href="moving-beyond-linearity.html#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb78-3"><a href="moving-beyond-linearity.html#cb78-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">y =</span> df<span class="sc">$</span>wage,<span class="at">x =</span> df[,i],<span class="at">xlab =</span> <span class="fu">names</span>(df)[i],<span class="at">ylab =</span> <span class="st">&quot;Wage&quot;</span>)</span>
<span id="cb78-4"><a href="moving-beyond-linearity.html#cb78-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grid</span>()</span>
<span id="cb78-5"><a href="moving-beyond-linearity.html#cb78-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(df)[i] <span class="sc">%&gt;%</span> <span class="fu">title</span>()</span>
<span id="cb78-6"><a href="moving-beyond-linearity.html#cb78-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-1.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-2.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-3.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-4.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-5.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-6.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-7.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-8.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-9.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-63-10.png" width="720" style="display: block; margin: auto;" /></p>
<p>Looking at race, it appears as if there is some relationship between race and wage the same with maritial status. Region only has values in one category, jobclass appear to visually have different means. The same goes for health and health insurance. Naturally log of wage has a non linear relationship with wage. Although the variable is the same, thus it cant be used for much to predict wage levels.</p>
<p>Since all the variables of interest, and we havenât worked with are all categorical, then we canât really do any polynomial regression with the data, as they are all factors.</p>
<p>What one could do is a mutlivariate linear model with different factors, or step functions or perhaps GAM where a continous varaible with polynomials are included.</p>
<p>Therefore, I will not elaborate much more on this.</p>
<p>Ana made three different models, notice, that these are linear models, as the polynomial regression is not able to handle this.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="moving-beyond-linearity.html#cb79-1" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">=</span> <span class="fu">lm</span>(wage <span class="sc">~</span> maritl, <span class="at">data =</span> df)</span>
<span id="cb79-2"><a href="moving-beyond-linearity.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(fit1) <span class="co"># here deviance = RSS</span></span></code></pre></div>
<pre><code>## [1] 4858941</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="moving-beyond-linearity.html#cb81-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-2"><a href="moving-beyond-linearity.html#cb81-2" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">=</span> <span class="fu">lm</span>(wage <span class="sc">~</span> jobclass, <span class="at">data =</span> df)</span>
<span id="cb81-3"><a href="moving-beyond-linearity.html#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(fit2)</span></code></pre></div>
<pre><code>## [1] 4998547</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="moving-beyond-linearity.html#cb83-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-2"><a href="moving-beyond-linearity.html#cb83-2" aria-hidden="true" tabindex="-1"></a>fit3 <span class="ot">=</span> <span class="fu">lm</span>(wage <span class="sc">~</span> maritl <span class="sc">+</span> jobclass, <span class="at">data =</span> df)</span>
<span id="cb83-3"><a href="moving-beyond-linearity.html#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(fit3)<span class="co"># Select model fit3 (smallest deviance)</span></span></code></pre></div>
<pre><code>## [1] 4654752</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="moving-beyond-linearity.html#cb85-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-2"><a href="moving-beyond-linearity.html#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wage ~ maritl + jobclass, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -107.108  -22.689   -5.749   16.445  212.492 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              85.315      1.679  50.818  &lt; 2e-16 ***
## maritl2. Married         25.356      1.776  14.279  &lt; 2e-16 ***
## maritl3. Widowed          8.137      9.178   0.887  0.37541    
## maritl4. Divorced         9.664      3.166   3.052  0.00229 ** 
## maritl5. Separated        7.189      5.539   1.298  0.19441    
## jobclass2. Information   16.523      1.442  11.460  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 39.43 on 2994 degrees of freedom
## Multiple R-squared:  0.1086, Adjusted R-squared:  0.1072 
## F-statistic: 72.98 on 5 and 2994 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can assess the groups with the contrasts function.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="moving-beyond-linearity.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To interpret first identify which is ref category</span></span>
<span id="cb87-2"><a href="moving-beyond-linearity.html#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(Wage<span class="sc">$</span>maritl) <span class="co"># Never Married is the reference category</span></span></code></pre></div>
<pre><code>##                  2. Married 3. Widowed 4. Divorced 5. Separated
## 1. Never Married          0          0           0            0
## 2. Married                1          0           0            0
## 3. Widowed                0          1           0            0
## 4. Divorced               0          0           1            0
## 5. Separated              0          0           0            1</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="moving-beyond-linearity.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(Wage<span class="sc">$</span>jobclass) <span class="co"># Industrial is the reference category</span></span></code></pre></div>
<pre><code>##                2. Information
## 1. Industrial               0
## 2. Information              1</code></pre>
<p>Anova can also show the deviances etc. but notice, these does not appear to be neste d(JK note)?????</p>
<ul>
<li>The answer, fit 1 and fit 2 are nested into fit 3. Thus we dont compare fit 1 and fit 2, as these are not nested.</li>
</ul>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="moving-beyond-linearity.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(fit1,fit2,fit3)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">Res.Df</th>
<th align="right">RSS</th>
<th align="right">Df</th>
<th align="right">Sum of Sq</th>
<th align="right">F</th>
<th align="right">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2995</td>
<td align="right">4858941</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="right">2998</td>
<td align="right">4998547</td>
<td align="right">-3</td>
<td align="right">-139606</td>
<td align="right">29.93215</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="right">2994</td>
<td align="right">4654752</td>
<td align="right">4</td>
<td align="right">343795</td>
<td align="right">55.28341</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<p>Now we can check the residuals</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="moving-beyond-linearity.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb92-2"><a href="moving-beyond-linearity.html#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit3)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-67-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Looking at:</p>
<ul>
<li>The top left of the plot (residuals vs fitted), we would like these to be around 0.</li>
<li>The top right, we want them to have a linear shape. This looks odd</li>
</ul>
<p>Based on this, the model may be questionable. The solution:</p>
<ul>
<li>Exclude the extreme values</li>
<li>Finding a variable that account for them.</li>
</ul>
</div>
<div id="exercise-8" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Exercise 8</h3>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="moving-beyond-linearity.html#cb93-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Auto</span></code></pre></div>
<p>Are we able to predict how old a car is based on the variables at hand?</p>
<p>Hence year = DV</p>
<p>Name contains a lot of value, let us only use the first word, as that appear to be the brand. Therefore a loop is created to correct all the misspelled names.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="moving-beyond-linearity.html#cb94-1" aria-hidden="true" tabindex="-1"></a>brand <span class="ot">&lt;-</span> <span class="fu">strsplit</span>(<span class="at">x =</span> <span class="fu">as.character</span>(df<span class="sc">$</span>name),<span class="at">split =</span> <span class="st">&quot; &quot;</span>)</span>
<span id="cb94-2"><a href="moving-beyond-linearity.html#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="moving-beyond-linearity.html#cb94-3" aria-hidden="true" tabindex="-1"></a>brand.name <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(<span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(brand)))</span>
<span id="cb94-4"><a href="moving-beyond-linearity.html#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="moving-beyond-linearity.html#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(brand))) {</span>
<span id="cb94-6"><a href="moving-beyond-linearity.html#cb94-6" aria-hidden="true" tabindex="-1"></a>  brand.name[i] <span class="ot">&lt;-</span> brand[[i]][<span class="dv">1</span>]</span>
<span id="cb94-7"><a href="moving-beyond-linearity.html#cb94-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb94-8"><a href="moving-beyond-linearity.html#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="moving-beyond-linearity.html#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(brand.name)</span></code></pre></div>
<pre><code>## brand.name
##           amc          audi           bmw         buick      cadillac 
##            27             7             2            17             2 
##         capri     chevroelt     chevrolet         chevy      chrysler 
##             1             1            43             3             6 
##        datsun         dodge          fiat          ford            hi 
##            23            28             8            48             1 
##         honda         maxda         mazda      mercedes mercedes-benz 
##            13             2            10             1             2 
##       mercury        nissan    oldsmobile          opel       peugeot 
##            11             1            10             4             8 
##      plymouth       pontiac       renault          saab        subaru 
##            31            16             3             4             4 
##        toyota       toyouta       triumph     vokswagen    volkswagen 
##            25             1             1             1            15 
##         volvo            vw 
##             6             6</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="moving-beyond-linearity.html#cb96-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-2"><a href="moving-beyond-linearity.html#cb96-2" aria-hidden="true" tabindex="-1"></a>misspelled <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">byrow =</span> <span class="cn">TRUE</span>,<span class="at">ncol =</span> <span class="dv">2</span></span>
<span id="cb96-3"><a href="moving-beyond-linearity.html#cb96-3" aria-hidden="true" tabindex="-1"></a>                     ,<span class="at">data =</span> <span class="fu">c</span>(<span class="st">&quot;mercedes&quot;</span>,<span class="st">&quot;mercedes-benz&quot;</span></span>
<span id="cb96-4"><a href="moving-beyond-linearity.html#cb96-4" aria-hidden="true" tabindex="-1"></a>                              ,<span class="st">&quot;toyouta&quot;</span>,<span class="st">&quot;toyota&quot;</span></span>
<span id="cb96-5"><a href="moving-beyond-linearity.html#cb96-5" aria-hidden="true" tabindex="-1"></a>                              ,<span class="st">&quot;chevroelt&quot;</span>,<span class="st">&quot;chevrolet&quot;</span></span>
<span id="cb96-6"><a href="moving-beyond-linearity.html#cb96-6" aria-hidden="true" tabindex="-1"></a>                              ,<span class="st">&quot;maxda&quot;</span>,<span class="st">&quot;mazda&quot;</span></span>
<span id="cb96-7"><a href="moving-beyond-linearity.html#cb96-7" aria-hidden="true" tabindex="-1"></a>                              ,<span class="st">&quot;vokswagen&quot;</span>,<span class="st">&quot;volkswagen&quot;</span></span>
<span id="cb96-8"><a href="moving-beyond-linearity.html#cb96-8" aria-hidden="true" tabindex="-1"></a>                              ,<span class="st">&quot;vw&quot;</span>,<span class="st">&quot;volkswagen&quot;</span>))</span>
<span id="cb96-9"><a href="moving-beyond-linearity.html#cb96-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-10"><a href="moving-beyond-linearity.html#cb96-10" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb96-11"><a href="moving-beyond-linearity.html#cb96-11" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb96-12"><a href="moving-beyond-linearity.html#cb96-12" aria-hidden="true" tabindex="-1"></a>bn.list <span class="ot">&lt;-</span> <span class="fu">as.list</span>(<span class="dv">0</span>)</span>
<span id="cb96-13"><a href="moving-beyond-linearity.html#cb96-13" aria-hidden="true" tabindex="-1"></a>brand.name.recent <span class="ot">&lt;-</span> brand.name</span>
<span id="cb96-14"><a href="moving-beyond-linearity.html#cb96-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">c</span>(misspelled[,<span class="dv">1</span>])) {</span>
<span id="cb96-15"><a href="moving-beyond-linearity.html#cb96-15" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> n <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb96-16"><a href="moving-beyond-linearity.html#cb96-16" aria-hidden="true" tabindex="-1"></a>  index <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">FALSE</span>,<span class="fu">length</span>(brand.name))</span>
<span id="cb96-17"><a href="moving-beyond-linearity.html#cb96-17" aria-hidden="true" tabindex="-1"></a>  index[brand.name <span class="sc">==</span> i] <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb96-18"><a href="moving-beyond-linearity.html#cb96-18" aria-hidden="true" tabindex="-1"></a>  bn.list[[n]] <span class="ot">&lt;-</span> <span class="fu">replace</span>(<span class="at">x =</span> brand.name.recent,<span class="at">list =</span> index,<span class="at">values =</span> misspelled[n,<span class="dv">2</span>])</span>
<span id="cb96-19"><a href="moving-beyond-linearity.html#cb96-19" aria-hidden="true" tabindex="-1"></a>  brand.name.recent <span class="ot">&lt;-</span> <span class="fu">replace</span>(<span class="at">x =</span> brand.name.recent,<span class="at">list =</span> index,<span class="at">values =</span> misspelled[n,<span class="dv">2</span>])</span>
<span id="cb96-20"><a href="moving-beyond-linearity.html#cb96-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb96-21"><a href="moving-beyond-linearity.html#cb96-21" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">cbind</span>(df[,<span class="sc">-</span><span class="dv">9</span>],<span class="fu">as.factor</span>(bn.list[[<span class="dv">6</span>]]))</span>
<span id="cb96-22"><a href="moving-beyond-linearity.html#cb96-22" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df)[<span class="fu">names</span>(df) <span class="sc">==</span> <span class="st">&#39;bn.list[[6]]&#39;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;brand.name&quot;</span></span></code></pre></div>
<p>Also we must convert origin to a factor.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="moving-beyond-linearity.html#cb97-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>origin <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(df<span class="sc">$</span>origin)</span></code></pre></div>
<p>Checking correlations.</p>
<p>The following can be run to see all the combinations</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="moving-beyond-linearity.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># par(mfrow = c(1,1))</span></span>
<span id="cb98-2"><a href="moving-beyond-linearity.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for (i in 1:dim(mm)[2]) {</span></span>
<span id="cb98-3"><a href="moving-beyond-linearity.html#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="co">#   plot(y = df$year,x = mm[,i],xlab = names(mm)[i],ylab = &quot;Year&quot;)</span></span>
<span id="cb98-4"><a href="moving-beyond-linearity.html#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="co">#   grid()</span></span>
<span id="cb98-5"><a href="moving-beyond-linearity.html#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="co">#   colnames(mm)[i] %&gt;% title()</span></span>
<span id="cb98-6"><a href="moving-beyond-linearity.html#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span></code></pre></div>
<p>Before training the model, we can partition the data to test the model out of sample</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="moving-beyond-linearity.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb99-2"><a href="moving-beyond-linearity.html#cb99-2" aria-hidden="true" tabindex="-1"></a>train.size <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="at">x =</span> <span class="fu">nrow</span>(df)<span class="sc">*</span><span class="fl">0.8</span>,<span class="at">digits =</span> <span class="dv">0</span>) <span class="co">#Setting the training size</span></span>
<span id="cb99-3"><a href="moving-beyond-linearity.html#cb99-3" aria-hidden="true" tabindex="-1"></a>train.index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)),<span class="at">size =</span> train.size) <span class="co">#setting seed and creating vector for index</span></span>
<span id="cb99-4"><a href="moving-beyond-linearity.html#cb99-4" aria-hidden="true" tabindex="-1"></a>mm <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(year <span class="sc">~</span> .,<span class="at">data =</span> df)[,<span class="sc">-</span><span class="dv">1</span>] <span class="co">#tried to make it mm first, to get rid of having variables that were in one partition but not the other.</span></span>
<span id="cb99-5"><a href="moving-beyond-linearity.html#cb99-5" aria-hidden="true" tabindex="-1"></a>year <span class="ot">&lt;-</span> df<span class="sc">$</span>year</span>
<span id="cb99-6"><a href="moving-beyond-linearity.html#cb99-6" aria-hidden="true" tabindex="-1"></a>train.df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(year,mm[train.index,])) <span class="co">#crating the training set</span></span>
<span id="cb99-7"><a href="moving-beyond-linearity.html#cb99-7" aria-hidden="true" tabindex="-1"></a>test.df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(year,mm[<span class="sc">-</span>train.index,])) <span class="co">#creating the testing set</span></span></code></pre></div>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="moving-beyond-linearity.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gam)</span>
<span id="cb100-2"><a href="moving-beyond-linearity.html#cb100-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-3"><a href="moving-beyond-linearity.html#cb100-3" aria-hidden="true" tabindex="-1"></a>gam.m1 <span class="ot">&lt;-</span> <span class="fu">gam</span>(year <span class="sc">~</span> <span class="fu">s</span>(train.df<span class="sc">$</span>mpg,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(train.df<span class="sc">$</span>cylinders,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(train.df<span class="sc">$</span>displacement,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(train.df<span class="sc">$</span>horsepower,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(train.df<span class="sc">$</span>weight,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(train.df<span class="sc">$</span>acceleration,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> .</span>
<span id="cb100-4"><a href="moving-beyond-linearity.html#cb100-4" aria-hidden="true" tabindex="-1"></a>           ,<span class="at">data =</span> train.df)</span>
<span id="cb100-5"><a href="moving-beyond-linearity.html#cb100-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-6"><a href="moving-beyond-linearity.html#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.m1)</span></code></pre></div>
<pre><code>## 
## Call: gam(formula = year ~ s(train.df$mpg, df = 5) + s(train.df$cylinders, 
##     df = 5) + s(train.df$displacement, df = 5) + s(train.df$horsepower, 
##     df = 5) + s(train.df$weight, df = 5) + s(train.df$acceleration, 
##     df = 5) + ., data = train.df)
## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.5818 -2.0705 -0.2068  2.0660  6.0843 
## 
## (Dispersion Parameter for gaussian family taken to be 8.889)
## 
##     Null Deviance: 2675.86 on 313 degrees of freedom
## Residual Deviance: 2266.693 on 255.0004 degrees of freedom
## AIC: 1631.771 
## 
## Number of Local Scoring Iterations: NA 
## 
## Anova for Parametric Effects
##                                               Df  Sum Sq Mean Sq F value
## s(train.df$mpg, df = 5)                        1   10.71  10.707  1.2045
## s(train.df$cylinders, df = 5)                  1    0.00   0.002  0.0003
## s(train.df$displacement, df = 5)               1    2.44   2.441  0.2747
## s(train.df$horsepower, df = 5)                 1    0.13   0.134  0.0151
## s(train.df$weight, df = 5)                     1    5.93   5.926  0.6667
## s(train.df$acceleration, df = 5)               1    1.52   1.517  0.1706
## origin2                                        1   16.05  16.050  1.8056
## origin3                                        1   11.10  11.097  1.2484
## `\\`as.factor(bn.list[[6]])\\`audi`            1    6.53   6.526  0.7342
## `\\`as.factor(bn.list[[6]])\\`bmw`             1   32.67  32.668  3.6751
## `\\`as.factor(bn.list[[6]])\\`buick`           1    8.31   8.307  0.9345
## `\\`as.factor(bn.list[[6]])\\`cadillac`        1    0.85   0.854  0.0960
## `\\`as.factor(bn.list[[6]])\\`capri`           1    0.00   0.003  0.0004
## `\\`as.factor(bn.list[[6]])\\`chevrolet`       1    0.23   0.230  0.0258
## `\\`as.factor(bn.list[[6]])\\`chevy`           1    0.00   0.003  0.0004
## `\\`as.factor(bn.list[[6]])\\`chrysler`        1   46.30  46.301  5.2088
## `\\`as.factor(bn.list[[6]])\\`datsun`          1   18.32  18.321  2.0611
## `\\`as.factor(bn.list[[6]])\\`dodge`           1    2.70   2.702  0.3040
## `\\`as.factor(bn.list[[6]])\\`fiat`            1    1.89   1.895  0.2132
## `\\`as.factor(bn.list[[6]])\\`ford`            1   25.85  25.847  2.9077
## `\\`as.factor(bn.list[[6]])\\`hi`              1    0.34   0.337  0.0379
## `\\`as.factor(bn.list[[6]])\\`honda`           1    3.40   3.401  0.3826
## `\\`as.factor(bn.list[[6]])\\`mazda`           1    4.99   4.986  0.5609
## `\\`as.factor(bn.list[[6]])\\`mercedes-benz`   1    0.01   0.014  0.0016
## `\\`as.factor(bn.list[[6]])\\`mercury`         1    0.31   0.313  0.0352
## `\\`as.factor(bn.list[[6]])\\`nissan`          1    3.68   3.684  0.4145
## `\\`as.factor(bn.list[[6]])\\`oldsmobile`      1   18.12  18.124  2.0389
## `\\`as.factor(bn.list[[6]])\\`opel`            1    0.30   0.303  0.0341
## `\\`as.factor(bn.list[[6]])\\`peugeot`         1    1.79   1.794  0.2018
## `\\`as.factor(bn.list[[6]])\\`plymouth`        1    0.53   0.527  0.0593
## `\\`as.factor(bn.list[[6]])\\`pontiac`         1    4.37   4.374  0.4921
## `\\`as.factor(bn.list[[6]])\\`renault`         1   16.53  16.533  1.8600
## `\\`as.factor(bn.list[[6]])\\`saab`            1    0.10   0.103  0.0115
## `\\`as.factor(bn.list[[6]])\\`subaru`          1    0.00   0.001  0.0001
## `\\`as.factor(bn.list[[6]])\\`volkswagen`      1    0.63   0.634  0.0713
## Residuals                                    255 2266.69   8.889        
##                                               Pr(&gt;F)  
## s(train.df$mpg, df = 5)                      0.27346  
## s(train.df$cylinders, df = 5)                0.98665  
## s(train.df$displacement, df = 5)             0.60068  
## s(train.df$horsepower, df = 5)               0.90226  
## s(train.df$weight, df = 5)                   0.41496  
## s(train.df$acceleration, df = 5)             0.67989  
## origin2                                      0.18023  
## origin3                                      0.26491  
## `\\`as.factor(bn.list[[6]])\\`audi`          0.39233  
## `\\`as.factor(bn.list[[6]])\\`bmw`           0.05635 .
## `\\`as.factor(bn.list[[6]])\\`buick`         0.33460  
## `\\`as.factor(bn.list[[6]])\\`cadillac`      0.75690  
## `\\`as.factor(bn.list[[6]])\\`capri`         0.98494  
## `\\`as.factor(bn.list[[6]])\\`chevrolet`     0.87244  
## `\\`as.factor(bn.list[[6]])\\`chevy`         0.98454  
## `\\`as.factor(bn.list[[6]])\\`chrysler`      0.02330 *
## `\\`as.factor(bn.list[[6]])\\`datsun`        0.15233  
## `\\`as.factor(bn.list[[6]])\\`dodge`         0.58186  
## `\\`as.factor(bn.list[[6]])\\`fiat`          0.64469  
## `\\`as.factor(bn.list[[6]])\\`ford`          0.08937 .
## `\\`as.factor(bn.list[[6]])\\`hi`            0.84586  
## `\\`as.factor(bn.list[[6]])\\`honda`         0.53679  
## `\\`as.factor(bn.list[[6]])\\`mazda`         0.45459  
## `\\`as.factor(bn.list[[6]])\\`mercedes-benz` 0.96840  
## `\\`as.factor(bn.list[[6]])\\`mercury`       0.85133  
## `\\`as.factor(bn.list[[6]])\\`nissan`        0.52028  
## `\\`as.factor(bn.list[[6]])\\`oldsmobile`    0.15454  
## `\\`as.factor(bn.list[[6]])\\`opel`          0.85372  
## `\\`as.factor(bn.list[[6]])\\`peugeot`       0.65362  
## `\\`as.factor(bn.list[[6]])\\`plymouth`      0.80775  
## `\\`as.factor(bn.list[[6]])\\`pontiac`       0.48365  
## `\\`as.factor(bn.list[[6]])\\`renault`       0.17383  
## `\\`as.factor(bn.list[[6]])\\`saab`          0.91452  
## `\\`as.factor(bn.list[[6]])\\`subaru`        0.99320  
## `\\`as.factor(bn.list[[6]])\\`volkswagen`    0.78966  
## Residuals                                             
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Anova for Nonparametric Effects
##                                              Npar Df  Npar F  Pr(F)
## (Intercept)                                                        
## s(train.df$mpg, df = 5)                            4 0.97458 0.4219
## s(train.df$cylinders, df = 5)                      3 0.63467 0.5933
## s(train.df$displacement, df = 5)                   4 1.72358 0.1452
## s(train.df$horsepower, df = 5)                     4 1.13935 0.3384
## s(train.df$weight, df = 5)                         4 0.69772 0.5941
## s(train.df$acceleration, df = 5)                   4 0.93342 0.4451
## mpg                                                                
## cylinders                                                          
## displacement                                                       
## horsepower                                                         
## weight                                                             
## acceleration                                                       
## origin2                                                            
## origin3                                                            
## `\\`as.factor(bn.list[[6]])\\`audi`                                
## `\\`as.factor(bn.list[[6]])\\`bmw`                                 
## `\\`as.factor(bn.list[[6]])\\`buick`                               
## `\\`as.factor(bn.list[[6]])\\`cadillac`                            
## `\\`as.factor(bn.list[[6]])\\`capri`                               
## `\\`as.factor(bn.list[[6]])\\`chevrolet`                           
## `\\`as.factor(bn.list[[6]])\\`chevy`                               
## `\\`as.factor(bn.list[[6]])\\`chrysler`                            
## `\\`as.factor(bn.list[[6]])\\`datsun`                              
## `\\`as.factor(bn.list[[6]])\\`dodge`                               
## `\\`as.factor(bn.list[[6]])\\`fiat`                                
## `\\`as.factor(bn.list[[6]])\\`ford`                                
## `\\`as.factor(bn.list[[6]])\\`hi`                                  
## `\\`as.factor(bn.list[[6]])\\`honda`                               
## `\\`as.factor(bn.list[[6]])\\`mazda`                               
## `\\`as.factor(bn.list[[6]])\\`mercedes-benz`                       
## `\\`as.factor(bn.list[[6]])\\`mercury`                             
## `\\`as.factor(bn.list[[6]])\\`nissan`                              
## `\\`as.factor(bn.list[[6]])\\`oldsmobile`                          
## `\\`as.factor(bn.list[[6]])\\`opel`                                
## `\\`as.factor(bn.list[[6]])\\`peugeot`                             
## `\\`as.factor(bn.list[[6]])\\`plymouth`                            
## `\\`as.factor(bn.list[[6]])\\`pontiac`                             
## `\\`as.factor(bn.list[[6]])\\`renault`                             
## `\\`as.factor(bn.list[[6]])\\`saab`                                
## `\\`as.factor(bn.list[[6]])\\`subaru`                              
## `\\`as.factor(bn.list[[6]])\\`toyota`                              
## `\\`as.factor(bn.list[[6]])\\`triumph`                             
## `\\`as.factor(bn.list[[6]])\\`volkswagen`                          
## `\\`as.factor(bn.list[[6]])\\`volvo`</code></pre>
<p>It appears as if non of the parameters are good predictors.</p>
<p>Then one could try out other models, or perhaps it is just very difficult with the data at hand to predict the year of the car.</p>
</div>
<div id="exercise-9" class="section level3" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Exercise 9</h3>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="moving-beyond-linearity.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb102-2"><a href="moving-beyond-linearity.html#cb102-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Boston</span>
<span id="cb102-3"><a href="moving-beyond-linearity.html#cb102-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(df<span class="sc">$</span>nox,df<span class="sc">$</span>dis))</span>
<span id="cb102-4"><a href="moving-beyond-linearity.html#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;nox&quot;</span>,<span class="st">&quot;dis&quot;</span>)</span></code></pre></div>
<div id="a-using-poly-function-to-fit-cubic-polynomial-regression" class="section level4" number="2.4.4.1">
<h4><span class="header-section-number">2.4.4.1</span> (a) using poly function to fit cubic polynomial regression</h4>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="moving-beyond-linearity.html#cb103-1" aria-hidden="true" tabindex="-1"></a>fit.poly <span class="ot">&lt;-</span> <span class="fu">lm</span>(nox <span class="sc">~</span> <span class="fu">poly</span>(dis,<span class="dv">3</span>),<span class="at">data =</span> df)</span>
<span id="cb103-2"><a href="moving-beyond-linearity.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.poly)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = nox ~ poly(dis, 3), data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.121130 -0.040619 -0.009738  0.023385  0.194904 
## 
## Coefficients:
##                Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)    0.554695   0.002759 201.021     &lt; 2e-16 ***
## poly(dis, 3)1 -2.003096   0.062071 -32.271     &lt; 2e-16 ***
## poly(dis, 3)2  0.856330   0.062071  13.796     &lt; 2e-16 ***
## poly(dis, 3)3 -0.318049   0.062071  -5.124 0.000000427 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.06207 on 502 degrees of freedom
## Multiple R-squared:  0.7148, Adjusted R-squared:  0.7131 
## F-statistic: 419.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Remember that we are not interested in the coefficients as they are misleading, thus we want to look at the shape.</p>
<p>The table above is mostly presented for explanatory reasons.</p>
<p>As we are interested in the curve, we can fit that.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="moving-beyond-linearity.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Defining range</span></span>
<span id="cb105-2"><a href="moving-beyond-linearity.html#cb105-2" aria-hidden="true" tabindex="-1"></a>dislims <span class="ot">&lt;-</span> <span class="fu">range</span>(df<span class="sc">$</span>dis)</span>
<span id="cb105-3"><a href="moving-beyond-linearity.html#cb105-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> (dislims[<span class="dv">2</span>]<span class="sc">-</span>dislims[<span class="dv">1</span>])<span class="sc">/</span><span class="fu">nrow</span>(df)</span>
<span id="cb105-4"><a href="moving-beyond-linearity.html#cb105-4" aria-hidden="true" tabindex="-1"></a>dis.grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> dislims[<span class="dv">1</span>],<span class="at">to =</span> dislims[<span class="dv">2</span>],<span class="at">by =</span> n)</span>
<span id="cb105-5"><a href="moving-beyond-linearity.html#cb105-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-6"><a href="moving-beyond-linearity.html#cb105-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Predictions for the plot</span></span>
<span id="cb105-7"><a href="moving-beyond-linearity.html#cb105-7" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.poly,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">dis =</span> dis.grid))</span>
<span id="cb105-8"><a href="moving-beyond-linearity.html#cb105-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-9"><a href="moving-beyond-linearity.html#cb105-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting</span></span>
<span id="cb105-10"><a href="moving-beyond-linearity.html#cb105-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nox <span class="sc">~</span> dis, <span class="at">data =</span> df, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb105-11"><a href="moving-beyond-linearity.html#cb105-11" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb105-12"><a href="moving-beyond-linearity.html#cb105-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> dis.grid,<span class="at">y =</span>  preds, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb105-13"><a href="moving-beyond-linearity.html#cb105-13" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Cubic polynomial&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-78-1.png" width="720" style="display: block; margin: auto;" /></p>
</div>
<div id="b-plotting-polynomial-fits-for-a-range-of-polynomials" class="section level4" number="2.4.4.2">
<h4><span class="header-section-number">2.4.4.2</span> (b) Plotting polynomial fits for a range of polynomials</h4>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="moving-beyond-linearity.html#cb106-1" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb106-2"><a href="moving-beyond-linearity.html#cb106-2" aria-hidden="true" tabindex="-1"></a>RSS <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb106-3"><a href="moving-beyond-linearity.html#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb106-4"><a href="moving-beyond-linearity.html#cb106-4" aria-hidden="true" tabindex="-1"></a>  models[[d]] <span class="ot">&lt;-</span> <span class="fu">lm</span>(nox <span class="sc">~</span> <span class="fu">poly</span>(dis,d),<span class="at">data =</span> df)</span>
<span id="cb106-5"><a href="moving-beyond-linearity.html#cb106-5" aria-hidden="true" tabindex="-1"></a>  RSS[d] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(models[[d]])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb106-6"><a href="moving-beyond-linearity.html#cb106-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb106-7"><a href="moving-beyond-linearity.html#cb106-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(RSS,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb106-8"><a href="moving-beyond-linearity.html#cb106-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="fu">which.min</span>(RSS),<span class="at">y =</span> RSS[<span class="fu">which.min</span>(RSS)],<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb106-9"><a href="moving-beyond-linearity.html#cb106-9" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb106-10"><a href="moving-beyond-linearity.html#cb106-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">min</span>(RSS),<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb106-11"><a href="moving-beyond-linearity.html#cb106-11" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;In-sample error&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-79-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We see that the RSS decrease with complexity, that it as expected, as we fit to the in sample data. We could do this with a partition of the data to see out of performance instead.</p>
</div>
<div id="c-using-cv-to-select-best-degree-of-d" class="section level4" number="2.4.4.3">
<h4><span class="header-section-number">2.4.4.3</span> (c) Using CV to select best degree of d</h4>
<p>Here we run a loop with cross validation to see how the different order of d performs. As the partitions are randomly selected, we preduce 10 simulations to see which orders that tend to occur most often.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="moving-beyond-linearity.html#cb107-1" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb107-2"><a href="moving-beyond-linearity.html#cb107-2" aria-hidden="true" tabindex="-1"></a>RSS <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-3"><a href="moving-beyond-linearity.html#cb107-3" aria-hidden="true" tabindex="-1"></a>CV.RSS <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-4"><a href="moving-beyond-linearity.html#cb107-4" aria-hidden="true" tabindex="-1"></a>CV.RSS.sim <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-5"><a href="moving-beyond-linearity.html#cb107-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-6"><a href="moving-beyond-linearity.html#cb107-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) {</span>
<span id="cb107-7"><a href="moving-beyond-linearity.html#cb107-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb107-8"><a href="moving-beyond-linearity.html#cb107-8" aria-hidden="true" tabindex="-1"></a>  models[[d]] <span class="ot">&lt;-</span> <span class="fu">glm</span>(nox <span class="sc">~</span> <span class="fu">poly</span>(dis,d),<span class="at">data =</span> df)</span>
<span id="cb107-9"><a href="moving-beyond-linearity.html#cb107-9" aria-hidden="true" tabindex="-1"></a>  RSS[d] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(models[[d]])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb107-10"><a href="moving-beyond-linearity.html#cb107-10" aria-hidden="true" tabindex="-1"></a>  CV.RSS[d] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(<span class="at">data =</span> df,<span class="at">glmfit =</span> models[[d]],<span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">2</span>] <span class="co">#Delta = prediction error (adjusted)</span></span>
<span id="cb107-11"><a href="moving-beyond-linearity.html#cb107-11" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb107-12"><a href="moving-beyond-linearity.html#cb107-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb107-13"><a href="moving-beyond-linearity.html#cb107-13" aria-hidden="true" tabindex="-1"></a>  CV.RSS.sim[i] <span class="ot">&lt;-</span> <span class="fu">which.min</span>(CV.RSS)</span>
<span id="cb107-14"><a href="moving-beyond-linearity.html#cb107-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb107-15"><a href="moving-beyond-linearity.html#cb107-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-16"><a href="moving-beyond-linearity.html#cb107-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting prediction error</span></span>
<span id="cb107-17"><a href="moving-beyond-linearity.html#cb107-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(CV.RSS,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb107-18"><a href="moving-beyond-linearity.html#cb107-18" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="fu">which.min</span>(CV.RSS),<span class="at">y =</span> CV.RSS[<span class="fu">which.min</span>(CV.RSS)],<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb107-19"><a href="moving-beyond-linearity.html#cb107-19" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb107-20"><a href="moving-beyond-linearity.html#cb107-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">min</span>(CV.RSS),<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb107-21"><a href="moving-beyond-linearity.html#cb107-21" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;CV K = 10 prediction error&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-80-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="moving-beyond-linearity.html#cb108-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-2"><a href="moving-beyond-linearity.html#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting simulations</span></span>
<span id="cb108-3"><a href="moving-beyond-linearity.html#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(CV.RSS.sim),<span class="at">xlab =</span> <span class="st">&quot;Degree of d&quot;</span>,<span class="at">ylab =</span> <span class="st">&quot;Frequency&quot;</span>)</span>
<span id="cb108-4"><a href="moving-beyond-linearity.html#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span>
<span id="cb108-5"><a href="moving-beyond-linearity.html#cb108-5" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;CV K = 10, Iterations = 20&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-80-2.png" width="720" style="display: block; margin: auto;" /></p>
<p>In these simulations we see that the best fit is likely to be with using .</p>
<p>It is actually quite interesting that a model with 10 degrees of d is as competitive as 4 in this example, although the cubic model is far superior than the other models.</p>
</div>
<div id="d-use-bs-to-fit-a-regression-spline" class="section level4" number="2.4.4.4">
<h4><span class="header-section-number">2.4.4.4</span> (d) Use <code>bs()</code> to fit a regression spline</h4>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="moving-beyond-linearity.html#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb109-2"><a href="moving-beyond-linearity.html#cb109-2" aria-hidden="true" tabindex="-1"></a>fit.bs <span class="ot">&lt;-</span> <span class="fu">lm</span>(nox <span class="sc">~</span> <span class="fu">bs</span>(dis,<span class="at">df =</span> <span class="dv">4</span>),<span class="at">data =</span> df) <span class="co">#Note as degree is not defined, default = 3</span></span>
<span id="cb109-3"><a href="moving-beyond-linearity.html#cb109-3" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.bs,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">dis =</span> dis.grid))</span>
<span id="cb109-4"><a href="moving-beyond-linearity.html#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> df<span class="sc">$</span>dis,<span class="at">y =</span> df<span class="sc">$</span>nox,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>,<span class="at">pch =</span> <span class="dv">20</span>,<span class="at">ylab =</span> <span class="st">&quot;nox&quot;</span>,<span class="at">xlab =</span> <span class="st">&quot;dis&quot;</span>)</span>
<span id="cb109-5"><a href="moving-beyond-linearity.html#cb109-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> dis.grid,<span class="at">y =</span> preds,<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb109-6"><a href="moving-beyond-linearity.html#cb109-6" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb109-7"><a href="moving-beyond-linearity.html#cb109-7" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Regression Spline df = 4&quot;</span>)</span>
<span id="cb109-8"><a href="moving-beyond-linearity.html#cb109-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">3.20745</span>,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">lty =</span> <span class="dv">2</span>) <span class="co">#This is the cut, found in next chunk</span></span>
<span id="cb109-9"><a href="moving-beyond-linearity.html#cb109-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;fit&quot;</span>,<span class="st">&quot;cut&quot;</span>),<span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>))</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-81-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Notice that we merely specified the amount of df that we wanted. The function merely specified them automatically. We can interpret these, by using <code>dim()</code> and <code>attr()</code>.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="moving-beyond-linearity.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(<span class="fu">bs</span>(df<span class="sc">$</span>dis,<span class="at">df =</span> <span class="dv">4</span>)))</span></code></pre></div>
<pre><code>## [1] 506   4</code></pre>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="moving-beyond-linearity.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(<span class="fu">bs</span>(df<span class="sc">$</span>dis,<span class="at">df =</span> <span class="dv">4</span>),<span class="st">&quot;knots&quot;</span>)</span></code></pre></div>
<pre><code>##     50% 
## 3.20745</code></pre>
<p>We see that a model with 4 degrees of freedom yields one cut. Where the model put this at 50%, hence the first half (up to 3.20745). For simplicity, this cut has been added to the plot above, to show where the spline is split.</p>
</div>
<div id="e-now-fit-a-regression-spline" class="section level4" number="2.4.4.5">
<h4><span class="header-section-number">2.4.4.5</span> (e) Now fit a regression spline</h4>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="moving-beyond-linearity.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb114-2"><a href="moving-beyond-linearity.html#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">4</span><span class="sc">:</span><span class="dv">7</span>) {</span>
<span id="cb114-3"><a href="moving-beyond-linearity.html#cb114-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">#The fit + preds</span></span>
<span id="cb114-4"><a href="moving-beyond-linearity.html#cb114-4" aria-hidden="true" tabindex="-1"></a>  fit.bs <span class="ot">&lt;-</span> <span class="fu">lm</span>(nox <span class="sc">~</span> <span class="fu">bs</span>(dis,<span class="at">df =</span> d,<span class="at">degree =</span> <span class="dv">3</span>),<span class="at">data =</span> df)</span>
<span id="cb114-5"><a href="moving-beyond-linearity.html#cb114-5" aria-hidden="true" tabindex="-1"></a>  preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.bs,<span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">dis =</span> dis.grid))</span>
<span id="cb114-6"><a href="moving-beyond-linearity.html#cb114-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb114-7"><a href="moving-beyond-linearity.html#cb114-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Cut</span></span>
<span id="cb114-8"><a href="moving-beyond-linearity.html#cb114-8" aria-hidden="true" tabindex="-1"></a>  cut <span class="ot">&lt;-</span> <span class="fu">attr</span>(<span class="fu">bs</span>(df<span class="sc">$</span>dis,<span class="at">df =</span> d),<span class="st">&quot;knots&quot;</span>)</span>
<span id="cb114-9"><a href="moving-beyond-linearity.html#cb114-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb114-10"><a href="moving-beyond-linearity.html#cb114-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Plot</span></span>
<span id="cb114-11"><a href="moving-beyond-linearity.html#cb114-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">x =</span> df<span class="sc">$</span>dis,<span class="at">y =</span> df<span class="sc">$</span>nox,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>,<span class="at">pch =</span> <span class="dv">20</span>,<span class="at">ylab =</span> <span class="st">&quot;nox&quot;</span>,<span class="at">xlab =</span> <span class="st">&quot;dis&quot;</span>)</span>
<span id="cb114-12"><a href="moving-beyond-linearity.html#cb114-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="at">x =</span> dis.grid,<span class="at">y =</span> preds,<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb114-13"><a href="moving-beyond-linearity.html#cb114-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grid</span>()</span>
<span id="cb114-14"><a href="moving-beyond-linearity.html#cb114-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">&quot;Cubic Regression Spline, df =&quot;</span>,d))</span>
<span id="cb114-15"><a href="moving-beyond-linearity.html#cb114-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> cut,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">lty =</span> <span class="dv">2</span>) <span class="co">#This is the cut, found in next chunk</span></span>
<span id="cb114-16"><a href="moving-beyond-linearity.html#cb114-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>,<span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;fit&quot;</span>,<span class="st">&quot;cut&quot;</span>),<span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>))</span>
<span id="cb114-17"><a href="moving-beyond-linearity.html#cb114-17" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-83-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We start at four degrees of freedom as a model with only three degrees of freedom, hence cubic regression (three orders of polynomials) = three degrees of freedom <strong>(this has to be fact checked)</strong>.</p>
<p>As we add complexity with knots we also adds degrees of freedom, where we add one degree of freedom for each cut, hence for the cubic spline with 7 degrees of freedom, four cuts and three polynomials <strong>(this has to be fact checked)</strong>.</p>
</div>
<div id="f-perform-cross-validation-to-select-degrees" class="section level4" number="2.4.4.6">
<h4><span class="header-section-number">2.4.4.6</span> (f) Perform cross-validation, to select degrees</h4>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="moving-beyond-linearity.html#cb115-1" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb115-2"><a href="moving-beyond-linearity.html#cb115-2" aria-hidden="true" tabindex="-1"></a>RSS <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb115-3"><a href="moving-beyond-linearity.html#cb115-3" aria-hidden="true" tabindex="-1"></a>CV.RSS.sim <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb115-4"><a href="moving-beyond-linearity.html#cb115-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-5"><a href="moving-beyond-linearity.html#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) {</span>
<span id="cb115-6"><a href="moving-beyond-linearity.html#cb115-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">4</span><span class="sc">:</span><span class="dv">15</span>) {</span>
<span id="cb115-7"><a href="moving-beyond-linearity.html#cb115-7" aria-hidden="true" tabindex="-1"></a>  models[[d]] <span class="ot">&lt;-</span> <span class="fu">glm</span>(nox <span class="sc">~</span> <span class="fu">bs</span>(dis,<span class="at">df =</span> d,<span class="at">degree =</span> <span class="dv">3</span>),<span class="at">data =</span> df)</span>
<span id="cb115-8"><a href="moving-beyond-linearity.html#cb115-8" aria-hidden="true" tabindex="-1"></a>  RSS[d] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(models[[d]])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb115-9"><a href="moving-beyond-linearity.html#cb115-9" aria-hidden="true" tabindex="-1"></a>  CV.RSS[d] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(<span class="at">data =</span> df,<span class="at">glmfit =</span> models[[d]],<span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">2</span>] <span class="co">#Delta = prediction error (adjusted)</span></span>
<span id="cb115-10"><a href="moving-beyond-linearity.html#cb115-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb115-11"><a href="moving-beyond-linearity.html#cb115-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb115-12"><a href="moving-beyond-linearity.html#cb115-12" aria-hidden="true" tabindex="-1"></a>  CV.RSS.sim[i] <span class="ot">&lt;-</span> <span class="fu">which.min</span>(CV.RSS)</span>
<span id="cb115-13"><a href="moving-beyond-linearity.html#cb115-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb115-14"><a href="moving-beyond-linearity.html#cb115-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-15"><a href="moving-beyond-linearity.html#cb115-15" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="fl">4.5</span>,<span class="fl">4.5</span>,<span class="fl">2.1</span>),<span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb115-16"><a href="moving-beyond-linearity.html#cb115-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-17"><a href="moving-beyond-linearity.html#cb115-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting prediction error</span></span>
<span id="cb115-18"><a href="moving-beyond-linearity.html#cb115-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(CV.RSS,<span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb115-19"><a href="moving-beyond-linearity.html#cb115-19" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="fu">which.min</span>(CV.RSS),<span class="at">y =</span> CV.RSS[<span class="fu">which.min</span>(CV.RSS)],<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,<span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb115-20"><a href="moving-beyond-linearity.html#cb115-20" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb115-21"><a href="moving-beyond-linearity.html#cb115-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">min</span>(CV.RSS),<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,<span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb115-22"><a href="moving-beyond-linearity.html#cb115-22" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;CV K = 10 prediction error&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-84-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="moving-beyond-linearity.html#cb116-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-2"><a href="moving-beyond-linearity.html#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting simulations</span></span>
<span id="cb116-3"><a href="moving-beyond-linearity.html#cb116-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(CV.RSS.sim),<span class="at">xlab =</span> <span class="st">&quot;Degree of d&quot;</span>,<span class="at">ylab =</span> <span class="st">&quot;Frequency&quot;</span>)</span>
<span id="cb116-4"><a href="moving-beyond-linearity.html#cb116-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span>
<span id="cb116-5"><a href="moving-beyond-linearity.html#cb116-5" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;CV K = 10, Iterations = 20&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-84-2.png" width="720" style="display: block; margin: auto;" /></p>
<p>First we see the last iteration and the prediction error hereof. Overall we see that it tend to be the rather complex models tend to be</p>
</div>
</div>
<div id="exercise-10" class="section level3" number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> Exercise 10</h3>
<div id="a-partitioning-the-data" class="section level4" number="2.4.5.1">
<h4><span class="header-section-number">2.4.5.1</span> (a) Partitioning the data</h4>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="moving-beyond-linearity.html#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Loading</span></span>
<span id="cb117-2"><a href="moving-beyond-linearity.html#cb117-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> College</span>
<span id="cb117-3"><a href="moving-beyond-linearity.html#cb117-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-4"><a href="moving-beyond-linearity.html#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Partitioning</span></span>
<span id="cb117-5"><a href="moving-beyond-linearity.html#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb117-6"><a href="moving-beyond-linearity.html#cb117-6" aria-hidden="true" tabindex="-1"></a>train.size <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="at">x =</span> <span class="fu">nrow</span>(df)<span class="sc">*</span><span class="fl">0.8</span>,<span class="at">digits =</span> <span class="dv">0</span>) <span class="co">#Setting the training size</span></span>
<span id="cb117-7"><a href="moving-beyond-linearity.html#cb117-7" aria-hidden="true" tabindex="-1"></a>train.index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)),<span class="at">size =</span> train.size) <span class="co">#setting seed and creating vector for index</span></span>
<span id="cb117-8"><a href="moving-beyond-linearity.html#cb117-8" aria-hidden="true" tabindex="-1"></a>train.df <span class="ot">&lt;-</span> df[train.index,] <span class="co">#crating the training set</span></span>
<span id="cb117-9"><a href="moving-beyond-linearity.html#cb117-9" aria-hidden="true" tabindex="-1"></a>test.df <span class="ot">&lt;-</span> df[<span class="sc">-</span>train.index,] <span class="co">#creating the testing set</span></span>
<span id="cb117-10"><a href="moving-beyond-linearity.html#cb117-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(train.size)</span>
<span id="cb117-11"><a href="moving-beyond-linearity.html#cb117-11" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(train.index)</span></code></pre></div>
<p>Finding the best subset using forward selection</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="moving-beyond-linearity.html#cb118-1" aria-hidden="true" tabindex="-1"></a>reg_null <span class="ot">&lt;-</span> <span class="fu">lm</span>(Outstate <span class="sc">~</span> <span class="dv">1</span>,<span class="at">data =</span> train.df) <span class="co">#The null  models</span></span>
<span id="cb118-2"><a href="moving-beyond-linearity.html#cb118-2" aria-hidden="true" tabindex="-1"></a>reg_full <span class="ot">&lt;-</span> <span class="fu">lm</span>(Outstate <span class="sc">~</span> .,<span class="at">data =</span> train.df) <span class="co">#The full model</span></span>
<span id="cb118-3"><a href="moving-beyond-linearity.html#cb118-3" aria-hidden="true" tabindex="-1"></a>step.for <span class="ot">&lt;-</span> <span class="fu">stepAIC</span>(<span class="at">direction =</span> <span class="st">&quot;forward&quot;</span>,<span class="at">object =</span> reg_null,<span class="at">trace =</span> <span class="cn">TRUE</span>,<span class="at">scope =</span> <span class="fu">list</span>(<span class="at">upper =</span> reg_full,<span class="at">lower =</span> reg_null)) <span class="co">#This could also have been done with regsubsets()</span></span></code></pre></div>
<pre><code>## Start:  AIC=10345.68
## Outstate ~ 1
## 
##               Df  Sum of Sq         RSS     AIC
## + Expend       1 4629213116  5745572675  9980.1
## + Room.Board   1 4590249421  5784536370  9984.3
## + Grad.Rate    1 3627082940  6747702852 10080.1
## + Top10perc    1 3457381465  6917404327 10095.6
## + perc.alumni  1 3450011987  6924773804 10096.2
## + S.F.Ratio    1 3359826734  7014959058 10104.3
## + Private      1 3075173999  7299611793 10129.0
## + Top25perc    1 2644625673  7730160118 10164.7
## + Terminal     1 2115825269  8258960523 10205.8
## + PhD          1 1886906305  8487879487 10222.8
## + Personal     1  735337851  9639447941 10301.9
## + P.Undergrad  1  571871111  9802914681 10312.4
## + F.Undergrad  1  404016737  9970769054 10323.0
## + Enroll       1  180722874 10194062918 10336.7
## + Apps         1   52841694 10321944097 10344.5
## &lt;none&gt;                      10374785791 10345.7
## + Books        1   14584993 10360200799 10346.8
## + Accept       1      90338 10374695453 10347.7
## 
## Step:  AIC=9980.11
## Outstate ~ Expend
## 
##               Df  Sum of Sq        RSS    AIC
## + Private      1 1549696145 4195876531 9786.6
## + Room.Board   1 1512070540 4233502135 9792.1
## + Grad.Rate    1 1328830834 4416741842 9818.5
## + perc.alumni  1 1089768014 4655804661 9851.3
## + Personal     1  501146318 5244426358 9925.3
## + S.F.Ratio    1  464809224 5280763451 9929.6
## + F.Undergrad  1  454756490 5290816185 9930.8
## + P.Undergrad  1  349207380 5396365295 9943.1
## + Top10perc    1  346779721 5398792955 9943.4
## + Top25perc    1  345553704 5400018971 9943.5
## + Enroll       1  331862326 5413710349 9945.1
## + Terminal     1  269786468 5475786207 9952.2
## + PhD          1  210830779 5534741896 9958.9
## + Apps         1  110876800 5634695875 9970.0
## + Accept       1   85558833 5660013842 9972.8
## + Books        1   18850616 5726722060 9980.1
## &lt;none&gt;                      5745572675 9980.1
## 
## Step:  AIC=9786.59
## Outstate ~ Expend + Private
## 
##               Df Sum of Sq        RSS    AIC
## + Room.Board   1 876836109 3319040422 9642.8
## + Terminal     1 743790341 3452086190 9667.2
## + Grad.Rate    1 703122598 3492753933 9674.5
## + PhD          1 693220664 3502655866 9676.3
## + perc.alumni  1 408270963 3787605568 9724.9
## + Top25perc    1 401010315 3794866215 9726.1
## + Top10perc    1 319956878 3875919653 9739.3
## + Accept       1 152840390 4043036140 9765.5
## + Personal     1 128047157 4067829374 9769.3
## + Apps         1 118364448 4077512083 9770.8
## + Enroll       1  37847497 4158029034 9783.0
## + S.F.Ratio    1  28944091 4166932439 9784.3
## + F.Undergrad  1  16848576 4179027955 9786.1
## &lt;none&gt;                     4195876531 9786.6
## + Books        1   5437161 4190439370 9787.8
## + P.Undergrad  1   3721562 4192154968 9788.0
## 
## Step:  AIC=9642.78
## Outstate ~ Expend + Private + Room.Board
## 
##               Df Sum of Sq        RSS    AIC
## + perc.alumni  1 419740777 2899299645 9560.7
## + Grad.Rate    1 412477957 2906562465 9562.2
## + PhD          1 379305385 2939735036 9569.3
## + Terminal     1 369656216 2949384206 9571.3
## + Top25perc    1 311299881 3007740541 9583.5
## + Top10perc    1 280108774 3038931648 9589.9
## + Personal     1  84780679 3234259743 9628.7
## + Accept       1  42471882 3276568540 9636.8
## + Books        1  35245677 3283794745 9638.1
## + P.Undergrad  1  29148614 3289891808 9639.3
## + S.F.Ratio    1  27660641 3291379781 9639.6
## + Apps         1  24109291 3294931131 9640.2
## + Enroll       1  12561816 3306478606 9642.4
## &lt;none&gt;                     3319040422 9642.8
## + F.Undergrad  1   2021439 3317018983 9644.4
## 
## Step:  AIC=9560.68
## Outstate ~ Expend + Private + Room.Board + perc.alumni
## 
##               Df Sum of Sq        RSS    AIC
## + PhD          1 248319695 2650979950 9507.0
## + Terminal     1 241588269 2657711376 9508.6
## + Grad.Rate    1 194250957 2705048688 9519.5
## + Top25perc    1 146504265 2752795380 9530.4
## + Top10perc    1 124783671 2774515974 9535.3
## + Accept       1  68407796 2830891849 9547.8
## + Apps         1  37351605 2861948040 9554.6
## + Personal     1  22021761 2877277884 9557.9
## + Enroll       1  21250394 2878049251 9558.1
## + Books        1  16826748 2882472897 9559.1
## + S.F.Ratio    1  12022816 2887276829 9560.1
## &lt;none&gt;                     2899299645 9560.7
## + F.Undergrad  1   9250654 2890048991 9560.7
## + P.Undergrad  1   5862353 2893437292 9561.4
## 
## Step:  AIC=9506.99
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD
## 
##               Df Sum of Sq        RSS    AIC
## + Grad.Rate    1 138096695 2512883255 9475.7
## + Top25perc    1  42169169 2608810781 9499.0
## + Top10perc    1  35379421 2615600528 9500.6
## + Terminal     1  26636450 2624343499 9502.7
## + Personal     1  26014281 2624965669 9502.9
## + Accept       1  25856057 2625123892 9502.9
## + S.F.Ratio    1  20504248 2630475702 9504.2
## + P.Undergrad  1  15565029 2635414921 9505.3
## + Books        1  14472738 2636507212 9505.6
## + Apps         1  10845099 2640134851 9506.4
## &lt;none&gt;                     2650979950 9507.0
## + Enroll       1   1715151 2649264798 9508.6
## + F.Undergrad  1    235072 2650744878 9508.9
## 
## Step:  AIC=9475.71
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate
## 
##               Df Sum of Sq        RSS    AIC
## + Terminal     1  29069264 2483813991 9470.5
## + S.F.Ratio    1  23980164 2488903091 9471.7
## + Personal     1  15282065 2497601190 9473.9
## + Top25perc    1  14080081 2498803174 9474.2
## + Books        1  13787420 2499095834 9474.3
## + Top10perc    1  10584708 2502298546 9475.1
## + Accept       1  10459352 2502423903 9475.1
## &lt;none&gt;                     2512883255 9475.7
## + P.Undergrad  1   5510297 2507372958 9476.3
## + F.Undergrad  1   1533109 2511350146 9477.3
## + Apps         1    974955 2511908300 9477.5
## + Enroll       1     16562 2512866693 9477.7
## 
## Step:  AIC=9470.48
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal
## 
##               Df Sum of Sq        RSS    AIC
## + S.F.Ratio    1  21234904 2462579087 9467.1
## + Books        1  18033724 2465780267 9467.9
## + Personal     1  17438422 2466375568 9468.1
## + Top25perc    1  11061012 2472752979 9469.7
## + Top10perc    1  10503050 2473310941 9469.8
## + Accept       1   8692072 2475121919 9470.3
## &lt;none&gt;                     2483813991 9470.5
## + P.Undergrad  1   6212554 2477601437 9470.9
## + F.Undergrad  1   2755569 2481058422 9471.8
## + Apps         1    617630 2483196361 9472.3
## + Enroll       1     29501 2483784490 9472.5
## 
## Step:  AIC=9467.13
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio
## 
##               Df Sum of Sq        RSS    AIC
## + Personal     1  19556149 2443022938 9464.2
## + Books        1  17642529 2444936558 9464.7
## + Accept       1  12220653 2450358434 9466.0
## + Top25perc    1  10915780 2451663307 9466.4
## + Top10perc    1   9884752 2452694335 9466.6
## &lt;none&gt;                     2462579087 9467.1
## + P.Undergrad  1   5425216 2457153871 9467.8
## + Apps         1   1708616 2460870471 9468.7
## + F.Undergrad  1   1057807 2461521279 9468.9
## + Enroll       1    169448 2462409639 9469.1
## 
## Step:  AIC=9464.18
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio + Personal
## 
##               Df Sum of Sq        RSS    AIC
## + Accept       1  16268824 2426754113 9462.0
## + Books        1  12585987 2430436951 9463.0
## + Top25perc    1  11662715 2431360223 9463.2
## + Top10perc    1  10776478 2432246460 9463.4
## &lt;none&gt;                     2443022938 9464.2
## + Apps         1   3319537 2439703401 9465.3
## + P.Undergrad  1   2234491 2440788447 9465.6
## + Enroll       1   1504102 2441518836 9465.8
## + F.Undergrad  1     17762 2443005175 9466.2
## 
## Step:  AIC=9462.02
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio + Personal + Accept
## 
##               Df Sum of Sq        RSS    AIC
## + F.Undergrad  1  34267884 2392486229 9455.2
## + Apps         1  28349962 2398404152 9456.7
## + Enroll       1  21722919 2405031194 9458.4
## + Books        1  13764794 2412989319 9460.5
## + Top10perc    1   9007820 2417746293 9461.7
## + Top25perc    1   8657418 2418096695 9461.8
## + P.Undergrad  1   7813774 2418940340 9462.0
## &lt;none&gt;                     2426754113 9462.0
## 
## Step:  AIC=9455.17
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad
## 
##               Df Sum of Sq        RSS    AIC
## + Apps         1  33237860 2359248369 9448.5
## + Top10perc    1  14141658 2378344572 9453.5
## + Top25perc    1  13224939 2379261290 9453.7
## + Books        1  11896812 2380589418 9454.1
## &lt;none&gt;                     2392486229 9455.2
## + P.Undergrad  1    793001 2391693228 9457.0
## + Enroll       1     19024 2392467206 9457.2
## 
## Step:  AIC=9448.47
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad + 
##     Apps
## 
##               Df Sum of Sq        RSS    AIC
## + Top10perc    1  34199271 2325049099 9441.4
## + Top25perc    1  24042385 2335205985 9444.1
## + Books        1  10661970 2348586399 9447.7
## &lt;none&gt;                     2359248369 9448.5
## + Enroll       1   1072888 2358175482 9450.2
## + P.Undergrad  1    755069 2358493300 9450.3
## 
## Step:  AIC=9441.39
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad + 
##     Apps + Top10perc
## 
##               Df Sum of Sq        RSS    AIC
## + Books        1  15416176 2309632923 9439.3
## &lt;none&gt;                     2325049099 9441.4
## + Enroll       1   3260513 2321788586 9442.5
## + Top25perc    1    166941 2324882158 9443.3
## + P.Undergrad  1       256 2325048842 9443.4
## 
## Step:  AIC=9439.25
## Outstate ~ Expend + Private + Room.Board + perc.alumni + PhD + 
##     Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + F.Undergrad + 
##     Apps + Top10perc + Books
## 
##               Df Sum of Sq        RSS    AIC
## &lt;none&gt;                     2309632923 9439.3
## + Enroll       1   3152187 2306480736 9440.4
## + Top25perc    1    252598 2309380324 9441.2
## + P.Undergrad  1        53 2309632870 9441.3</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="moving-beyond-linearity.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(step.for)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Outstate ~ Expend + Private + Room.Board + perc.alumni + 
##     PhD + Grad.Rate + Terminal + S.F.Ratio + Personal + Accept + 
##     F.Undergrad + Apps + Top10perc + Books, data = train.df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -6206  -1233     10   1325   5390 
## 
## Coefficients:
##                Estimate  Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2286.93077   865.13003  -2.643 0.008418 ** 
## Expend          0.16360     0.02379   6.878 1.51e-11 ***
## PrivateYes   2242.59519   268.94651   8.338 5.06e-16 ***
## Room.Board      0.94499     0.09401  10.052  &lt; 2e-16 ***
## perc.alumni    42.13604     8.50315   4.955 9.38e-07 ***
## PhD            14.09387    10.16544   1.386 0.166118    
## Grad.Rate      28.68532     6.13106   4.679 3.56e-06 ***
## Terminal       32.16147    11.27001   2.854 0.004468 ** 
## S.F.Ratio     -61.88646    28.54270  -2.168 0.030531 *  
## Personal       -0.19357     0.13361  -1.449 0.147935    
## Accept          0.67907     0.13205   5.143 3.66e-07 ***
## F.Undergrad    -0.14101     0.03952  -3.568 0.000388 ***
## Apps           -0.28245     0.07533  -3.749 0.000194 ***
## Top10perc      23.14400     7.23340   3.200 0.001448 ** 
## Books          -1.00042     0.49701  -2.013 0.044572 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1951 on 607 degrees of freedom
## Multiple R-squared:  0.7774, Adjusted R-squared:  0.7722 
## F-statistic: 151.4 on 14 and 607 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>NOTE:</strong> Anaâs solution has a nice example with regsubsets, where she applies standard errors to see if the results of the different combinations are the same.</p>
<p>We see that the forward selection decides on 14 variables to be included</p>
</div>
<div id="b-fitting-a-gam" class="section level4" number="2.4.5.2">
<h4><span class="header-section-number">2.4.5.2</span> (b) Fitting a GAM</h4>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="moving-beyond-linearity.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb122-2"><a href="moving-beyond-linearity.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(forecast)</span>
<span id="cb122-3"><a href="moving-beyond-linearity.html#cb122-3" aria-hidden="true" tabindex="-1"></a>best.formula <span class="ot">&lt;-</span> <span class="fu">formula</span>(step.for)</span>
<span id="cb122-4"><a href="moving-beyond-linearity.html#cb122-4" aria-hidden="true" tabindex="-1"></a>fit.gam <span class="ot">&lt;-</span>gam<span class="sc">::</span><span class="fu">gam</span>(<span class="at">formula =</span> best.formula,<span class="at">data =</span> train.df) <span class="co">#Notice that it is linear</span></span>
<span id="cb122-5"><a href="moving-beyond-linearity.html#cb122-5" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="at">object =</span> fit.gam,<span class="at">newdata =</span> test.df)</span>
<span id="cb122-6"><a href="moving-beyond-linearity.html#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> preds,<span class="at">y =</span> preds<span class="sc">-</span>test.df<span class="sc">$</span>Outstate,<span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,<span class="at">xlab =</span> <span class="st">&quot;Predicted values&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-87-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>From the plot we see that in general we have a resdiausl around 0 withinn +/- 5000. Also the residuals do look rather normal. Although one could argue that the variance is a but smaller in the lower region of the predicted value, and it does in fact appear as if we are under estimating these result.</p>
<p>We can interprete how Outstate responds in the following:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="moving-beyond-linearity.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>))</span>
<span id="cb123-2"><a href="moving-beyond-linearity.html#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.gam <span class="co">#Note, automatically identifies the GAM object, hence plots for each variable</span></span>
<span id="cb123-3"><a href="moving-beyond-linearity.html#cb123-3" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">se =</span> <span class="cn">TRUE</span></span>
<span id="cb123-4"><a href="moving-beyond-linearity.html#cb123-4" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">col =</span><span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-88-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Interpreting the plot: Recall that the plots assumes that we hold the other variables fixed, hence we see the following:</p>
<ul>
<li>e.g., Expend: We see that holding the other variables fixed, the outstate tends to increase over the expenditure.</li>
<li>e.g., Apps: I assume that this is applicants, we see that holder the other variables fixed, outstate students tend to decrease as amount of applicants decrease.</li>
</ul>
<p>We can also assess the overall accuracy</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="moving-beyond-linearity.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(preds,<span class="at">x =</span> test.df<span class="sc">$</span>Outstate)</span></code></pre></div>
<pre><code>##                ME    RMSE      MAE       MPE     MAPE
## Test set 85.03267 2041.19 1587.582 -1.863903 16.86691</code></pre>
<p>We see that the MAE is 1587, where the mean absolute percentage error is almost 17%, hence it appear to be rather high.</p>
</div>
<div id="c-evaluating-on-the-test-set" class="section level4" number="2.4.5.3">
<h4><span class="header-section-number">2.4.5.3</span> (c) Evaluating on the test set</h4>
<p>This is what was done above. It is expected that if we compared applying on the train and test set, we will observe that the model has a lot of optimism on the train data, thus we should also see that the MAPE is lower on this partition, has this is what the model was trained on.</p>
</div>
<div id="d-which-variables-appear-to-have-a-non-linear-relationship" class="section level4" number="2.4.5.4">
<h4><span class="header-section-number">2.4.5.4</span> (d) Which variables appear to have a non linear relationship?</h4>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="moving-beyond-linearity.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb126-2"><a href="moving-beyond-linearity.html#cb126-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-3"><a href="moving-beyond-linearity.html#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>,<span class="dv">10</span><span class="sc">:</span><span class="dv">18</span>)) {</span>
<span id="cb126-4"><a href="moving-beyond-linearity.html#cb126-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">y =</span> df<span class="sc">$</span>Outstate,<span class="at">x =</span> df[,i],<span class="at">xlab =</span> <span class="fu">names</span>(df)[i],<span class="at">ylab =</span> <span class="st">&quot;Outstate&quot;</span>,<span class="at">pch =</span> <span class="dv">20</span>,<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb126-5"><a href="moving-beyond-linearity.html#cb126-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">grid</span>()</span>
<span id="cb126-6"><a href="moving-beyond-linearity.html#cb126-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(df)[i] <span class="sc">%&gt;%</span> <span class="fu">title</span>()</span>
<span id="cb126-7"><a href="moving-beyond-linearity.html#cb126-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-90-1.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-90-2.png" width="720" style="display: block; margin: auto;" /></p>
<p>It appears as if expend has som non linear relationship with Outstate. Perhaps enroll, F.Undergrad, and P.Undergrad also have a non linear trend.</p>
<p>To further decide if there is evidence for a non linear relationship, one could make, e.g., a smoothing model to assess the performance hereof.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="moving-beyond-linearity.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1337</span>)</span>
<span id="cb127-2"><a href="moving-beyond-linearity.html#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span>
<span id="cb127-3"><a href="moving-beyond-linearity.html#cb127-3" aria-hidden="true" tabindex="-1"></a>gam.mgcv <span class="ot">&lt;-</span> mgcv<span class="sc">::</span><span class="fu">gam</span>(Outstate <span class="sc">~</span> Private <span class="sc">+</span> <span class="fu">s</span>(Room.Board) <span class="sc">+</span> <span class="fu">s</span>(PhD) <span class="sc">+</span> <span class="fu">s</span>(perc.alumni) <span class="sc">+</span> <span class="fu">s</span>(Expend) <span class="sc">+</span> <span class="fu">s</span>(Grad.Rate), <span class="at">data =</span> train.df,<span class="at">method =</span> <span class="st">&#39;REML&#39;</span>)</span>
<span id="cb127-4"><a href="moving-beyond-linearity.html#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.mgcv)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## Outstate ~ Private + s(Room.Board) + s(PhD) + s(perc.alumni) + 
##     s(Expend) + s(Grad.Rate)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   8775.6      175.3   50.05   &lt;2e-16 ***
## PrivateYes    2370.6      217.5   10.90   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##                  edf Ref.df     F   p-value    
## s(Room.Board)  2.318  2.953 25.81   &lt; 2e-16 ***
## s(PhD)         1.258  1.478 11.03  0.000303 ***
## s(perc.alumni) 1.677  2.115 10.98 0.0000173 ***
## s(Expend)      5.727  6.882 31.37   &lt; 2e-16 ***
## s(Grad.Rate)   2.816  3.593 12.85   &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.794   Deviance explained = 79.9%
## -REML = 5531.6  Scale est. = 3.446e+06  n = 622</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="moving-beyond-linearity.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We look at the Approximate significance of smooth terms table, in particular to edf.</span></span>
<span id="cb129-2"><a href="moving-beyond-linearity.html#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="co"># These edfÂ´s suggests that the previous gam model (imposing all nonlinear) may be a little too restrictive </span></span>
<span id="cb129-3"><a href="moving-beyond-linearity.html#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb129-4"><a href="moving-beyond-linearity.html#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.mgcv, <span class="at">se =</span> T, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb129-5"><a href="moving-beyond-linearity.html#cb129-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check residuals diagnostics</span></span>
<span id="cb129-6"><a href="moving-beyond-linearity.html#cb129-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-91-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="moving-beyond-linearity.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gam.check</span>(gam.mgcv)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-91-2.png" width="720" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Method: REML   Optimizer: outer newton
## full convergence after 5 iterations.
## Gradient range [-0.001152703,0.003980755]
## (score 5531.561 &amp; scale 3445952).
## Hessian positive definite, eigenvalue range [0.01619762,307.5188].
## Model rank =  47 / 47 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##                  k&#39;  edf k-index p-value  
## s(Room.Board)  9.00 2.32    0.98   0.290  
## s(PhD)         9.00 1.26    0.94   0.075 .
## s(perc.alumni) 9.00 1.68    1.07   0.955  
## s(Expend)      9.00 5.73    1.08   0.970  
## s(Grad.Rate)   9.00 2.82    0.95   0.120  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
</div>
<div id="FacebookCasestudy" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Casestudy - Predicting the Return on Advertising Spent</h2>
<p>(Case and data source: kaggle.com)</p>
<p><br />
</p>
<div id="background" class="section level3" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> 1. Background</h3>
<p>Along with Googleâs search and display networks, Facebook is one of the big players when it comes to online advertising. As Facebook users interact with the platform, adding demographic information, liking particular pages and commenting on specific posts, Facebook builds a profile of that user based on who they are and what theyâre interested in.</p>
<p>This fact makes Facebook very attractive for advertisers. Advertisers can create Facebook adverts, then create an âAudienceâ for that advert or group of adverts. Audiences can be built from a range of attributes including gender, age, location and interests. This specific targeting means advertisers can tailor content appropriately for a specific audience, even if the product being marketed is the same.</p>
<p>For example, letâs imagine a company wants to advertise its new car. They may wish to promote one set of features, performance and the 2 kW stereo, to women in their early twenties. They might decide that they want to talk about itâs fuel efficiency and reduced emissions to men in their thirties, and they might want to push the spacious interior and safety rating to men and women in their thirties and early forties who are interested in Families magazine and who like pages of nappy and baby clothes manufacturers.</p>
<p>In 2016, Facebookâs revenue from advertising was <strong>26bn, up from 17bn</strong> the year before. This compares to Googleâs <strong>79bn, the </strong>638m that Twitter advertising made in Q4 2016, and $173m that LinkedIn made from ads in Q3 2016. These figures illustrate just how big an advertising platform is, although it faces challenges for the future with a decline in younger users in 2017, with generation Z moving to Snapchat and Instagram. When it comes to analyzing the Facebook adverts dataset, there are a lot of questions we can ask, and a lot of insight we can generate. However, from a business perspective we want to ask questions that will give us answers we can use to improve business performance.</p>
<p>Â 
Â </p>
</div>
<div id="case-study-business-understanding-phase" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> 2. Case study (Business Understanding Phase)</h3>
<p>The company xyz is an e-marketing company. It earns money based on how many people click the ads and how many people actually purchase the product advertised. The company will receive from the manufacturer <strong>5 for each user that clicks on ad and signs up in the e-commerce shop, and respectively, </strong>100 for each registered user that buys the product. On the other hand, the xyz pays to Facebook to display the ads. The objective of the company xyz is to maximize the return on advertising spent, while minimizing the amount spent on advertising. In order to attain this objective, the company needs to identify the factors (e.g.Â spending, campaign type, customer type, etc.) influencing most significantly the return on advertising spent (ROAS) based on the available data.</p>
<p>The analytical objective in this case is to predict the Return on Advertising Spent (ROAS) using the dataset KAG_conversion_data.csv. Given the scope of this case study, we aim to set up a non-linear regression model. The model can be used either to predict the value of the target variable, ROAS, for future observations of the explanatory variables or to provide a better understanding of the relationships (form and direction) between dependent and independent variables.</p>
<p>Â 
Â </p>
</div>
<div id="the-data-data-understanding-phase" class="section level3" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> 3. The data (Data Understanding Phase)</h3>
<p>The documentation describes the columns in the data as follows:</p>
<ol style="list-style-type: decimal">
<li>ad_id: unique ID for each ad. It is just an identifier.</li>
<li>xyz_campaign_id: an ID associated with each ad campaign of XYZ company</li>
<li>fb_campaign_id: an ID associated with how Facebook tracks each campaign. Ignore this variable in the analysis</li>
<li>age: age of the person to whom the ad is shown</li>
<li>gender: gender of the person to whom the add is shown</li>
<li>interest: a code specifying the category to which the personâs interest belongs (interests are as mentioned in the personâs Facebook public profile)</li>
<li>Impressions: the number of times the ad was shown</li>
<li>Clicks: number of clicks on for that ad</li>
<li>Spent: Amount paid by company xyz to Facebook, to show that ad</li>
<li>Total conversion: Total number of people who sign up to the webshop after seeing the ad (here we (the add company) receives 5$)</li>
<li>Approved conversion/leads: Total number of people who bought the product after seeing the ad (here we (the add company) receives 10$)</li>
</ol>
<p><strong>We want to predict ROAS, that is return on advertising spending</strong></p>
<p>Â 
Â </p>
</div>
<div id="specific-requirements" class="section level3" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> 4. Specific requirements:</h3>
<p>Â 
Â </p>
<div id="task-1---import-and-overview" class="section level4" number="2.5.4.1">
<h4><span class="header-section-number">2.5.4.1</span> 4.1 Task 1 - Import and overview</h4>
<p>Import and view the data. A critical step in the data mining process is to understand the variables, and further to ensure that the software interprets the variables in the correct way.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="moving-beyond-linearity.html#cb132-1" aria-hidden="true" tabindex="-1"></a>KAG_conversion_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;Data/1. MovingBeyondLinearity/FacebookCasestudy/KAG_conversion_data.csv&quot;</span>)</span></code></pre></div>
<p>Â 
Â </p>
</div>
<div id="task-2---data-inspection" class="section level4" number="2.5.4.2">
<h4><span class="header-section-number">2.5.4.2</span> 4.2 Task 2 - Data inspection</h4>
<p>Inspect your data and do the required variable adaptations and transformations. These may include: treating the missing data, treating the outliers, log transformations, binning, standardizing, and creating additional features (e.g.Â in this particular case, one needs to calculate the DV (ROAS), which is not ready available in the dataset).</p>
<p><strong>NOTICE for the exam, we will not be asked to transform the data. Or at least the will specify what to do</strong></p>
<p>In real practice, data preprocessing takes about 75% of the total time assigned to a project.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="moving-beyond-linearity.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb133-2"><a href="moving-beyond-linearity.html#cb133-2" aria-hidden="true" tabindex="-1"></a>dataTf <span class="ot">&lt;-</span> KAG_conversion_data</span>
<span id="cb133-3"><a href="moving-beyond-linearity.html#cb133-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(KAG_conversion_data) <span class="co">#Just a quick inspection</span></span></code></pre></div>
<pre><code>## Rows: 1,143
## Columns: 11
## $ ad_id               &lt;int&gt; 708746, 708749, 708771, 708815, 708818, 708820, 70â¦
## $ xyz_campaign_id     &lt;int&gt; 916, 916, 916, 916, 916, 916, 916, 916, 916, 916, â¦
## $ fb_campaign_id      &lt;int&gt; 103916, 103917, 103920, 103928, 103928, 103929, 10â¦
## $ age                 &lt;chr&gt; &quot;30-34&quot;, &quot;30-34&quot;, &quot;30-34&quot;, &quot;30-34&quot;, &quot;30-34&quot;, &quot;30-3â¦
## $ gender              &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, â¦
## $ interest            &lt;int&gt; 15, 16, 20, 28, 28, 29, 15, 16, 27, 28, 31, 7, 16,â¦
## $ Impressions         &lt;int&gt; 7350, 17861, 693, 4259, 4133, 1915, 15615, 10951, â¦
## $ Clicks              &lt;int&gt; 1, 2, 0, 1, 1, 0, 3, 1, 1, 3, 0, 0, 0, 0, 7, 0, 1,â¦
## $ Spent               &lt;dbl&gt; 1.43, 1.82, 0.00, 1.25, 1.29, 0.00, 4.77, 1.27, 1.â¦
## $ Total_Conversion    &lt;int&gt; 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,â¦
## $ Approved_Conversion &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,â¦</code></pre>
<p>Renaming variables</p>
<p><em>In general it is a good idea not to have special charactors in the variable name.</em></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="moving-beyond-linearity.html#cb135-1" aria-hidden="true" tabindex="-1"></a>dataTf <span class="ot">&lt;-</span> dataTf <span class="sc">%&gt;%</span> </span>
<span id="cb135-2"><a href="moving-beyond-linearity.html#cb135-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">xyzCampId =</span> xyz_campaign_id, <span class="at">fbCampId =</span> fb_campaign_id, <span class="at">impr =</span> Impressions,</span>
<span id="cb135-3"><a href="moving-beyond-linearity.html#cb135-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">conv =</span> Total_Conversion, <span class="at">appConv =</span> Approved_Conversion)</span></code></pre></div>
<p>Create new features and calculate ROAS</p>
<p><em>Notice, that we use <code>mutate()</code> to make the new variables</em></p>
<ul>
<li>Click-through-rate: how many of the impressions became clicks.CTR = Clicks/Impressions</li>
<li>Cost Per Click: how much on average did each click cost.CPC = Spent/Clicks</li>
</ul>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="moving-beyond-linearity.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb136-2"><a href="moving-beyond-linearity.html#cb136-2" aria-hidden="true" tabindex="-1"></a>dataTf <span class="ot">&lt;-</span> dataTf <span class="sc">%&gt;%</span>  </span>
<span id="cb136-3"><a href="moving-beyond-linearity.html#cb136-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">CTR =</span> ((Clicks <span class="sc">/</span> impr) <span class="sc">*</span> <span class="dv">100</span>), <span class="at">CPC =</span> Spent <span class="sc">/</span> Clicks) </span>
<span id="cb136-4"><a href="moving-beyond-linearity.html#cb136-4" aria-hidden="true" tabindex="-1"></a>dataTf<span class="sc">$</span>CTR <span class="ot">&lt;-</span> <span class="fu">round</span>(dataTf<span class="sc">$</span>CTR, <span class="dv">4</span>) <span class="co">#NB: Sometime rounding is necessary for packages to work</span></span>
<span id="cb136-5"><a href="moving-beyond-linearity.html#cb136-5" aria-hidden="true" tabindex="-1"></a>dataTf<span class="sc">$</span>CPC <span class="ot">&lt;-</span> <span class="fu">round</span>(dataTf<span class="sc">$</span>CPC, <span class="dv">2</span>)</span></code></pre></div>
<p>Now we can also make the following variables:</p>
<ul>
<li>Conversion value: conval = 5 * conv</li>
<li>Approved conversion value: appConVal = 100 * appConv</li>
<li>Total conversion: totConVal = conval + appConVal</li>
<li>Cost per conversion: costPerCon = Spent/totConv</li>
<li>Return on Advertising Spend (the revenue as a % of advertising spend) : ROAS = totConVal/Spent</li>
<li>Cost Per Mille: the cost of one thousand impressions. If our objective is ad exposure to increase brand awareness,this might be an important to measure. CPM = Spent/Impr *1000. I.e. how much of the overall spenditure divided by the amount of impressions. We multiply with 1000, to have more readable numbers instead of decimals.</li>
</ul>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="moving-beyond-linearity.html#cb137-1" aria-hidden="true" tabindex="-1"></a>dataTf <span class="ot">&lt;-</span> dataTf <span class="sc">%&gt;%</span></span>
<span id="cb137-2"><a href="moving-beyond-linearity.html#cb137-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">totConv =</span> conv <span class="sc">+</span> appConv,</span>
<span id="cb137-3"><a href="moving-beyond-linearity.html#cb137-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">conVal =</span> conv <span class="sc">*</span> <span class="dv">5</span>,</span>
<span id="cb137-4"><a href="moving-beyond-linearity.html#cb137-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">appConVal =</span> appConv <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">%&gt;%</span></span>
<span id="cb137-5"><a href="moving-beyond-linearity.html#cb137-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">totConVal =</span> conVal <span class="sc">+</span> appConVal) <span class="sc">%&gt;%</span></span>
<span id="cb137-6"><a href="moving-beyond-linearity.html#cb137-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">costPerCon =</span> <span class="fu">round</span>(Spent <span class="sc">/</span> totConv, <span class="dv">2</span>),</span>
<span id="cb137-7"><a href="moving-beyond-linearity.html#cb137-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">ROAS =</span> <span class="fu">round</span>(totConVal <span class="sc">/</span> Spent, <span class="dv">2</span>))</span>
<span id="cb137-8"><a href="moving-beyond-linearity.html#cb137-8" aria-hidden="true" tabindex="-1"></a>dataTf <span class="ot">&lt;-</span> dataTf <span class="sc">%&gt;%</span></span>
<span id="cb137-9"><a href="moving-beyond-linearity.html#cb137-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">CPM =</span> <span class="fu">round</span>((Spent <span class="sc">/</span> impr) <span class="sc">*</span> <span class="dv">1000</span>, <span class="dv">2</span>)) <span class="co">#Cost per Mille.</span></span></code></pre></div>
<p>We see that this produce a lot of INF, that is because some of the underlying varaibles contain NaNs, which is regarded as non available values, hence we cant do math with it. These observations will be identified in the following.</p>
<p>Hence: Dealing with missing, currupt and invalid data</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="moving-beyond-linearity.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Decide between missing values imputation or deletion </span></span>
<span id="cb138-2"><a href="moving-beyond-linearity.html#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">is.na</span>(dataTf)) </span></code></pre></div>
<pre><code>## 
## FALSE  TRUE 
## 22647   213</code></pre>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="moving-beyond-linearity.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dataTf) </span></code></pre></div>
<pre><code>##      ad_id           xyzCampId       fbCampId          age           
##  Min.   : 708746   Min.   : 916   Min.   :103916   Length:1143       
##  1st Qu.: 777632   1st Qu.: 936   1st Qu.:115716   Class :character  
##  Median :1121185   Median :1178   Median :144549   Mode  :character  
##  Mean   : 987261   Mean   :1067   Mean   :133784                     
##  3rd Qu.:1121804   3rd Qu.:1178   3rd Qu.:144658                     
##  Max.   :1314415   Max.   :1178   Max.   :179982                     
##                                                                      
##     gender             interest           impr             Clicks      
##  Length:1143        Min.   :  2.00   Min.   :     87   Min.   :  0.00  
##  Class :character   1st Qu.: 16.00   1st Qu.:   6504   1st Qu.:  1.00  
##  Mode  :character   Median : 25.00   Median :  51509   Median :  8.00  
##                     Mean   : 32.77   Mean   : 186732   Mean   : 33.39  
##                     3rd Qu.: 31.00   3rd Qu.: 221769   3rd Qu.: 37.50  
##                     Max.   :114.00   Max.   :3052003   Max.   :421.00  
##                                                                        
##      Spent             conv           appConv            CTR         
##  Min.   :  0.00   Min.   : 0.000   Min.   : 0.000   Min.   :0.00000  
##  1st Qu.:  1.48   1st Qu.: 1.000   1st Qu.: 0.000   1st Qu.:0.01005  
##  Median : 12.37   Median : 1.000   Median : 1.000   Median :0.01600  
##  Mean   : 51.36   Mean   : 2.856   Mean   : 0.944   Mean   :0.01642  
##  3rd Qu.: 60.02   3rd Qu.: 3.000   3rd Qu.: 1.000   3rd Qu.:0.02340  
##  Max.   :639.95   Max.   :60.000   Max.   :21.000   Max.   :0.10590  
##                                                                      
##       CPC           totConv         conVal         appConVal     
##  Min.   :0.180   Min.   : 0.0   Min.   :  0.00   Min.   :   0.0  
##  1st Qu.:1.390   1st Qu.: 1.0   1st Qu.:  5.00   1st Qu.:   0.0  
##  Median :1.500   Median : 2.0   Median :  5.00   Median : 100.0  
##  Mean   :1.499   Mean   : 3.8   Mean   : 14.28   Mean   :  94.4  
##  3rd Qu.:1.643   3rd Qu.: 4.0   3rd Qu.: 15.00   3rd Qu.: 100.0  
##  Max.   :2.210   Max.   :77.0   Max.   :300.00   Max.   :2100.0  
##  NA&#39;s   :207                                                     
##    totConVal        costPerCon          ROAS             CPM        
##  Min.   :   0.0   Min.   : 0.000   Min.   : 0.000   Min.   :0.0000  
##  1st Qu.:   5.0   1st Qu.: 1.048   1st Qu.: 0.800   1st Qu.:0.1500  
##  Median : 105.0   Median : 6.290   Median : 3.025   Median :0.2500  
##  Mean   : 108.7   Mean   :   Inf   Mean   :   Inf   Mean   :0.2392  
##  3rd Qu.: 110.0   3rd Qu.:17.415   3rd Qu.:33.097   3rd Qu.:0.3300  
##  Max.   :2300.0   Max.   :   Inf   Max.   :   Inf   Max.   :1.5000  
##                   NA&#39;s   :3        NA&#39;s   :3</code></pre>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="moving-beyond-linearity.html#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DataExplorer) </span>
<span id="cb142-2"><a href="moving-beyond-linearity.html#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_missing</span>(dataTf)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-98-1.png" width="720" style="display: block; margin: auto;" /></p>
<p><strong>Genereal rule of thumb, if we have less than 5%, we can exclude them, if more than 5%, then we should impute values, so we preserve the dataset. One must make aware of this</strong></p>
<p>We just choose to ommit despite missing 18% in CPC</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="moving-beyond-linearity.html#cb143-1" aria-hidden="true" tabindex="-1"></a>dataTfo <span class="ot">=</span> <span class="fu">na.omit</span>(dataTf) <span class="co">#Df with the NA&#39;s omitted</span></span>
<span id="cb143-2"><a href="moving-beyond-linearity.html#cb143-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-3"><a href="moving-beyond-linearity.html#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividing by zero create Inf values</span></span>
<span id="cb143-4"><a href="moving-beyond-linearity.html#cb143-4" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>costPerCon <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(dataTfo<span class="sc">$</span>costPerCon <span class="sc">==</span> <span class="cn">Inf</span>, <span class="dv">0</span>, dataTfo<span class="sc">$</span>costPerCon) </span>
<span id="cb143-5"><a href="moving-beyond-linearity.html#cb143-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dataTfo<span class="sc">$</span>costPerCon)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   3.072   9.215  16.131  19.773 332.990</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="moving-beyond-linearity.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-check the correctness of data type and adapt</span></span>
<span id="cb145-2"><a href="moving-beyond-linearity.html#cb145-2" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>xyzCampId<span class="ot">=</span><span class="fu">as.factor</span>(dataTfo<span class="sc">$</span>xyzCampId)</span>
<span id="cb145-3"><a href="moving-beyond-linearity.html#cb145-3" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>age<span class="ot">=</span><span class="fu">as.factor</span>(dataTfo<span class="sc">$</span>age)</span>
<span id="cb145-4"><a href="moving-beyond-linearity.html#cb145-4" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>gender<span class="ot">=</span><span class="fu">as.factor</span>(dataTfo<span class="sc">$</span>gender)</span>
<span id="cb145-5"><a href="moving-beyond-linearity.html#cb145-5" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>interest<span class="ot">=</span><span class="fu">as.factor</span>(dataTfo<span class="sc">$</span>interest) <span class="co"># too many (non-representative) categories?</span></span>
<span id="cb145-6"><a href="moving-beyond-linearity.html#cb145-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-7"><a href="moving-beyond-linearity.html#cb145-7" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>appConv<span class="ot">=</span> <span class="fu">as.numeric</span>(dataTfo<span class="sc">$</span>appConv)</span>
<span id="cb145-8"><a href="moving-beyond-linearity.html#cb145-8" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>conv<span class="ot">=</span> <span class="fu">as.numeric</span>(dataTfo<span class="sc">$</span>conv)</span>
<span id="cb145-9"><a href="moving-beyond-linearity.html#cb145-9" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>impr<span class="ot">=</span> <span class="fu">as.numeric</span>(dataTfo<span class="sc">$</span>impr)</span>
<span id="cb145-10"><a href="moving-beyond-linearity.html#cb145-10" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>Clicks<span class="ot">=</span> <span class="fu">as.numeric</span>(dataTfo<span class="sc">$</span>Clicks)</span>
<span id="cb145-11"><a href="moving-beyond-linearity.html#cb145-11" aria-hidden="true" tabindex="-1"></a>dataTfo<span class="sc">$</span>totConv<span class="ot">=</span> <span class="fu">as.numeric</span>(dataTfo<span class="sc">$</span>totConv)</span>
<span id="cb145-12"><a href="moving-beyond-linearity.html#cb145-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-13"><a href="moving-beyond-linearity.html#cb145-13" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(dataTfo)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    936 obs. of  20 variables:
##  $ ad_id     : int  708746 708749 708815 708818 708889 708895 708953 708958 709059 709115 ...
##  $ xyzCampId : Factor w/ 3 levels &quot;916&quot;,&quot;936&quot;,&quot;1178&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ fbCampId  : int  103916 103917 103928 103928 103940 103941 103951 103952 103968 103978 ...
##  $ age       : Factor w/ 4 levels &quot;30-34&quot;,&quot;35-39&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ gender    : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ interest  : Factor w/ 40 levels &quot;2&quot;,&quot;7&quot;,&quot;10&quot;,&quot;15&quot;,..: 4 5 16 16 4 5 15 16 8 18 ...
##  $ impr      : num  7350 17861 4259 4133 15615 ...
##  $ Clicks    : num  1 2 1 1 3 1 1 3 7 1 ...
##  $ Spent     : num  1.43 1.82 1.25 1.29 4.77 ...
##  $ conv      : num  2 2 1 1 1 1 1 1 1 1 ...
##  $ appConv   : num  1 0 0 1 0 1 0 0 1 0 ...
##  $ CTR       : num  0.0136 0.0112 0.0235 0.0242 0.0192 0.0091 0.0425 0.0316 0.0477 0.0434 ...
##  $ CPC       : num  1.43 0.91 1.25 1.29 1.59 1.27 1.5 1.05 1.47 0.57 ...
##  $ totConv   : num  3 2 1 2 1 2 1 1 2 1 ...
##  $ conVal    : num  10 10 5 5 5 5 5 5 5 5 ...
##  $ appConVal : num  100 0 0 100 0 100 0 0 100 0 ...
##  $ totConVal : num  110 10 5 105 5 105 5 5 105 5 ...
##  $ costPerCon: num  0.48 0.91 1.25 0.64 4.77 0.63 1.5 3.16 5.14 0.57 ...
##  $ ROAS      : num  76.92 5.49 4 81.4 1.05 ...
##  $ CPM       : num  0.19 0.1 0.29 0.31 0.31 0.12 0.64 0.33 0.7 0.25 ...
##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:207] 3 6 11 12 13 14 16 18 21 25 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:207] &quot;3&quot; &quot;6&quot; &quot;11&quot; &quot;12&quot; ...</code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="moving-beyond-linearity.html#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check distribution of variables (univariate analysis) </span></span>
<span id="cb147-2"><a href="moving-beyond-linearity.html#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DataExplorer) </span>
<span id="cb147-3"><a href="moving-beyond-linearity.html#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">repr.plot.width=</span><span class="dv">4</span>, <span class="at">repr.plot.height=</span><span class="dv">4</span>)</span>
<span id="cb147-4"><a href="moving-beyond-linearity.html#cb147-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_bar</span>(dataTfo) <span class="co">#This is a really good tool to plot all bars</span></span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-101-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We can also check all of the distributions.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="moving-beyond-linearity.html#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_histogram</span>(dataTfo)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-102-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Now we can treat outliers.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="moving-beyond-linearity.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Treat outliers</span></span>
<span id="cb149-2"><a href="moving-beyond-linearity.html#cb149-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(dataTfo)</span>
<span id="cb149-3"><a href="moving-beyond-linearity.html#cb149-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># e.g. for ROAS</span></span>
<span id="cb149-4"><a href="moving-beyond-linearity.html#cb149-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">boxplot</span>(ROAS)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-103-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We want to see the boxplot being like a plot.</p>
<p>We observe that some of the observations are outside of the whiskers, these appear to be outliers. We can identify these with the $out command, see the following.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="moving-beyond-linearity.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(ROAS)<span class="sc">$</span>out</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-104-1.png" width="720" style="display: block; margin: auto;" /></p>
<pre><code>##   [1]  76.92  81.40  82.68  23.76  34.31  35.23  62.13  12.40  68.63  65.22
##  [11]  37.50  97.35  18.32  18.65  19.23  71.92  36.97  77.78  68.63  15.52
##  [21]  75.54  66.88  81.40  74.47  31.44  20.15  20.83  66.46  36.21 122.09
##  [31]  19.23  19.13  16.67  91.30  33.02 107.14  70.95  35.71  22.83  44.49
##  [41]  28.23  16.56  23.97 214.29  62.50  31.72  21.04  34.37 184.21  58.01
##  [51]  88.98  23.26  76.09  29.33  70.00 583.33  19.48 106.06  19.89 100.00
##  [61]  78.36  57.38  23.18  33.98  14.59  39.62  25.42 145.83  62.50  29.33
##  [71]  38.18  60.69  26.58  78.95  73.94  85.37  80.29  76.09  17.77  25.74
##  [81]  22.98  64.02  85.37  72.92  38.46  45.26  17.02  22.27  16.56  23.57
##  [91]  67.31  34.27  36.97  48.50  70.00  25.75  68.18  25.64  32.31  66.46
## [101]  41.98 132.08  33.33  12.20  69.54  67.31  69.18  13.62  13.92  52.38
## [111]  14.85  21.40  14.10  37.10  15.80  63.25  26.16  19.49  18.42  35.29
## [121]  27.85  13.62  13.32  21.78  24.88  23.03  60.69  40.08  37.77  39.92
## [131]  25.93  18.75  55.78  40.89  35.94  35.47  12.69  70.47  14.46  18.81
## [141]  12.22</code></pre>
<p>We see that all of these above are outliers.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="moving-beyond-linearity.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here I assign outliers to a vector and remove them</span></span>
<span id="cb152-2"><a href="moving-beyond-linearity.html#cb152-2" aria-hidden="true" tabindex="-1"></a>  outliers <span class="ot">&lt;-</span> <span class="fu">boxplot</span>(ROAS</span>
<span id="cb152-3"><a href="moving-beyond-linearity.html#cb152-3" aria-hidden="true" tabindex="-1"></a>                      ,<span class="at">plot=</span><span class="cn">FALSE</span>)<span class="sc">$</span>out <span class="co">#PLOT = TRUE will plot the boxplot</span></span>
<span id="cb152-4"><a href="moving-beyond-linearity.html#cb152-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-5"><a href="moving-beyond-linearity.html#cb152-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove the rows containing the outliers</span></span>
<span id="cb152-6"><a href="moving-beyond-linearity.html#cb152-6" aria-hidden="true" tabindex="-1"></a>  dataTfo1 <span class="ot">&lt;-</span> dataTfo[<span class="sc">-</span><span class="fu">which</span>(ROAS <span class="sc">%in%</span> outliers),] </span>
<span id="cb152-7"><a href="moving-beyond-linearity.html#cb152-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">boxplot</span>(dataTfo1<span class="sc">$</span>ROAS)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-105-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="moving-beyond-linearity.html#cb153-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Alterative ways to treat outliers exist </span></span></code></pre></div>
<p>Naturally there are other ways of identifying outliers, this is just one approach.</p>
<p>Looking at the new boxplot, we see that most observations are within the whiskers (the lines).</p>
<p><em>Notice, it does not have to be errors in the data and thus it does not necessarily be non representing of the data, thus one may not want to delete them</em></p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="moving-beyond-linearity.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check bivariate distributions or measures of association </span></span>
<span id="cb154-2"><a href="moving-beyond-linearity.html#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pairs(data) # only numeric</span></span>
<span id="cb154-3"><a href="moving-beyond-linearity.html#cb154-3" aria-hidden="true" tabindex="-1"></a><span class="co"># or one by one</span></span>
<span id="cb154-4"><a href="moving-beyond-linearity.html#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataTfo1, <span class="fu">aes</span>(<span class="at">x=</span>xyzCampId, <span class="at">y=</span>ROAS)) <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="at">geom=</span><span class="st">&quot;bar&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-106-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="moving-beyond-linearity.html#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataTfo1, <span class="fu">aes</span>(<span class="at">x=</span>age, <span class="at">y=</span>ROAS)) <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="at">geom=</span><span class="st">&quot;bar&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-107-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="moving-beyond-linearity.html#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataTfo1, <span class="fu">aes</span>(<span class="at">x=</span>gender, <span class="at">y=</span>ROAS)) <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="at">geom=</span><span class="st">&quot;bar&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-108-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="moving-beyond-linearity.html#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataTfo1, <span class="fu">aes</span>(<span class="at">x=</span>interest, <span class="at">y=</span>ROAS)) <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun.y=</span><span class="st">&quot;mean&quot;</span>, <span class="at">geom=</span><span class="st">&quot;bar&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-109-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="moving-beyond-linearity.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataTfo1, <span class="fu">aes</span>(Spent, ROAS)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Amount spent on campaign&quot;</span>, <span class="at">y =</span> <span class="st">&quot;ROAS&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-110-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="moving-beyond-linearity.html#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">repr.plot.width=</span><span class="dv">6</span>, <span class="at">repr.plot.height=</span><span class="dv">3</span>)</span>
<span id="cb159-2"><a href="moving-beyond-linearity.html#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataTfo1, <span class="fu">aes</span>(Spent, totConv)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Amount spent on campaign&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Total Conversions&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-111-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="moving-beyond-linearity.html#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure of associations </span></span>
<span id="cb160-2"><a href="moving-beyond-linearity.html#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb160-3"><a href="moving-beyond-linearity.html#cb160-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb160-4"><a href="moving-beyond-linearity.html#cb160-4" aria-hidden="true" tabindex="-1"></a>cormatrix <span class="ot">=</span> <span class="fu">corrplot</span>(<span class="fu">cor</span>(dataTfo1 [, <span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)]))</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-112-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>We seee that some variables appear to have a strong correlation with other variables, e.g.Â CTR and CPM.</p>
<p>We want to see how they interact with ROAS as well, to get an idea of wether they are positive and negative relationship.</p>
<p><strong><em>NOTICE, this is based on a linear relationship between the variables. Since we are now in non linear models, we cannot use it the same way as when working in the linear scenarios. But we must be aware of variables that appear to have a really high correlation with one and other, hence we may exclude these.</em></strong></p>
<p>The following is the same, but presented just with actual correlations.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="moving-beyond-linearity.html#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(dataTfo1[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)],dataTfo1<span class="sc">$</span>ROAS)</span></code></pre></div>
<pre><code>##                   [,1]
## impr       -0.12495737
## Clicks     -0.19118476
## Spent      -0.18289935
## conv        0.09806281
## appConv     0.27926699
## CTR        -0.11644181
## CPC        -0.08201179
## totConv     0.15227797
## conVal      0.09806281
## appConVal   0.27926699
## totConVal   0.26170129
## costPerCon -0.37974060
## ROAS        1.00000000
## CPM        -0.17622449</code></pre>
<p>Â 
Â </p>
<p>Other exploration techniques can be implemented. Data pre-processing remains an active area of research because of the huge amount of inconsistent or âdirtyâ data.</p>
<p>Â 
Â </p>
</div>
<div id="task-3---building-different-models" class="section level4" number="2.5.4.3">
<h4><span class="header-section-number">2.5.4.3</span> 4.3 Task 3 - Building different models</h4>
<p>Build several predictive models and evaluate their performance. First, consider reflecting about these specific questions:</p>
<p>Q1) Which of the so far introduced models can be applied to explain and predict the return on advertising spent (ROAS)? Later on, construct several models that answer to this question, using a training dataset.</p>
<p>Q2) Which sample should one use to evaluate the model? Furthermore, which model assessment and selection criteria applies in this case? Later on, evaluate the model based on applicable assessment and selection criteria.</p>
<p>Q3) Reflect about how one can improve the model prediction power.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="moving-beyond-linearity.html#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the data</span></span>
<span id="cb163-2"><a href="moving-beyond-linearity.html#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb163-3"><a href="moving-beyond-linearity.html#cb163-3" aria-hidden="true" tabindex="-1"></a>smp_size <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.75</span> <span class="sc">*</span> <span class="fu">nrow</span>(dataTfo1))</span>
<span id="cb163-4"><a href="moving-beyond-linearity.html#cb163-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb163-5"><a href="moving-beyond-linearity.html#cb163-5" aria-hidden="true" tabindex="-1"></a>train_ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq_len</span>(<span class="fu">nrow</span>(dataTfo1)), <span class="at">size =</span> smp_size)</span>
<span id="cb163-6"><a href="moving-beyond-linearity.html#cb163-6" aria-hidden="true" tabindex="-1"></a>trainTfo1 <span class="ot">&lt;-</span> dataTfo1[train_ind, ]</span>
<span id="cb163-7"><a href="moving-beyond-linearity.html#cb163-7" aria-hidden="true" tabindex="-1"></a>testTfo1 <span class="ot">&lt;-</span> dataTfo1[<span class="sc">-</span>train_ind, ]</span></code></pre></div>
<p>Â 
Â </p>
<div id="a-generalized-additive-model-gam-to-predict-roas" class="section level5" number="2.5.4.3.1">
<h5><span class="header-section-number">2.5.4.3.1</span> A Generalized Additive Model (GAM) to predict ROAS</h5>
<p>The questions that arise are:</p>
<ol style="list-style-type: lower-roman">
<li>What subset of all the predictors can best explain and predict ROAS?</li>
<li>What is the form of the relationships between them and ROAS?</li>
</ol>
<p>Â 
Â </p>
<p>Feature selection approaches include:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Generally looking at the variables âone by one,â to understand what features are important and to figure out how they contribute towards solving the problem.</p></li>
<li><p>Looking at the correlation matrix: If we are working with a model which assumes a linear relationship between the dependent and the independent variables, corr matrix can help us come up with an initial list of variable importance. However, corr matrix also works as a ârough informative toolâ for nonlinear modelling.</p></li>
<li><p>Running automatic feature selection algorithms. Functions in R include, among others:</p>
<ul>
<li>c1. regsubsets() function in âleapsâ library (presented in ISL, p.Â 244); used to select the best size model that contains a given number of predictors, where best is quantified using Residual Sum of Squares (RSS). Although regsubsets() is based on testing linear models, it works as a âroughâ list for nonlinear models.</li>
<li>c2. step.Gam() function in âgamâ library for stepwise selection of variables in GAM models. This is useful when the number of predictors is not very high.</li>
<li>c3. advanced feature selection methods based on other data-mining techniques, including but not only: random forests, Bayesian Networks, Neural Networks, or other. Notice, that we can use very complex methods of feature selection, and then construct a model that is more transparant, for instance GAMs</li>
</ul></li>
</ol>
<div id="c1-feature-selection-using-regsubsets" class="section level6" number="2.5.4.3.1.1">
<h6><span class="header-section-number">2.5.4.3.1.1</span> c1) Feature selection using regsubsets()</h6>
<p>One may exclude ad_id and variables used to calculate our dependent variable ROAS, as they have a deterministic relation with it. The variable âinterestsâ can be preprocessed to reduce the categories to the most representative ones, but for simplicity we keep it as it is. See later it creates some problems.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="moving-beyond-linearity.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(trainTfo1)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    596 obs. of  20 variables:
##  $ ad_id     : int  1121532 1121642 781508 1121798 950068 776861 1121284 1121110 1121132 710480 ...
##  $ xyzCampId : Factor w/ 3 levels &quot;916&quot;,&quot;936&quot;,&quot;1178&quot;: 3 3 2 3 2 2 3 3 3 1 ...
##  $ fbCampId  : int  144612 144630 116397 144656 123438 115574 144571 144534 144537 104205 ...
##  $ age       : Factor w/ 4 levels &quot;30-34&quot;,&quot;35-39&quot;,..: 3 4 1 1 1 4 1 1 1 1 ...
##  $ gender    : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 1 1 2 2 2 2 2 1 ...
##  $ interest  : Factor w/ 40 levels &quot;2&quot;,&quot;7&quot;,&quot;10&quot;,&quot;15&quot;,..: 18 11 22 12 3 2 25 6 9 5 ...
##  $ impr      : num  123126 99698 5040 107548 4012 ...
##  $ Clicks    : num  25 21 1 19 1 6 23 123 12 14 ...
##  $ Spent     : num  39.73 33.35 1.44 29.31 1.57 ...
##  $ conv      : num  2 1 1 1 1 1 1 6 4 1 ...
##  $ appConv   : num  1 0 0 0 0 0 0 2 2 1 ...
##  $ CTR       : num  0.0203 0.0211 0.0198 0.0177 0.0249 0.0364 0.0131 0.014 0.0104 0.0243 ...
##  $ CPC       : num  1.59 1.59 1.44 1.54 1.57 1.54 1.77 1.71 1.98 1.29 ...
##  $ totConv   : num  3 1 1 1 1 1 1 8 6 2 ...
##  $ conVal    : num  10 5 5 5 5 5 5 30 20 5 ...
##  $ appConVal : num  100 0 0 0 0 0 0 200 200 100 ...
##  $ totConVal : num  110 5 5 5 5 5 5 230 220 105 ...
##  $ costPerCon: num  13.24 33.35 1.44 29.31 1.57 ...
##  $ ROAS      : num  2.77 0.15 3.47 0.17 3.18 0.54 0.12 1.09 9.27 5.81 ...
##  $ CPM       : num  0.32 0.33 0.29 0.27 0.39 0.56 0.23 0.24 0.21 0.31 ...
##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:207] 3 6 11 12 13 14 16 18 21 25 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:207] &quot;3&quot; &quot;6&quot; &quot;11&quot; &quot;12&quot; ...</code></pre>
<p>Now we can make the selection with forward selection.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="moving-beyond-linearity.html#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps) </span>
<span id="cb166-2"><a href="moving-beyond-linearity.html#cb166-2" aria-hidden="true" tabindex="-1"></a>reg.fit <span class="ot">=</span> <span class="fu">regsubsets</span>(trainTfo1<span class="sc">$</span>ROAS <span class="sc">~</span> ., <span class="at">data =</span> trainTfo1[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>, <span class="dv">14</span><span class="sc">:</span><span class="dv">17</span>)], <span class="at">method =</span> <span class="st">&quot;forward&quot;</span>, <span class="at">nvmax=</span><span class="dv">14</span>)</span>
<span id="cb166-3"><a href="moving-beyond-linearity.html#cb166-3" aria-hidden="true" tabindex="-1"></a>reg.summary <span class="ot">=</span> <span class="fu">summary</span>(reg.fit)</span>
<span id="cb166-4"><a href="moving-beyond-linearity.html#cb166-4" aria-hidden="true" tabindex="-1"></a>reg.summary</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(trainTfo1$ROAS ~ ., data = trainTfo1[, -c(1, 
##     3, 14:17)], method = &quot;forward&quot;, nvmax = 14)
## 54 Variables  (and intercept)
##               Forced in Forced out
## xyzCampId936      FALSE      FALSE
## xyzCampId1178     FALSE      FALSE
## age35-39          FALSE      FALSE
## age40-44          FALSE      FALSE
## age45-49          FALSE      FALSE
## genderM           FALSE      FALSE
## interest7         FALSE      FALSE
## interest10        FALSE      FALSE
## interest15        FALSE      FALSE
## interest16        FALSE      FALSE
## interest18        FALSE      FALSE
## interest19        FALSE      FALSE
## interest20        FALSE      FALSE
## interest21        FALSE      FALSE
## interest22        FALSE      FALSE
## interest23        FALSE      FALSE
## interest24        FALSE      FALSE
## interest25        FALSE      FALSE
## interest26        FALSE      FALSE
## interest27        FALSE      FALSE
## interest28        FALSE      FALSE
## interest29        FALSE      FALSE
## interest30        FALSE      FALSE
## interest31        FALSE      FALSE
## interest32        FALSE      FALSE
## interest36        FALSE      FALSE
## interest63        FALSE      FALSE
## interest64        FALSE      FALSE
## interest65        FALSE      FALSE
## interest66        FALSE      FALSE
## interest100       FALSE      FALSE
## interest101       FALSE      FALSE
## interest102       FALSE      FALSE
## interest103       FALSE      FALSE
## interest104       FALSE      FALSE
## interest105       FALSE      FALSE
## interest106       FALSE      FALSE
## interest107       FALSE      FALSE
## interest108       FALSE      FALSE
## interest109       FALSE      FALSE
## interest110       FALSE      FALSE
## interest111       FALSE      FALSE
## interest112       FALSE      FALSE
## interest113       FALSE      FALSE
## interest114       FALSE      FALSE
## impr              FALSE      FALSE
## Clicks            FALSE      FALSE
## Spent             FALSE      FALSE
## conv              FALSE      FALSE
## appConv           FALSE      FALSE
## CTR               FALSE      FALSE
## CPC               FALSE      FALSE
## costPerCon        FALSE      FALSE
## CPM               FALSE      FALSE
## 1 subsets of each size up to 14
## Selection Algorithm: forward
##           xyzCampId936 xyzCampId1178 age35-39 age40-44 age45-49 genderM
## 1  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 2  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 3  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 4  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 5  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 6  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 7  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 8  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 9  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 10  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 11  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 12  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 13  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 14  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
##           interest7 interest10 interest15 interest16 interest18 interest19
## 1  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;       
##           interest20 interest21 interest22 interest23 interest24 interest25
## 1  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest26 interest27 interest28 interest29 interest30 interest31
## 1  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest32 interest36 interest63 interest64 interest65 interest66
## 1  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest100 interest101 interest102 interest103 interest104
## 1  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 2  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 3  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 4  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 5  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 6  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 7  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 8  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 9  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 10  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 11  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 12  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 13  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 14  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
##           interest105 interest106 interest107 interest108 interest109
## 1  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 2  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 3  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 4  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 5  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 6  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 7  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 8  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 9  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 10  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 11  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 12  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 13  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 14  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
##           interest110 interest111 interest112 interest113 interest114 impr
## 1  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot; 
## 2  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot; 
## 3  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 4  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 5  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 6  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 7  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 8  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 9  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 10  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 11  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 12  ( 1 ) &quot;*&quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 13  ( 1 ) &quot;*&quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 14  ( 1 ) &quot;*&quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
##           Clicks Spent conv appConv CTR CPC costPerCon CPM
## 1  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot; &quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 2  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 3  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 4  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 5  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 6  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 7  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 8  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 9  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 10  ( 1 ) &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 11  ( 1 ) &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 12  ( 1 ) &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 13  ( 1 ) &quot; &quot;    &quot;*&quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 14  ( 1 ) &quot; &quot;    &quot;*&quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;</code></pre>
<p>Recall, an asteriks indicates that a given variable is included in the corresponding model. The default is â1 subsets of each size up to 8,â but using ânvmaxâ option we can control as many variables as are desired.</p>
<p>Use a prediction error criteria (Adj^R2 BIC, Cp) to select the âbest overallâ model: names(reg.summary)</p>
<p>In this example <span class="math inline">\(R^2\)</span> is applied.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="moving-beyond-linearity.html#cb168-1" aria-hidden="true" tabindex="-1"></a>reg.summary<span class="sc">$</span>rsq</span></code></pre></div>
<pre><code>##  [1] 0.1596916 0.2163240 0.3276550 0.3437687 0.3584745 0.3676564 0.3766836
##  [8] 0.3843347 0.3880903 0.3913334 0.3946034 0.3973916 0.4003809 0.4033252</code></pre>
<p>Rsquare increases monotonically as more variables are included. This is expected; the model containing all predictors will always have the smallest RSS and the largest <span class="math inline">\(R^2\)</span>.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="moving-beyond-linearity.html#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(reg.summary<span class="sc">$</span>adjr2, <span class="at">xlab =</span> <span class="st">&quot;Number of Variables&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Adjusted R2&quot;</span>, <span class="at">type =</span> <span class="st">&quot;p&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>), <span class="at">main =</span> <span class="st">&quot; Adj R2 for each model&quot;</span>)</span>
<span id="cb170-2"><a href="moving-beyond-linearity.html#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb170-3"><a href="moving-beyond-linearity.html#cb170-3" aria-hidden="true" tabindex="-1"></a>max.adjr2 <span class="ot">=</span> <span class="fu">max</span>(reg.summary<span class="sc">$</span>adjr2)</span>
<span id="cb170-4"><a href="moving-beyond-linearity.html#cb170-4" aria-hidden="true" tabindex="-1"></a>std.adjr2 <span class="ot">=</span> <span class="fu">sd</span>(reg.summary<span class="sc">$</span>adjr2)</span>
<span id="cb170-5"><a href="moving-beyond-linearity.html#cb170-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> max.adjr2 <span class="sc">+</span> <span class="fl">0.2</span> <span class="sc">*</span> std.adjr2, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb170-6"><a href="moving-beyond-linearity.html#cb170-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> max.adjr2 <span class="sc">-</span> <span class="fl">0.2</span> <span class="sc">*</span> std.adjr2, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-118-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>The plot shows the size 10-11 is the minimum size for the subset for which the scores are within 0.2 standard deviations of optimum.</p>
<p>That is because the <span class="math inline">\(R^2\)</span> does not significantly change. Therefore, we can just limit ourselves with a more simple model.</p>
<p><strong>We can see how the selection changes if we use the full model</strong></p>
<p><em>A common approach when dataset is small is to find best 10 variables using entire dataset.</em></p>
<p>Notice, that some researches will argue that this implies data leakage.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="moving-beyond-linearity.html#cb171-1" aria-hidden="true" tabindex="-1"></a>reg.fit <span class="ot">=</span> <span class="fu">regsubsets</span>(dataTfo1<span class="sc">$</span>ROAS <span class="sc">~</span> ., <span class="at">data =</span> dataTfo1[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>, <span class="dv">14</span><span class="sc">:</span><span class="dv">17</span>)], <span class="at">method =</span> <span class="st">&quot;forward&quot;</span>, <span class="at">nvmax=</span><span class="dv">14</span>)</span>
<span id="cb171-2"><a href="moving-beyond-linearity.html#cb171-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg.fit)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(dataTfo1$ROAS ~ ., data = dataTfo1[, -c(1, 
##     3, 14:17)], method = &quot;forward&quot;, nvmax = 14)
## 54 Variables  (and intercept)
##               Forced in Forced out
## xyzCampId936      FALSE      FALSE
## xyzCampId1178     FALSE      FALSE
## age35-39          FALSE      FALSE
## age40-44          FALSE      FALSE
## age45-49          FALSE      FALSE
## genderM           FALSE      FALSE
## interest7         FALSE      FALSE
## interest10        FALSE      FALSE
## interest15        FALSE      FALSE
## interest16        FALSE      FALSE
## interest18        FALSE      FALSE
## interest19        FALSE      FALSE
## interest20        FALSE      FALSE
## interest21        FALSE      FALSE
## interest22        FALSE      FALSE
## interest23        FALSE      FALSE
## interest24        FALSE      FALSE
## interest25        FALSE      FALSE
## interest26        FALSE      FALSE
## interest27        FALSE      FALSE
## interest28        FALSE      FALSE
## interest29        FALSE      FALSE
## interest30        FALSE      FALSE
## interest31        FALSE      FALSE
## interest32        FALSE      FALSE
## interest36        FALSE      FALSE
## interest63        FALSE      FALSE
## interest64        FALSE      FALSE
## interest65        FALSE      FALSE
## interest66        FALSE      FALSE
## interest100       FALSE      FALSE
## interest101       FALSE      FALSE
## interest102       FALSE      FALSE
## interest103       FALSE      FALSE
## interest104       FALSE      FALSE
## interest105       FALSE      FALSE
## interest106       FALSE      FALSE
## interest107       FALSE      FALSE
## interest108       FALSE      FALSE
## interest109       FALSE      FALSE
## interest110       FALSE      FALSE
## interest111       FALSE      FALSE
## interest112       FALSE      FALSE
## interest113       FALSE      FALSE
## interest114       FALSE      FALSE
## impr              FALSE      FALSE
## Clicks            FALSE      FALSE
## Spent             FALSE      FALSE
## conv              FALSE      FALSE
## appConv           FALSE      FALSE
## CTR               FALSE      FALSE
## CPC               FALSE      FALSE
## costPerCon        FALSE      FALSE
## CPM               FALSE      FALSE
## 1 subsets of each size up to 14
## Selection Algorithm: forward
##           xyzCampId936 xyzCampId1178 age35-39 age40-44 age45-49 genderM
## 1  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 2  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 3  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 4  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 5  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 6  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 7  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 8  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 9  ( 1 )  &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 10  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot; &quot;      &quot; &quot;    
## 11  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 12  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 13  ( 1 ) &quot; &quot;          &quot; &quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
## 14  ( 1 ) &quot; &quot;          &quot;*&quot;           &quot; &quot;      &quot; &quot;      &quot;*&quot;      &quot; &quot;    
##           interest7 interest10 interest15 interest16 interest18 interest19
## 1  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;       &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;       &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest20 interest21 interest22 interest23 interest24 interest25
## 1  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;        &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest26 interest27 interest28 interest29 interest30 interest31
## 1  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest32 interest36 interest63 interest64 interest65 interest66
## 1  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 2  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 3  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 4  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 5  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 6  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 7  ( 1 )  &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 8  ( 1 )  &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 9  ( 1 )  &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 10  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 11  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 12  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 13  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
## 14  ( 1 ) &quot; &quot;        &quot;*&quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;        &quot; &quot;       
##           interest100 interest101 interest102 interest103 interest104
## 1  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 2  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 3  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 4  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 5  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 6  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 7  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 8  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 9  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 10  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 11  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 12  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 13  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 14  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
##           interest105 interest106 interest107 interest108 interest109
## 1  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 2  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 3  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 4  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 5  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 6  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 7  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 8  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 9  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 10  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 11  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 12  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 13  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 14  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
##           interest110 interest111 interest112 interest113 interest114 impr
## 1  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot; 
## 2  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot; 
## 3  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 4  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 5  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 6  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 7  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 8  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot; 
## 9  ( 1 )  &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
## 10  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
## 11  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
## 12  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
## 13  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
## 14  ( 1 ) &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot;*&quot;         &quot; &quot;         &quot;*&quot; 
##           Clicks Spent conv appConv CTR CPC costPerCon CPM
## 1  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot; &quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 2  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 3  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot; &quot;
## 4  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot;*&quot;     &quot; &quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 5  ( 1 )  &quot; &quot;    &quot; &quot;   &quot; &quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 6  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 7  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 8  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 9  ( 1 )  &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 10  ( 1 ) &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 11  ( 1 ) &quot; &quot;    &quot; &quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 12  ( 1 ) &quot; &quot;    &quot;*&quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 13  ( 1 ) &quot; &quot;    &quot;*&quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;
## 14  ( 1 ) &quot; &quot;    &quot;*&quot;   &quot;*&quot;  &quot;*&quot;     &quot;*&quot; &quot; &quot; &quot;*&quot;        &quot;*&quot;</code></pre>
<p>We want to select 10 variables.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="moving-beyond-linearity.html#cb173-1" aria-hidden="true" tabindex="-1"></a>coefi <span class="ot">=</span> <span class="fu">coef</span>(reg.fit</span>
<span id="cb173-2"><a href="moving-beyond-linearity.html#cb173-2" aria-hidden="true" tabindex="-1"></a>             , <span class="at">id=</span><span class="dv">10</span>) <span class="co">#Selection 10 variables</span></span>
<span id="cb173-3"><a href="moving-beyond-linearity.html#cb173-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(coefi) <span class="co"># most important variables based on regsubsets() feature selection</span></span></code></pre></div>
<pre><code>##  [1] &quot;(Intercept)&quot; &quot;interest15&quot;  &quot;interest22&quot;  &quot;interest36&quot;  &quot;interest113&quot;
##  [6] &quot;impr&quot;        &quot;conv&quot;        &quot;appConv&quot;     &quot;CTR&quot;         &quot;costPerCon&quot; 
## [11] &quot;CPM&quot;</code></pre>
<div id="i-run-a-linear-regression-using-the-selected-features-to-have-a-benchmark" class="section level7" number="2.5.4.3.1.1.1">
<p class="heading" number="2.5.4.3.1.1.1"><span class="header-section-number">2.5.4.3.1.1.1</span> 1) I run a linear regression using the selected features, to have a benchmark</p>
<p>In general, one should start with a simple model to see graudally advance, instead of jumping directly to complex models.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="moving-beyond-linearity.html#cb175-1" aria-hidden="true" tabindex="-1"></a>model1<span class="ot">&lt;-</span><span class="fu">lm</span>(ROAS <span class="sc">~</span>  interest <span class="sc">+</span> impr <span class="sc">+</span> conv <span class="sc">+</span> appConv <span class="sc">+</span> CTR <span class="sc">+</span> costPerCon <span class="sc">+</span> CPM, <span class="at">data =</span> trainTfo1)</span>
<span id="cb175-2"><a href="moving-beyond-linearity.html#cb175-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1) <span class="co"># summary.aov(model1) #Alternatice</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ROAS ~ interest + impr + conv + appConv + CTR + 
##     costPerCon + CPM, data = trainTfo1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.1662 -1.2349 -0.4432  0.7776  8.6722 
## 
## Coefficients:
##                   Estimate     Std. Error t value     Pr(&gt;|t|)    
## (Intercept)   3.3763879089   0.7060735443   4.782 0.0000022324 ***
## interest7    -0.0583638167   0.8693104092  -0.067     0.946496    
## interest10    0.3734569279   0.7214261646   0.518     0.604900    
## interest15    1.2644513305   0.8019953469   1.577     0.115455    
## interest16    0.3374435422   0.7113702609   0.474     0.635434    
## interest18    1.1057788779   0.7863738142   1.406     0.160237    
## interest19    0.9323670080   0.8226657597   1.133     0.257562    
## interest20    0.5455427694   0.7734543762   0.705     0.480902    
## interest21    0.4776314482   0.8140258838   0.587     0.557611    
## interest22   -0.9810753002   0.8151526846  -1.204     0.229282    
## interest23   -0.3833780502   0.8244934737  -0.465     0.642125    
## interest24    0.4104935167   0.8587144041   0.478     0.632817    
## interest25    0.0810874657   0.8224831441   0.099     0.921501    
## interest26   -0.3607817261   0.8259069717  -0.437     0.662405    
## interest27    0.5381079815   0.7408381010   0.726     0.467933    
## interest28    0.0740605292   0.7759433375   0.095     0.923996    
## interest29    0.1688327587   0.7394183904   0.228     0.819473    
## interest30    0.2578505667   0.9459553954   0.273     0.785277    
## interest31    1.1640980299   1.0164662116   1.145     0.252608    
## interest32    0.0403419742   0.8559559577   0.047     0.962426    
## interest36    2.1016082503   0.9016484081   2.331     0.020122 *  
## interest63    0.5435879396   0.7789099391   0.698     0.485545    
## interest64    0.6639079122   0.7722974638   0.860     0.390355    
## interest65    0.2431197639   0.9014227709   0.270     0.787487    
## interest66    0.8330235765   1.0160487256   0.820     0.412648    
## interest100   0.8516465674   1.2281375647   0.693     0.488323    
## interest101  -0.0641588098   1.3666934608  -0.047     0.962574    
## interest102   0.9276318226   1.3630425863   0.681     0.496437    
## interest103   0.8208051392   1.2313704057   0.667     0.505321    
## interest104   0.7144614519   1.4011137921   0.510     0.610309    
## interest105  -0.0104297991   1.1441788217  -0.009     0.992730    
## interest106  -0.4410980827   1.1341681728  -0.389     0.697488    
## interest107   0.8812751901   1.0344524204   0.852     0.394627    
## interest108   0.8096481829   1.1383717983   0.711     0.477241    
## interest109   0.2773726276   1.1440075722   0.242     0.808516    
## interest110   1.6981730219   1.0731149157   1.582     0.114117    
## interest111   0.8419734890   1.2354163562   0.682     0.495823    
## interest112  -0.8046055498   1.2397091379  -0.649     0.516591    
## interest113   1.5852227480   1.1413885504   1.389     0.165439    
## interest114  -0.9363468962   1.2231230818  -0.766     0.444280    
## impr         -0.0000030933   0.0000005615  -5.509 0.0000000556 ***
## conv         -0.1835589139   0.0474635767  -3.867     0.000123 ***
## appConv       1.1139191669   0.0950624431  11.718      &lt; 2e-16 ***
## CTR         109.1401641334  31.7080460628   3.442     0.000621 ***
## costPerCon   -0.0195060636   0.0047454249  -4.110 0.0000455039 ***
## CPM         -10.0887785837   2.3882226628  -4.224 0.0000280419 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.058 on 550 degrees of freedom
## Multiple R-squared:  0.413,  Adjusted R-squared:  0.3649 
## F-statistic: 8.598 on 45 and 550 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="moving-beyond-linearity.html#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(model1)</span></code></pre></div>
<pre><code>## [1] 2597.989</code></pre>
<p>The model diagnostics</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="moving-beyond-linearity.html#cb179-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb179-2"><a href="moving-beyond-linearity.html#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model1)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-123-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>Notice, that the top right is the theoretical distribution (or quantiles) of a normal distribution. We see that the is a wave in the residuals, implying that there may be some non linearity.</p>
<p>Further inspecting the residuals</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="moving-beyond-linearity.html#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb180-2"><a href="moving-beyond-linearity.html#cb180-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(model1<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-124-1.png" width="720" style="display: block; margin: auto;" /></p>
<p>The residuals look quite well</p>
</div>
<div id="linked-to-the-topic-of-this-lecture-run-a-non-liner-model-such-as-the-generalized-additive-model-gam-with-smoothing-splines." class="section level7" number="2.5.4.3.1.1.2">
<p class="heading" number="2.5.4.3.1.1.2"><span class="header-section-number">2.5.4.3.1.1.2</span> 2) Linked to the topic of this lecture, run a non-liner model, such as the Generalized Additive Model (GAM) with smoothing splines.**</p>
<p>We have two available approaches at hand:</p>
<ol style="list-style-type: decimal">
<li>Option 1: using âgamâ function from âgamâ package</li>
<li>Option 2: using âgamâ function from âmgcvâ package</li>
</ol>
<ul>
<li>Recall mgcv package and gam package may interact. Uncall one of them if you get an error.</li>
</ul>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="moving-beyond-linearity.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&#39;mgcv&#39;</span>)</span>
<span id="cb181-2"><a href="moving-beyond-linearity.html#cb181-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb181-3"><a href="moving-beyond-linearity.html#cb181-3" aria-hidden="true" tabindex="-1"></a>gam.mgcv <span class="ot">&lt;-</span> mgcv<span class="sc">::</span><span class="fu">gam</span>(ROAS <span class="sc">~</span> interest <span class="sc">+</span> <span class="fu">s</span>(impr) <span class="sc">+</span> <span class="fu">s</span>(conv) <span class="sc">+</span> <span class="fu">s</span>(appConv) <span class="sc">+</span> <span class="fu">s</span>(CTR) <span class="sc">+</span> <span class="fu">s</span>(costPerCon) <span class="sc">+</span> <span class="fu">s</span>(CPM), <span class="at">data =</span> trainTfo1) <span class="co"># &quot;gam&quot; function from &quot;mgcv&quot; package estimates the smoothness automatically based on cross-validation (LOOCV, the default) or REML (Restricted Maximum Likelihood)</span></span>
<span id="cb181-4"><a href="moving-beyond-linearity.html#cb181-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.mgcv)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ROAS ~ interest + s(impr) + s(conv) + s(appConv) + s(CTR) + s(costPerCon) + 
##     s(CPM)
## 
## Parametric coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)  2.17980    0.43775   4.980 0.000000869 ***
## interest7    0.24123    0.57911   0.417      0.6772    
## interest10   0.18304    0.48420   0.378      0.7056    
## interest15   0.38606    0.53880   0.717      0.4740    
## interest16   0.11986    0.47583   0.252      0.8012    
## interest18  -0.09142    0.52458  -0.174      0.8617    
## interest19   0.82120    0.54766   1.499      0.1344    
## interest20   0.13610    0.51544   0.264      0.7918    
## interest21   0.02640    0.53909   0.049      0.9610    
## interest22   0.09319    0.53946   0.173      0.8629    
## interest23  -0.56172    0.54856  -1.024      0.3063    
## interest24  -0.08992    0.57528  -0.156      0.8759    
## interest25  -0.04603    0.55372  -0.083      0.9338    
## interest26  -0.30219    0.55505  -0.544      0.5864    
## interest27   0.51573    0.49750   1.037      0.3004    
## interest28   0.12121    0.51861   0.234      0.8153    
## interest29  -0.03154    0.49268  -0.064      0.9490    
## interest30  -0.40478    0.62268  -0.650      0.5159    
## interest31   0.83810    0.66915   1.252      0.2110    
## interest32   0.26961    0.56769   0.475      0.6350    
## interest36   1.12968    0.59284   1.906      0.0573 .  
## interest63  -0.04652    0.51940  -0.090      0.9287    
## interest64   0.41739    0.51161   0.816      0.4150    
## interest65   0.03027    0.59451   0.051      0.9594    
## interest66   0.52158    0.66676   0.782      0.4344    
## interest100 -0.18099    0.82344  -0.220      0.8261    
## interest101  0.13743    0.90918   0.151      0.8799    
## interest102 -0.04860    0.90212  -0.054      0.9571    
## interest103  0.47957    0.81182   0.591      0.5550    
## interest104 -1.30671    0.94229  -1.387      0.1661    
## interest105  1.19263    0.76381   1.561      0.1190    
## interest106  0.24244    0.74505   0.325      0.7450    
## interest107  0.32292    0.70132   0.460      0.6454    
## interest108 -0.63161    0.75881  -0.832      0.4056    
## interest109 -0.29230    0.76418  -0.383      0.7022    
## interest110  0.53685    0.71649   0.749      0.4540    
## interest111  0.68357    0.83983   0.814      0.4161    
## interest112  0.36783    0.88298   0.417      0.6772    
## interest113  0.49405    0.76593   0.645      0.5192    
## interest114 -0.50182    0.80131  -0.626      0.5314    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##                 edf Ref.df      F  p-value    
## s(impr)       6.429  7.371 15.745  &lt; 2e-16 ***
## s(conv)       6.220  7.047  3.453 0.000984 ***
## s(appConv)    7.191  7.717 84.077  &lt; 2e-16 ***
## s(CTR)        5.281  6.340  2.375 0.026540 *  
## s(costPerCon) 7.980  8.690 28.832  &lt; 2e-16 ***
## s(CPM)        3.160  4.015  4.666 0.001024 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.735   Deviance explained = 76.9%
## GCV =  2.027  Scale est. = 1.7676    n = 596</code></pre>
<p>We see from the <strong>approximate significance of the smooth terms</strong>, that the smoothed variables appear to be siginificant, hence the smoothing splines are justified. We can also see the edf (estimated degrees of freedom), for each of the variables.</p>
<p>The first <strong><em>parametric coefficients</em></strong> show how the varibles perform compared to the reference (baseline), we see that there are different p-values, where it looks as if interest 36 is the only significant, although we must remember that this is in reference to the baseline, hence it does not imply that the categories with p &gt; 5% is insignificant. If we look further into the variable, we see that some of the categories have very few variables, hence we can bin the categories together. E.g.,</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="moving-beyond-linearity.html#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(trainTfo1<span class="sc">$</span>interest)</span></code></pre></div>
<pre><code>## 
##   2   7  10  15  16  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32 
##  10  13  48  21  60  23  17  25  18  19  17  14  18  17  37  26  38   9   7  14 
##  36  63  64  65  66 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 
##  11  24  25  11   7   4   3   3   4   3   5   5   7   5   5   6   4   4   5   4</code></pre>
<p>E.g. interest 114 have only four obsevationbs, we can aggregate groups, e.g., by looking at the means of the groups.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="moving-beyond-linearity.html#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="fu">aggregate</span>(trainTfo1<span class="sc">$</span>ROAS,<span class="at">by =</span> <span class="fu">list</span>(trainTfo1<span class="sc">$</span>interest),mean)</span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left">Group.1</th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2</td>
<td align="right">2.3410000</td>
</tr>
<tr class="even">
<td align="left">7</td>
<td align="right">2.2507692</td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="right">2.2810417</td>
</tr>
<tr class="even">
<td align="left">15</td>
<td align="right">3.1423810</td>
</tr>
<tr class="odd">
<td align="left">16</td>
<td align="right">2.3663333</td>
</tr>
<tr class="even">
<td align="left">18</td>
<td align="right">2.8282609</td>
</tr>
<tr class="odd">
<td align="left">19</td>
<td align="right">2.8247059</td>
</tr>
<tr class="even">
<td align="left">20</td>
<td align="right">2.9944000</td>
</tr>
<tr class="odd">
<td align="left">21</td>
<td align="right">2.6822222</td>
</tr>
<tr class="even">
<td align="left">22</td>
<td align="right">0.5963158</td>
</tr>
<tr class="odd">
<td align="left">23</td>
<td align="right">1.4623529</td>
</tr>
<tr class="even">
<td align="left">24</td>
<td align="right">2.2957143</td>
</tr>
<tr class="odd">
<td align="left">25</td>
<td align="right">1.8588889</td>
</tr>
<tr class="even">
<td align="left">26</td>
<td align="right">1.3188235</td>
</tr>
<tr class="odd">
<td align="left">27</td>
<td align="right">1.5972973</td>
</tr>
<tr class="even">
<td align="left">28</td>
<td align="right">1.6423077</td>
</tr>
<tr class="odd">
<td align="left">29</td>
<td align="right">2.4734211</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="right">2.4400000</td>
</tr>
<tr class="odd">
<td align="left">31</td>
<td align="right">3.5300000</td>
</tr>
<tr class="even">
<td align="left">32</td>
<td align="right">2.0764286</td>
</tr>
<tr class="odd">
<td align="left">36</td>
<td align="right">4.4445455</td>
</tr>
<tr class="even">
<td align="left">63</td>
<td align="right">2.2858333</td>
</tr>
<tr class="odd">
<td align="left">64</td>
<td align="right">2.6624000</td>
</tr>
<tr class="even">
<td align="left">65</td>
<td align="right">2.4272727</td>
</tr>
<tr class="odd">
<td align="left">66</td>
<td align="right">2.8700000</td>
</tr>
<tr class="even">
<td align="left">100</td>
<td align="right">3.8350000</td>
</tr>
<tr class="odd">
<td align="left">101</td>
<td align="right">1.9500000</td>
</tr>
<tr class="even">
<td align="left">102</td>
<td align="right">3.3900000</td>
</tr>
<tr class="odd">
<td align="left">103</td>
<td align="right">1.7875000</td>
</tr>
<tr class="even">
<td align="left">104</td>
<td align="right">2.3033333</td>
</tr>
<tr class="odd">
<td align="left">105</td>
<td align="right">0.9300000</td>
</tr>
<tr class="even">
<td align="left">106</td>
<td align="right">1.1320000</td>
</tr>
<tr class="odd">
<td align="left">107</td>
<td align="right">3.1528571</td>
</tr>
<tr class="even">
<td align="left">108</td>
<td align="right">3.2280000</td>
</tr>
<tr class="odd">
<td align="left">109</td>
<td align="right">1.1380000</td>
</tr>
<tr class="even">
<td align="left">110</td>
<td align="right">4.3983333</td>
</tr>
<tr class="odd">
<td align="left">111</td>
<td align="right">3.6075000</td>
</tr>
<tr class="even">
<td align="left">112</td>
<td align="right">1.9150000</td>
</tr>
<tr class="odd">
<td align="left">113</td>
<td align="right">3.7440000</td>
</tr>
<tr class="even">
<td align="left">114</td>
<td align="right">1.0275000</td>
</tr>
</tbody>
</table>
</div>
<p>We see that they are close to each other, where one could combine groups based on this.</p>
<p><br />
</p>
<p>For which variables, if any, is there evidence of a non-linear relationship with the response? in the output, the edf (estimated degrees of freedom) column allow us to interpret the degree of nonlinearity for each predictor.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="moving-beyond-linearity.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># contrasts(trainTfo1$interest)</span></span>
<span id="cb186-2"><a href="moving-beyond-linearity.html#cb186-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-3"><a href="moving-beyond-linearity.html#cb186-3" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(gam.mgcv) <span class="co"># decreases compared to linear model</span></span></code></pre></div>
<pre><code>## [1] 2103.794</code></pre>
<p><strong><em>Other checks</em></strong></p>
<p><strong>One should always check the residuals</strong></p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="moving-beyond-linearity.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb188-2"><a href="moving-beyond-linearity.html#cb188-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.mgcv<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-129-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="moving-beyond-linearity.html#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(gam.mgcv<span class="sc">$</span>residuals)</span>
<span id="cb189-2"><a href="moving-beyond-linearity.html#cb189-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(gam.mgcv<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-129-2.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="moving-beyond-linearity.html#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>((gam.mgcv<span class="sc">$</span>residuals))</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-129-3.png" width="720" style="display: block; margin: auto;" /></p>
<p>interpreting the partial effects of the smooth terms visually using plot(gam_model) (see p.Â 284 ISL).</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="moving-beyond-linearity.html#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb191-2"><a href="moving-beyond-linearity.html#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.mgcv, <span class="at">se =</span> T, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">scheme=</span><span class="dv">1</span>,<span class="at">unconditional =</span> <span class="cn">TRUE</span>, <span class="at">residuals =</span> <span class="cn">TRUE</span>) </span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-130-1.png" width="720" style="display: block; margin: auto;" /><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-130-2.png" width="720" style="display: block; margin: auto;" /></p>
<p>Holding all the other variables in the model fixed, return on advertising spent tends to decrease with increasing impr (the number of times the ad was shown). holding all the other variables in the model fixed, total people signing up to the webshop (conv) has no significant effect on ROAS. Holding all the other variables in the model fixed, total people purchasing the product (appConv) and click-through-rate (CTR) have a significant positive nonlinear effect on ROAS. However, the CI in some areas is also very large due to very few observations, thus more data is required to evaluate the results. Finally cost-per-click (CPM = Spent/Impr *1000) has a decreasing nonlinear effect on ROAS, but the effect is very uncertain given CI become very large when CPM &gt;1. Overall, the relationships found are intuitive but the effec ts are not strong.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="moving-beyond-linearity.html#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating test MSE using using k-fold cross-validation</span></span>
<span id="cb192-2"><a href="moving-beyond-linearity.html#cb192-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb192-3"><a href="moving-beyond-linearity.html#cb192-3" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">5</span></span>
<span id="cb192-4"><a href="moving-beyond-linearity.html#cb192-4" aria-hidden="true" tabindex="-1"></a>folds <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="fu">nrow</span>(trainTfo1), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb192-5"><a href="moving-beyond-linearity.html#cb192-5" aria-hidden="true" tabindex="-1"></a>gam.err <span class="ot">=</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, k)</span>
<span id="cb192-6"><a href="moving-beyond-linearity.html#cb192-6" aria-hidden="true" tabindex="-1"></a>gam.tss <span class="ot">=</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, k)</span>
<span id="cb192-7"><a href="moving-beyond-linearity.html#cb192-7" aria-hidden="true" tabindex="-1"></a>cv.r2 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, k)</span>
<span id="cb192-8"><a href="moving-beyond-linearity.html#cb192-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-9"><a href="moving-beyond-linearity.html#cb192-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb192-10"><a href="moving-beyond-linearity.html#cb192-10" aria-hidden="true" tabindex="-1"></a>  gam.mgcv <span class="ot">=</span> mgcv<span class="sc">::</span><span class="fu">gam</span>(ROAS <span class="sc">~</span> <span class="fu">s</span>(impr) <span class="sc">+</span> <span class="fu">s</span>(conv) <span class="sc">+</span> <span class="fu">s</span>(appConv) <span class="sc">+</span> <span class="fu">s</span>(CTR) <span class="sc">+</span> <span class="fu">s</span>(costPerCon) <span class="sc">+</span> <span class="fu">s</span>(CPM), <span class="at">data =</span> trainTfo1[folds<span class="sc">!=</span>j,])  <span class="co"># folds !=j returns the in-sample data</span></span>
<span id="cb192-11"><a href="moving-beyond-linearity.html#cb192-11" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(gam.mgcv, trainTfo1[folds<span class="sc">==</span>j,])</span>
<span id="cb192-12"><a href="moving-beyond-linearity.html#cb192-12" aria-hidden="true" tabindex="-1"></a>  gam.err[j] <span class="ot">=</span> <span class="fu">mean</span>((trainTfo1<span class="sc">$</span>ROAS[folds<span class="sc">==</span>j] <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb192-13"><a href="moving-beyond-linearity.html#cb192-13" aria-hidden="true" tabindex="-1"></a>  gam.tss[j] <span class="ot">=</span> <span class="fu">mean</span>((trainTfo1<span class="sc">$</span>ROAS[folds<span class="sc">==</span>j] <span class="sc">-</span> <span class="fu">mean</span>(trainTfo1<span class="sc">$</span>ROAS[folds<span class="sc">==</span>j]))<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb192-14"><a href="moving-beyond-linearity.html#cb192-14" aria-hidden="true" tabindex="-1"></a>  cv.r2[j] <span class="ot">=</span> <span class="dv">1</span> <span class="sc">-</span> gam.err[j]<span class="sc">/</span>gam.tss[j] </span>
<span id="cb192-15"><a href="moving-beyond-linearity.html#cb192-15" aria-hidden="true" tabindex="-1"></a>  cv.r2[j] </span>
<span id="cb192-16"><a href="moving-beyond-linearity.html#cb192-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb192-17"><a href="moving-beyond-linearity.html#cb192-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-18"><a href="moving-beyond-linearity.html#cb192-18" aria-hidden="true" tabindex="-1"></a>cv.r2</span></code></pre></div>
<pre><code>##           [,1]
## [1,] 0.6894129
## [2,] 0.7195426
## [3,] 0.7299054
## [4,] 0.7580761
## [5,] 0.5879333</code></pre>
<p>We see the different R squares, this we are now going to take the mean of.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="moving-beyond-linearity.html#cb194-1" aria-hidden="true" tabindex="-1"></a>mean.cv.R2<span class="ot">=</span><span class="fu">apply</span> (cv.r2, <span class="dv">2</span>, mean)</span>
<span id="cb194-2"><a href="moving-beyond-linearity.html#cb194-2" aria-hidden="true" tabindex="-1"></a>mean.cv.R2</span></code></pre></div>
<pre><code>## [1] 0.6969741</code></pre>
<p>The mean is 0.697. Now we can compare the model with the test partition.</p>
<p><strong><em>Evaluating the model test MSE and R^2 using the test dataset</em></strong></p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="moving-beyond-linearity.html#cb196-1" aria-hidden="true" tabindex="-1"></a>gam.pred <span class="ot">=</span> <span class="fu">predict</span>(gam.mgcv, testTfo1)</span>
<span id="cb196-2"><a href="moving-beyond-linearity.html#cb196-2" aria-hidden="true" tabindex="-1"></a>gam.err <span class="ot">=</span> <span class="fu">mean</span>((testTfo1<span class="sc">$</span>ROAS <span class="sc">-</span> gam.pred)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb196-3"><a href="moving-beyond-linearity.html#cb196-3" aria-hidden="true" tabindex="-1"></a>gam.tss <span class="ot">=</span> <span class="fu">mean</span>((testTfo1<span class="sc">$</span>ROAS <span class="sc">-</span> <span class="fu">mean</span>(testTfo1<span class="sc">$</span>ROAS))<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb196-4"><a href="moving-beyond-linearity.html#cb196-4" aria-hidden="true" tabindex="-1"></a>test.r2 <span class="ot">=</span> <span class="dv">1</span> <span class="sc">-</span> gam.err<span class="sc">/</span>gam.tss </span>
<span id="cb196-5"><a href="moving-beyond-linearity.html#cb196-5" aria-hidden="true" tabindex="-1"></a>test.r2 <span class="co"># the % of explained variance (R^2 in the data test)</span></span></code></pre></div>
<pre><code>## [1] 0.6703216</code></pre>
<p>We see that R2 is similar, hence the model does not appear to have high optimisim.</p>
<p><strong><em>Accuracy</em></strong></p>
<p>Accuracy: correlation Actual vs.Â Predicted values</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="moving-beyond-linearity.html#cb198-1" aria-hidden="true" tabindex="-1"></a>actuals_preds <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(<span class="at">actuals=</span>testTfo1<span class="sc">$</span>ROAS,<span class="at">predicteds=</span>gam.pred))  </span>
<span id="cb198-2"><a href="moving-beyond-linearity.html#cb198-2" aria-hidden="true" tabindex="-1"></a>correlation_accuracy <span class="ot">&lt;-</span> <span class="fu">cor</span>(actuals_preds)  </span>
<span id="cb198-3"><a href="moving-beyond-linearity.html#cb198-3" aria-hidden="true" tabindex="-1"></a>correlation_accuracy</span></code></pre></div>
<pre><code>##              actuals predicteds
## actuals    1.0000000  0.8305841
## predicteds 0.8305841  1.0000000</code></pre>
<p>Â 
Â </p>
<p>General discussion: Although the fitted GAM yields to a decent R^2 on the test set and cross-validation, it could further be evaluated in comparison with the fit of other models. It is also possible to improve the model by including additional variables. Product advertised or Price may improve the model fit. It is important to consider all possible input variables and make a good selection early in the analytical process.</p>
<p>Â 
Â </p>
</div>
</div>
<div id="c2-feature-selection-using-step.gam" class="section level6" number="2.5.4.3.1.2">
<h6><span class="header-section-number">2.5.4.3.1.2</span> c2) Feature selection using step.GAM</h6>
<p>Notice, that this is just another approach.</p>
<p>in package gam, the analysist has to search for the effective df. One can create a scope list with possible splines for each continous variable. Below, we considered splines up to maximum 10 degrees of freedom.</p>
<p>Notice, that the categorical variables are just included as raw data, as we canât take the polynomial of this.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="moving-beyond-linearity.html#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gam)</span>
<span id="cb200-2"><a href="moving-beyond-linearity.html#cb200-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-3"><a href="moving-beyond-linearity.html#cb200-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Making a list with the different models</span></span>
<span id="cb200-4"><a href="moving-beyond-linearity.html#cb200-4" aria-hidden="true" tabindex="-1"></a>scope_list <span class="ot">=</span> <span class="fu">list</span>(<span class="st">&quot;xyzCampId&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> xyzCampId,</span>
<span id="cb200-5"><a href="moving-beyond-linearity.html#cb200-5" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;age&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> age,</span>
<span id="cb200-6"><a href="moving-beyond-linearity.html#cb200-6" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;gender&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> gender,</span>
<span id="cb200-7"><a href="moving-beyond-linearity.html#cb200-7" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;interest&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> interest,</span>
<span id="cb200-8"><a href="moving-beyond-linearity.html#cb200-8" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;impr&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> impr <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df =</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(impr, <span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-9"><a href="moving-beyond-linearity.html#cb200-9" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Clicks&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span><span class="sc">+</span>Clicks<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">5</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">6</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">7</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">8</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">9</span>)<span class="sc">+</span><span class="fu">s</span>(Clicks, <span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-10"><a href="moving-beyond-linearity.html#cb200-10" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Spent&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> Spent <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(Spent, <span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-11"><a href="moving-beyond-linearity.html#cb200-11" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;conv&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> conv <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(conv, <span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-12"><a href="moving-beyond-linearity.html#cb200-12" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;appConv&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span><span class="sc">+</span>appConv<span class="sc">+</span><span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(appConv,<span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-13"><a href="moving-beyond-linearity.html#cb200-13" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;CTR&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> CTR <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-14"><a href="moving-beyond-linearity.html#cb200-14" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;CPC&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> CPC <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df=</span><span class="dv">10</span>),</span>
<span id="cb200-15"><a href="moving-beyond-linearity.html#cb200-15" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;CPM&quot;</span> <span class="ot">=</span> <span class="er">~</span><span class="dv">1</span> <span class="sc">+</span> CPM <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">8</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">s</span>(CPM, <span class="at">df=</span><span class="dv">10</span>))</span>
<span id="cb200-16"><a href="moving-beyond-linearity.html#cb200-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-17"><a href="moving-beyond-linearity.html#cb200-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the starting model</span></span>
<span id="cb200-18"><a href="moving-beyond-linearity.html#cb200-18" aria-hidden="true" tabindex="-1"></a>gam.start <span class="ot">&lt;-</span> gam<span class="sc">::</span><span class="fu">gam</span>(ROAS <span class="sc">~</span> xyzCampId <span class="sc">+</span> age <span class="sc">+</span> gender <span class="sc">+</span> interest <span class="sc">+</span> impr <span class="sc">+</span> Clicks <span class="sc">+</span> Spent <span class="sc">+</span> conv <span class="sc">+</span> appConv <span class="sc">+</span> CTR <span class="sc">+</span> CPC <span class="sc">+</span> CPM, <span class="at">data =</span> trainTfo1) <span class="co">#Note, this is just linear</span></span>
<span id="cb200-19"><a href="moving-beyond-linearity.html#cb200-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-20"><a href="moving-beyond-linearity.html#cb200-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Finally, search among different models</span></span>
<span id="cb200-21"><a href="moving-beyond-linearity.html#cb200-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove &quot;tick / check mark&quot; from the package mgcv if you get an error next</span></span>
<span id="cb200-22"><a href="moving-beyond-linearity.html#cb200-22" aria-hidden="true" tabindex="-1"></a>gam<span class="sc">::</span><span class="fu">step.Gam</span>(gam.start,<span class="at">scope =</span> scope_list)</span></code></pre></div>
<pre><code>## Start:  ROAS ~ xyzCampId + age + gender + interest + impr + Clicks +      Spent + conv + appConv + CTR + CPC + CPM; AIC= 2611.094 
## Step:1 ROAS ~ xyzCampId + age + gender + interest + impr + Clicks +      Spent + conv + s(appConv, df = 2) + CTR + CPC + CPM ; AIC= 2541.326 
## Step:2 ROAS ~ xyzCampId + age + gender + interest + impr + Clicks +      Spent + conv + s(appConv, df = 3) + CTR + CPC + CPM ; AIC= 2487.485 
## Step:3 ROAS ~ xyzCampId + age + gender + impr + Clicks + Spent + conv +      s(appConv, df = 3) + CTR + CPC + CPM ; AIC= 2449.452 
## Step:4 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 2) +      Spent + conv + s(appConv, df = 3) + CTR + CPC + CPM ; AIC= 2409.099 
## Step:5 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 2) +      Spent + conv + s(appConv, df = 4) + CTR + CPC + CPM ; AIC= 2368.959 
## Step:6 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 3) +      Spent + conv + s(appConv, df = 4) + CTR + CPC + CPM ; AIC= 2347.551 
## Step:7 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 3) +      Spent + conv + s(appConv, df = 4) + CTR + s(CPC, df = 2) +      CPM ; AIC= 2324.868 
## Step:8 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 3) +      Spent + conv + s(appConv, df = 5) + CTR + s(CPC, df = 2) +      CPM ; AIC= 2302.618 
## Step:9 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 4) +      Spent + conv + s(appConv, df = 5) + CTR + s(CPC, df = 2) +      CPM ; AIC= 2290.11 
## Step:10 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 4) +      Spent + conv + s(appConv, df = 6) + CTR + s(CPC, df = 2) +      CPM ; AIC= 2279.844 
## Step:11 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 4) +      Spent + conv + s(appConv, df = 6) + CTR + s(CPC, df = 3) +      CPM ; AIC= 2272.156 
## Step:12 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 5) +      Spent + conv + s(appConv, df = 6) + CTR + s(CPC, df = 3) +      CPM ; AIC= 2265.412 
## Step:13 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 5) +      Spent + conv + s(appConv, df = 7) + CTR + s(CPC, df = 3) +      CPM ; AIC= 2261.135 
## Step:14 ROAS ~ xyzCampId + age + gender + impr + s(Clicks, df = 5) +      Spent + conv + s(appConv, df = 7) + s(CTR, df = 2) + s(CPC,      df = 3) + CPM ; AIC= 2256.847 
## Step:15 ROAS ~ xyzCampId + gender + impr + s(Clicks, df = 5) + Spent +      conv + s(appConv, df = 7) + s(CTR, df = 2) + s(CPC, df = 3) +      CPM ; AIC= 2253.772 
## Step:16 ROAS ~ xyzCampId + gender + impr + s(Clicks, df = 6) + Spent +      conv + s(appConv, df = 7) + s(CTR, df = 2) + s(CPC, df = 3) +      CPM ; AIC= 2250.892 
## Step:17 ROAS ~ xyzCampId + impr + s(Clicks, df = 6) + Spent + conv +      s(appConv, df = 7) + s(CTR, df = 2) + s(CPC, df = 3) + CPM ; AIC= 2249.067 
## Step:18 ROAS ~ xyzCampId + impr + s(Clicks, df = 6) + Spent + s(appConv,      df = 7) + s(CTR, df = 2) + s(CPC, df = 3) + CPM ; AIC= 2247.226 
## Step:19 ROAS ~ xyzCampId + impr + s(Clicks, df = 6) + s(appConv, df = 7) +      s(CTR, df = 2) + s(CPC, df = 3) + CPM ; AIC= 2245.43 
## Step:20 ROAS ~ impr + s(Clicks, df = 6) + s(appConv, df = 7) + s(CTR,      df = 2) + s(CPC, df = 3) + CPM ; AIC= 2243.744 
## Step:21 ROAS ~ impr + s(Clicks, df = 6) + s(appConv, df = 8) + s(CTR,      df = 2) + s(CPC, df = 3) + CPM ; AIC= 2242.46 
## Step:22 ROAS ~ impr + s(Clicks, df = 7) + s(appConv, df = 8) + s(CTR,      df = 2) + s(CPC, df = 3) + CPM ; AIC= 2241.376 
## Step:23 ROAS ~ impr + s(Clicks, df = 7) + s(appConv, df = 8) + s(CTR,      df = 2) + s(CPC, df = 4) + CPM ; AIC= 2241.011 
## Step:24 ROAS ~ impr + s(Clicks, df = 7) + s(appConv, df = 8) + s(CTR,      df = 3) + s(CPC, df = 4) + CPM ; AIC= 2240.781 
## Step:25 ROAS ~ impr + s(Clicks, df = 8) + s(appConv, df = 8) + s(CTR,      df = 3) + s(CPC, df = 4) + CPM ; AIC= 2240.731</code></pre>
<pre><code>## Call:
## gam::gam(formula = ROAS ~ impr + s(Clicks, df = 8) + s(appConv, 
##     df = 8) + s(CTR, df = 3) + s(CPC, df = 4) + CPM, data = trainTfo1, 
##     trace = FALSE)
## 
## Degrees of Freedom: 595 total; 570 Residual
## Residual Deviance: 1368.37</code></pre>
<p>The last model is the best model, hence</p>
<p>gam::gam(formula = ROAS ~ impr + s(Clicks, df = 5) + s(appConv,df = 5) + s(CTR, df = 3) + s(CPC, df = 4) + CPM, data = trainTfo1,trace = FALSE)</p>
<ul>
<li>AIC is used internally to pick the best model.</li>
<li>This is reported in the function output at every step.</li>
<li>The âbestâ model meaning it has the lowest AIC in the space it has searched.</li>
<li>In this case, the best model was:</li>
<li>ROAS ~ impr + s(Clicks, df = 5) + s(appConv,df = 5) + s(CTR, df = 3) + s(CPC, df = 4) + CPM.</li>
</ul>
<p><strong>Next, run the recommended model and evaluate the AIC. Is it better than the previous ones?</strong></p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="moving-beyond-linearity.html#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gam)</span>
<span id="cb203-2"><a href="moving-beyond-linearity.html#cb203-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb203-3"><a href="moving-beyond-linearity.html#cb203-3" aria-hidden="true" tabindex="-1"></a><span class="co">#The model found in gam.step()</span></span>
<span id="cb203-4"><a href="moving-beyond-linearity.html#cb203-4" aria-hidden="true" tabindex="-1"></a>gam.step <span class="ot">&lt;-</span> gam<span class="sc">::</span><span class="fu">gam</span>(ROAS <span class="sc">~</span>  impr <span class="sc">+</span> <span class="fu">s</span>(Clicks, <span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(appConv,<span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">s</span>(CTR, <span class="at">df =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">s</span>(CPC, <span class="at">df =</span> <span class="dv">4</span>) <span class="sc">+</span> CPM, <span class="at">data =</span> trainTfo1)</span>
<span id="cb203-5"><a href="moving-beyond-linearity.html#cb203-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb203-6"><a href="moving-beyond-linearity.html#cb203-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.step)</span></code></pre></div>
<pre><code>## 
## Call: gam::gam(formula = ROAS ~ impr + s(Clicks, df = 5) + s(appConv, 
##     df = 5) + s(CTR, df = 3) + s(CPC, df = 4) + CPM, data = trainTfo1)
## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1065 -0.9782 -0.4980  0.6788  6.5816 
## 
## (Dispersion Parameter for gaussian family taken to be 2.5114)
## 
##     Null Deviance: 3969.325 on 595 degrees of freedom
## Residual Deviance: 1446.543 on 575.9999 degrees of freedom
## AIC: 2261.843 
## 
## Number of Local Scoring Iterations: NA 
## 
## Anova for Parametric Effects
##                     Df  Sum Sq Mean Sq  F value        Pr(&gt;F)    
## impr                 1   77.85   77.85  30.9991 0.00000003963 ***
## s(Clicks, df = 5)    1  315.38  315.38 125.5807     &lt; 2.2e-16 ***
## s(appConv, df = 5)   1  958.24  958.24 381.5628     &lt; 2.2e-16 ***
## s(CTR, df = 3)       1    3.49    3.49   1.3895       0.23897    
## s(CPC, df = 4)       1   70.54   70.54  28.0879 0.00000016536 ***
## CPM                  1   15.41   15.41   6.1352       0.01354 *  
## Residuals          576 1446.54    2.51                           
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Anova for Nonparametric Effects
##                    Npar Df Npar F          Pr(F)    
## (Intercept)                                         
## impr                                                
## s(Clicks, df = 5)        4 35.223      &lt; 2.2e-16 ***
## s(appConv, df = 5)       4 99.433      &lt; 2.2e-16 ***
## s(CTR, df = 3)           2  5.207       0.005736 ** 
## s(CPC, df = 4)           3 14.083 0.000000007189 ***
## CPM                                                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="moving-beyond-linearity.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb205-2"><a href="moving-beyond-linearity.html#cb205-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam.step, <span class="at">se =</span> T, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="Machine-Learning-Notes_files/figure-html/unnamed-chunk-137-1.png" width="720" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="moving-beyond-linearity.html#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(gam.step)</span></code></pre></div>
<pre><code>## [1] 2261.843</code></pre>
<p>Compare AIC values for all fitted models</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="moving-beyond-linearity.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(model1, gam.mgcv, gam.step) </span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">df</th>
<th align="right">AIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">model1</td>
<td align="right">47.00000</td>
<td align="right">2597.989</td>
</tr>
<tr class="even">
<td align="left">gam.mgcv</td>
<td align="right">39.72367</td>
<td align="right">1640.318</td>
</tr>
<tr class="odd">
<td align="left">gam.step</td>
<td align="right">8.00000</td>
<td align="right">2261.843</td>
</tr>
</tbody>
</table>
</div>
<p>We see that the second approach appear to be better than the step model.</p>
<p>Â 
Â </p>
</div>
<div id="c3-fetaure-selection-using-random-forest" class="section level6" number="2.5.4.3.1.3">
<h6><span class="header-section-number">2.5.4.3.1.3</span> c3) Fetaure selection using random forest</h6>
<p>Advanced feature selection methods is an active domain of research. Modern techniques include, among others, using random forests to find a set of predictors that best explains the variance in the response variable (Random forests models is covered in Ch.8). For a review of all methods, consider this suplementary readings: A. JoviÄ, K. BrkiÄ and N. BogunoviÄ âA review of feature selection methods with applications.â</p>
<p>Feature selection using Random Forest (RF)</p>
<p>RF is covered in-depth in the next lectures</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="moving-beyond-linearity.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(party)</span>
<span id="cb209-2"><a href="moving-beyond-linearity.html#cb209-2" aria-hidden="true" tabindex="-1"></a>cf1 <span class="ot">&lt;-</span> <span class="fu">cforest</span>(trainTfo1<span class="sc">$</span>ROAS <span class="sc">~</span> . , <span class="at">data =</span> trainTfo1[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">14</span><span class="sc">:</span><span class="dv">17</span>)]</span>
<span id="cb209-3"><a href="moving-beyond-linearity.html#cb209-3" aria-hidden="true" tabindex="-1"></a>               ,<span class="at">control=</span><span class="fu">cforest_unbiased</span>(<span class="at">mtry=</span><span class="dv">2</span>,<span class="at">ntree=</span><span class="dv">100</span>))</span>
<span id="cb209-4"><a href="moving-beyond-linearity.html#cb209-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-5"><a href="moving-beyond-linearity.html#cb209-5" aria-hidden="true" tabindex="-1"></a><span class="co">#We sort the variables for where importance is decreasing</span></span>
<span id="cb209-6"><a href="moving-beyond-linearity.html#cb209-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(<span class="fu">varimp</span>(cf1), <span class="at">decreasing =</span> <span class="cn">TRUE</span>)  </span></code></pre></div>
<pre><code>##     appConv  costPerCon      Clicks       Spent        impr        conv 
## 4.982585204 2.651832080 1.193535977 1.028742086 0.664008402 0.269207329 
##         CPM   xyzCampId         CTR         age         CPC    interest 
## 0.248213011 0.225695612 0.136556541 0.119650200 0.089944044 0.042268625 
##      gender 
## 0.009347033</code></pre>
<p>Based on the estimated variable importance, select the most relevant ones. Different feature selection methods, yield to slighly different variables, but majority coincide.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="moving-beyond-linearity.html#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span>
<span id="cb211-2"><a href="moving-beyond-linearity.html#cb211-2" aria-hidden="true" tabindex="-1"></a>gam.rforest <span class="ot">&lt;-</span> mgcv<span class="sc">::</span><span class="fu">gam</span>(ROAS <span class="sc">~</span> <span class="fu">s</span>(appConv) <span class="sc">+</span> <span class="fu">s</span>(costPerCon) <span class="sc">+</span> <span class="fu">s</span>(Spent) <span class="sc">+</span> <span class="fu">s</span>(Clicks) <span class="sc">+</span> <span class="fu">s</span>(impr) <span class="sc">+</span> <span class="fu">s</span>(conv) <span class="sc">+</span> <span class="fu">s</span>(CPM) <span class="sc">+</span> xyzCampId <span class="sc">+</span> <span class="fu">s</span>(CTR), <span class="at">data =</span> trainTfo1)</span>
<span id="cb211-3"><a href="moving-beyond-linearity.html#cb211-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam.rforest)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ROAS ~ s(appConv) + s(costPerCon) + s(Spent) + s(Clicks) + s(impr) + 
##     s(conv) + s(CPM) + xyzCampId + s(CTR)
## 
## Parametric coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     2.6312     0.3864   6.809 2.57e-11 ***
## xyzCampId936   -0.4511     0.3706  -1.217    0.224    
## xyzCampId1178  -0.2580     0.4153  -0.621    0.535    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##                 edf Ref.df      F  p-value    
## s(appConv)    7.475  7.871 90.791  &lt; 2e-16 ***
## s(costPerCon) 8.052  8.727 29.497  &lt; 2e-16 ***
## s(Spent)      1.000  1.000  0.368 0.544537    
## s(Clicks)     3.372  4.335  1.748 0.127220    
## s(impr)       5.979  6.988  2.725 0.008853 ** 
## s(conv)       6.429  7.214  4.220 0.000236 ***
## s(CPM)        3.506  4.434  4.058 0.002305 ** 
## s(CTR)        5.515  6.594  2.771 0.008908 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =   0.74   Deviance explained = 75.9%
## GCV = 1.8726  Scale est. = 1.7334    n = 596</code></pre>
<p>discuss sig.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="moving-beyond-linearity.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(gam.rforest)</span></code></pre></div>
<pre><code>## [1] 2063.802</code></pre>
<p>Compare AIC values for all fitted models</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="moving-beyond-linearity.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(model1, gam.mgcv, gam.step, gam.rforest) </span></code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">df</th>
<th align="right">AIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">model1</td>
<td align="right">47.00000</td>
<td align="right">2597.989</td>
</tr>
<tr class="even">
<td align="left">gam.mgcv</td>
<td align="right">39.72367</td>
<td align="right">1640.318</td>
</tr>
<tr class="odd">
<td align="left">gam.step</td>
<td align="right">8.00000</td>
<td align="right">2261.843</td>
</tr>
<tr class="even">
<td align="left">gam.rforest</td>
<td align="right">45.32798</td>
<td align="right">2063.802</td>
</tr>
</tbody>
</table>
</div>
<p>Evaluate the gam.rforest using k-fold cv-error, MSE and R^2 as done before with gam.mgcv.</p>
<p>Conclude about the best model.</p>

</div>
</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tree-based-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
