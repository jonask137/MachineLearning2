<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.1 The Basics of Decision Trees | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="3.1 The Basics of Decision Trees | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3.1 The Basics of Decision Trees | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.1 The Basics of Decision Trees | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tree-based-methods.html"/>
<link rel="next" href="bagging-random-forests-boosting.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html"><i class="fa fa-check"></i><b>2.1</b> Models Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#degrees-of-freedom"><i class="fa fa-check"></i><b>2.1.3.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="2.1.3.5" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.5</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#feature-selection"><i class="fa fa-check"></i><b>2.1.6.1</b> Feature Selection</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for regression problems</a>
<ul>
<li class="chapter" data-level="2.1.6.2.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#interpretation-of-output"><i class="fa fa-check"></i><b>2.1.6.2.1</b> Interpretation of output</a></li>
</ul></li>
<li class="chapter" data-level="2.1.6.3" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.3</b> GAM for classification problems</a></li>
</ul></li>
<li class="chapter" data-level="2.1.7" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#model-assessment"><i class="fa fa-check"></i><b>2.1.7</b> Model assessment</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>2.2</b> Lecture notes</a></li>
<li class="chapter" data-level="2.3" data-path="lab-section.html"><a href="lab-section.html"><i class="fa fa-check"></i><b>2.3</b> Lab section</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lab-section.html"><a href="lab-section.html#polynomial-regression-and-step-functions"><i class="fa fa-check"></i><b>2.3.1</b> Polynomial Regression and Step Functions</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="lab-section.html"><a href="lab-section.html#continous-model"><i class="fa fa-check"></i><b>2.3.1.1</b> Continous model</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="lab-section.html"><a href="lab-section.html#logarithmic-model"><i class="fa fa-check"></i><b>2.3.1.2</b> Logarithmic model</a></li>
<li class="chapter" data-level="2.3.1.3" data-path="lab-section.html"><a href="lab-section.html#step-function"><i class="fa fa-check"></i><b>2.3.1.3</b> Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="lab-section.html"><a href="lab-section.html#splines"><i class="fa fa-check"></i><b>2.3.2</b> Splines</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="lab-section.html"><a href="lab-section.html#basis-function-splines"><i class="fa fa-check"></i><b>2.3.2.1</b> Basis Function Splines</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="lab-section.html"><a href="lab-section.html#natural-splines"><i class="fa fa-check"></i><b>2.3.2.2</b> Natural Splines</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="lab-section.html"><a href="lab-section.html#smooth-splines"><i class="fa fa-check"></i><b>2.3.2.3</b> Smooth Splines</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="lab-section.html"><a href="lab-section.html#local-regression-1"><i class="fa fa-check"></i><b>2.3.2.4</b> Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3.3" data-path="lab-section.html"><a href="lab-section.html#gams"><i class="fa fa-check"></i><b>2.3.3</b> GAMs</a>
<ul>
<li class="chapter" data-level="2.3.3.1" data-path="lab-section.html"><a href="lab-section.html#with-only-natural-splines"><i class="fa fa-check"></i><b>2.3.3.1</b> With only natural splines</a></li>
<li class="chapter" data-level="2.3.3.2" data-path="lab-section.html"><a href="lab-section.html#with-different-splines"><i class="fa fa-check"></i><b>2.3.3.2</b> With different splines</a></li>
<li class="chapter" data-level="2.3.3.3" data-path="lab-section.html"><a href="lab-section.html#but-what-variables-to-include"><i class="fa fa-check"></i><b>2.3.3.3</b> But what variables to include?</a></li>
<li class="chapter" data-level="2.3.3.4" data-path="lab-section.html"><a href="lab-section.html#gam-with-local-regression"><i class="fa fa-check"></i><b>2.3.3.4</b> GAM with local regression</a></li>
<li class="chapter" data-level="2.3.3.5" data-path="lab-section.html"><a href="lab-section.html#logistic-regression"><i class="fa fa-check"></i><b>2.3.3.5</b> Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="exercises.html"><a href="exercises.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a>
<ul>
<li class="chapter" data-level="2.4.1.1" data-path="exercises.html"><a href="exercises.html#a-polynomial-regression"><i class="fa fa-check"></i><b>2.4.1.1</b> 6.a Polynomial Regression</a></li>
<li class="chapter" data-level="2.4.1.2" data-path="exercises.html"><a href="exercises.html#b-step-function"><i class="fa fa-check"></i><b>2.4.1.2</b> 6.b Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.4.2" data-path="exercises.html"><a href="exercises.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises.html"><a href="exercises.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises.html"><a href="exercises.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a>
<ul>
<li class="chapter" data-level="2.4.4.1" data-path="exercises.html"><a href="exercises.html#a-using-poly-function-to-fit-cubic-polynomial-regression"><i class="fa fa-check"></i><b>2.4.4.1</b> (a) using poly function to fit cubic polynomial regression</a></li>
<li class="chapter" data-level="2.4.4.2" data-path="exercises.html"><a href="exercises.html#b-plotting-polynomial-fits-for-a-range-of-polynomials"><i class="fa fa-check"></i><b>2.4.4.2</b> (b) Plotting polynomial fits for a range of polynomials</a></li>
<li class="chapter" data-level="2.4.4.3" data-path="exercises.html"><a href="exercises.html#c-using-cv-to-select-best-degree-of-d"><i class="fa fa-check"></i><b>2.4.4.3</b> (c) Using CV to select best degree of d</a></li>
<li class="chapter" data-level="2.4.4.4" data-path="exercises.html"><a href="exercises.html#d-use-bs-to-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.4</b> (d) Use <code>bs()</code> to fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.5" data-path="exercises.html"><a href="exercises.html#e-now-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.5</b> (e) Now fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.6" data-path="exercises.html"><a href="exercises.html#f-perform-cross-validation-to-select-degrees"><i class="fa fa-check"></i><b>2.4.4.6</b> (f) Perform cross-validation, to select degrees</a></li>
</ul></li>
<li class="chapter" data-level="2.4.5" data-path="exercises.html"><a href="exercises.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a>
<ul>
<li class="chapter" data-level="2.4.5.1" data-path="exercises.html"><a href="exercises.html#a-partitioning-the-data"><i class="fa fa-check"></i><b>2.4.5.1</b> (a) Partitioning the data</a></li>
<li class="chapter" data-level="2.4.5.2" data-path="exercises.html"><a href="exercises.html#b-fitting-a-gam"><i class="fa fa-check"></i><b>2.4.5.2</b> (b) Fitting a GAM</a></li>
<li class="chapter" data-level="2.4.5.3" data-path="exercises.html"><a href="exercises.html#c-evaluating-on-the-test-set"><i class="fa fa-check"></i><b>2.4.5.3</b> (c) Evaluating on the test set</a></li>
<li class="chapter" data-level="2.4.5.4" data-path="exercises.html"><a href="exercises.html#d-which-variables-appear-to-have-a-non-linear-relationship"><i class="fa fa-check"></i><b>2.4.5.4</b> (d) Which variables appear to have a non linear relationship?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html"><i class="fa fa-check"></i><b>2.5</b> Predicting the Return on Advertising Spent</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#background"><i class="fa fa-check"></i><b>2.5.1</b> 1. Background</a></li>
<li class="chapter" data-level="2.5.2" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#case-study-business-understanding-phase"><i class="fa fa-check"></i><b>2.5.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.3" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#the-data-data-understanding-phase"><i class="fa fa-check"></i><b>2.5.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.4" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#specific-requirements"><i class="fa fa-check"></i><b>2.5.4</b> 4. Specific requirements:</a>
<ul>
<li class="chapter" data-level="2.5.4.1" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#task-1---import-and-overview"><i class="fa fa-check"></i><b>2.5.4.1</b> 4.1 Task 1 - Import and overview</a></li>
<li class="chapter" data-level="2.5.4.2" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#task-2---data-inspection"><i class="fa fa-check"></i><b>2.5.4.2</b> 4.2 Task 2 - Data inspection</a></li>
<li class="chapter" data-level="2.5.4.3" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#task-3---building-different-models"><i class="fa fa-check"></i><b>2.5.4.3</b> 4.3 Task 3 - Building different models</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#a-generalized-additive-model-gam-to-predict-roas"><i class="fa fa-check"></i><b>2.5.4.3.1</b> A Generalized Additive Model (GAM) to predict ROAS</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1.1" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#c1-feature-selection-using-regsubsets"><i class="fa fa-check"></i><b>2.5.4.3.1.1</b> c1) Feature selection using regsubsets()</a></li>
<li class="chapter" data-level="2.5.4.3.1.2" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#c2-feature-selection-using-step.gam"><i class="fa fa-check"></i><b>2.5.4.3.1.2</b> c2) Feature selection using step.GAM</a></li>
<li class="chapter" data-level="2.5.4.3.1.3" data-path="FacebookCasestudy.html"><a href="FacebookCasestudy.html#c3-fetaure-selection-using-random-forest"><i class="fa fa-check"></i><b>2.5.4.3.1.3</b> c3) Fetaure selection using random forest</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html"><i class="fa fa-check"></i><b>3.1</b> The Basics of Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>3.1.1</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#how-to-make-the-decision-trees"><i class="fa fa-check"></i><b>3.1.1.1</b> How to make the decision trees</a>
<ul>
<li class="chapter" data-level="3.1.1.1.1" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#the-goal-of-regression"><i class="fa fa-check"></i><b>3.1.1.1.1</b> The goal of regression</a></li>
<li class="chapter" data-level="3.1.1.1.2" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#tree-pruning-algoritm"><i class="fa fa-check"></i><b>3.1.1.1.2</b> Tree Pruning &amp; Algoritm</a></li>
<li class="chapter" data-level="3.1.1.1.3" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#setting-contrains-of-the-tree-sise"><i class="fa fa-check"></i><b>3.1.1.1.3</b> Setting contrains of the tree sise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.1.2" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#classification-trees"><i class="fa fa-check"></i><b>3.1.2</b> Classification Trees</a></li>
<li class="chapter" data-level="3.1.3" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#tree-vs.-lienar-models"><i class="fa fa-check"></i><b>3.1.3</b> Tree vs. Lienar Models</a></li>
<li class="chapter" data-level="3.1.4" data-path="the-basics-of-decision-trees.html"><a href="the-basics-of-decision-trees.html#advantages-and-disadvantages-of-trees"><i class="fa fa-check"></i><b>3.1.4</b> Advantages and Disadvantages of Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bagging-random-forests-boosting.html"><a href="bagging-random-forests-boosting.html"><i class="fa fa-check"></i><b>3.2</b> Bagging, Random Forests, Boosting</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bagging-random-forests-boosting.html"><a href="bagging-random-forests-boosting.html#bagging-bootstrap-aggregation"><i class="fa fa-check"></i><b>3.2.1</b> Bagging (Bootstrap Aggregation)</a></li>
<li class="chapter" data-level="3.2.2" data-path="bagging-random-forests-boosting.html"><a href="bagging-random-forests-boosting.html#random-forests"><i class="fa fa-check"></i><b>3.2.2</b> Random Forests</a></li>
<li class="chapter" data-level="3.2.3" data-path="bagging-random-forests-boosting.html"><a href="bagging-random-forests-boosting.html#boosting-i.e.-gradient-boosting"><i class="fa fa-check"></i><b>3.2.3</b> Boosting (i.e. Gradient Boosting)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="application-in-r.html"><a href="application-in-r.html"><i class="fa fa-check"></i><b>3.3</b> Application in R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="application-in-r.html"><a href="application-in-r.html#decision-trees"><i class="fa fa-check"></i><b>3.3.1</b> Decision trees</a></li>
<li class="chapter" data-level="3.3.2" data-path="application-in-r.html"><a href="application-in-r.html#bagging"><i class="fa fa-check"></i><b>3.3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3.3" data-path="application-in-r.html"><a href="application-in-r.html#random-forests-1"><i class="fa fa-check"></i><b>3.3.3</b> Random Forests</a></li>
<li class="chapter" data-level="3.3.4" data-path="application-in-r.html"><a href="application-in-r.html#boosting"><i class="fa fa-check"></i><b>3.3.4</b> Boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="subject-1.html"><a href="subject-1.html"><i class="fa fa-check"></i><b>4</b> Subject 1</a></li>
<li class="chapter" data-level="5" data-path="subject-1-1.html"><a href="subject-1-1.html"><i class="fa fa-check"></i><b>5</b> Subject 1</a></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-basics-of-decision-trees" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> The Basics of Decision Trees</h2>
<p>The decision trees can be applied both for regression and classification. The following sections will firstly be about regression and then classification. A decision tree looks like the following:</p>
<div class="figure" style="text-align: center"><span id="fig:SubtreeHitters"></span>
<img src="Images/DecisionTree.png" alt="Decision Tree" width="222" />
<p class="caption">
Figure 3.1: Decision Tree
</p>
</div>
<p>We see that the illustration has <strong>2 internal nodes</strong> - years &lt; 4.5 and hits &lt; 117.5 – and <strong>3 terminal nodes</strong> (i.e. leaf/leaves). The leaves consist of the mean of the response given the criterias in the decision tree. The lines connecting the nodes (internal and terminal) are called branches.</p>
<p><em>Notice, that the same variables can occurs several times in the tree.</em></p>
<p>Each leaf can also be written as:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R_1=\)</span>{X|Years&lt;4.5}</li>
<li><span class="math inline">\(R_2=\)</span>{X|Years&gt;=4.5,Hits&lt;117.5}</li>
<li><span class="math inline">\(R_2=\)</span>{X|Years&gt;=4.5,HIts&gt;=117.5}</li>
</ol>
<p><strong><em>Terminology:</em></strong></p>
<ul>
<li>Root: best predictor</li>
<li>Splitting: This is when we do the splitting on the internal nodes / decision node</li>
<li>Leaf: The leafs contain the mean or the mode of the respons from the observations which meet the circumstances that are set in the splits.</li>
</ul>
<div id="regression-trees" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Regression Trees</h3>
<p>We can take an example:</p>
<div class="figure" style="text-align: center"><span id="fig:DecisionTree"></span>
<img src="Images/DecisionTreeHitters.png" alt="Decision Tree Hitters data" width="295" />
<p class="caption">
Figure 3.2: Decision Tree Hitters data
</p>
</div>
<p><em>Note: based on the hitters data.</em></p>
<p>We see that the observations has been separated into regions which meet the criteria, one can then take the mean of the response variable of the observations in the regions to define the terminal nodes. This may be a simplification although, it is easily interpreted.</p>
<p>It should also be mentioned that the criteria, also shown in previous section, indicate that we are working with hard boundaries, hence the regions does not overlap with each other.</p>
<div id="how-to-make-the-decision-trees" class="section level4" number="3.1.1.1">
<h4><span class="header-section-number">3.1.1.1</span> How to make the decision trees</h4>
<p>Precedure:</p>
<ol style="list-style-type: decimal">
<li>Divide the predictor space into regions, as in the example above. We end up with J dostinct and non overlapping regions, we call these <span class="math inline">\(R_1,R_2,...,R_J\)</span></li>
<li>For all observations in region <span class="math inline">\(R_J\)</span>, we take the mean of the response variable.</li>
</ol>
<p>Notice, that the way of optimizing RSS is with an approach starting on one varaible and then spreading out. We call this the <em>recursive binary splitting</em>, that is a top-down approach which is said to be greedy. In the very initial phase of the splitting procedure, all observations are in the same region, and then we start splitting up the regions, figure <a href="the-basics-of-decision-trees.html#fig:DTDims">3.3</a> show examples of having a more complex model and also including the mean of the response variable in the illustration.</p>
<p>Notice the top left illustration, this is not from a recursive binary splitting process, hence it yields more strange regions.</p>
<div id="the-goal-of-regression" class="section level5" number="3.1.1.1.1">
<h5><span class="header-section-number">3.1.1.1.1</span> The goal of regression</h5>
<p>We want to minimize the RSS. That can be written as:</p>
<p><span class="math display" id="eq:DTRSS">\[\begin{equation}
\text{}\sum_{j=1}^J\sum_{i\ \in R_j}^{ }\left(y_i-\hat{y}_{R_j}\right)^{^2}
\tag{3.1}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(J\)</span> = total number of regions</li>
<li><span class="math inline">\(j\)</span> = spefic number of regions in the range of 1 to J</li>
<li><span class="math inline">\(R_j\)</span> = each region, R for region i guess.</li>
<li><span class="math inline">\(i \in R_J\)</span> = the special e sign, means member ship off. Hence it can be seen as a filter on the specific regions.</li>
<li><span class="math inline">\(\hat{y}_{R_j}\)</span> = The predicted values in the specific regions, i.e. the mean response</li>
</ul>
<p>So what it says is that we take the sum of all squeard residuals. But then how do we split the regions? In general terms in can be written as the following:</p>
<p><span class="math display" id="eq:InternalNodesSplit">\[\begin{equation}
R_1\left(j,s\right)=\left\{X|X_j&lt;s\right\}\ AND\ R_2\left(j,s\right)=\left\{X|X_j\ge s\right\}
\tag{3.2}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(s\)</span> = cutpoint</li>
<li><span class="math inline">\(j\)</span> = reflectsion the regions, hence,</li>
<li><span class="math inline">\(X_j\)</span> = the x variable region.</li>
<li><span class="math inline">\((X|X_j&lt;s)\)</span> = the region of predictor space in which <span class="math inline">\(X_j\)</span> takes o a value less than s (the cutpoint)</li>
</ul>
<p>Thus in general terms we wish to select j and s that minimize the RSS, therefore we can also write the equation for RSS in general terms with:</p>
<p><span class="math display" id="eq:InternalNodesSplit">\[\begin{equation}
\sum_{i:\ e_i\in R_1\left(j,s\right)}^{ }\left(y_i-\hat{y}_{R_1}\right)^{^2}+\sum_{i:\ e_i\in R_2\left(j,s\right)}^{ }\left(y_i-\hat{y}_{R_2}\right)^{^2}
\tag{3.2}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(R_1(j,s)\)</span> define the training observations, which are a basis for <span class="math inline">\(\hat{y}_{R_1}\)</span></li>
</ul>
<p><strong><em>Adding third dimension - response variable:</em></strong></p>
<p>Basically software can quite quickly compute cutpoints of the x variables to optimize RSS. The example above in <a href="the-basics-of-decision-trees.html#fig:DecisionTree">3.2</a> express two dimensions, one could also have added the response variable as a dimension. Where the fitted y (mean of response in region <span class="math inline">\(R_j\)</span>) will express the hight in the specifc region.</p>
<p><strong><em>Adding more cutpoints:</em></strong></p>
<p>Naturally we could also have hadded more cutpoints, to separate the regions even further. See an example in the following:</p>
<div class="figure" style="text-align: center"><span id="fig:DTDims"></span>
<img src="Images/DTDims.png" alt="Decision Tree Hitters data" width="455" />
<p class="caption">
Figure 3.3: Decision Tree Hitters data
</p>
</div>
<p><em>Note: the top left show regions that is not from the procedure listed above, namely the recusive binary splitting</em></p>
</div>
<div id="tree-pruning-algoritm" class="section level5" number="3.1.1.1.2">
<h5><span class="header-section-number">3.1.1.1.2</span> Tree Pruning &amp; Algoritm</h5>
<p>The more regions that you add, the more complexity and hence flexibilty. Thus, sometimes it is a good idea to have rather simple trees, to avoid fitting too much to specific observations. Also I recon, that in regions with few observations we are prone to overfitting in the specific regions.</p>
<p><strong><em>Considerations on making a stable tree</em></strong></p>
<p>One can perhaps add nodes until you don’t lower RSS, like forward selection, although sometimes a significant improvement of the model may come after a certain cutpoint, hence that is not a good approach.</p>
<p>A better approach is the opposite, where we start with making a huge tree (this we call <span class="math inline">\(T_0\)</span>) and cuts it down a subtree (merely cutting branches off). To do so, one can do <em>cost complexity pruning</em>, i.e. <em>weakest link pruning</em>. To do so we introduce <span class="math inline">\(\alpha\)</span>, which is a nonnegative tuning parameter. This can be compared with backward selection. See examples:</p>
<ul>
<li>T0: figure <a href="the-basics-of-decision-trees.html#fig:T0Hitters">3.4</a></li>
<li>MSE with different alpha values in figure <a href="the-basics-of-decision-trees.html#fig:TreePruningHitters">3.5</a></li>
<li>Selected subtree in figure <a href="the-basics-of-decision-trees.html#fig:SubtreeHitters">3.1</a></li>
</ul>
<p><span class="math inline">\(\alpha\)</span> behaves in the following way:</p>
<ul>
<li>If <span class="math inline">\(\alpha\)</span> = 0, then the subtree = <span class="math inline">\(T_0\)</span>, hence the big tree and therefore controls the complexity of the model</li>
<li>As <span class="math inline">\(\alpha\)</span> increases we prune <span class="math inline">\(T_0\)</span> into a subtree.</li>
</ul>
<p>Thus the higher the tuning parameter, the smaller the tree. This can be compared with the lasso <span class="citation">(<a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al. 2013</a>)</span>. Thus, for each <span class="math inline">\(\alpha\)</span> there corresponds a subtree of <span class="math inline">\(T_0\)</span>, i.e. <span class="math inline">\(T \subset T_0\)</span>. This can be written as:</p>
<p><span class="math display" id="eq:Subtree">\[\begin{equation}
\sum_{m=1}^{\left|T\right|}\sum_{i:\ x_i\in R_m}^{ }\left(y_{i\ }-\hat{y}_{R_m}\right)^{^{2\ }}+\ \alpha\left|T\right|
\tag{3.3}
\end{equation}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(|T|\)</span> = the number of terminal nodes of the tree <span class="math inline">\(T,R_m\)</span> (the subset of predictor space)</li>
</ul>
<p>Therefore we can write the procedure for building a regression tree with:</p>
<ol style="list-style-type: decimal">
<li>Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fwer than some minumum number of observations (to avoid overfitting). See an example of T0 in figure <a href="the-basics-of-decision-trees.html#fig:T0Hitters">3.4</a></li>
<li>Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of <span class="math inline">\(\alpha\)</span></li>
<li>Use K-fold cross validation to choose <span class="math inline">\(\alpha\)</span>. That is, divide the training observation into K-folds. For each <span class="math inline">\(k=1,...,K\)</span>: Where we do the following:
<ol style="list-style-type: lower-alpha">
<li>Repat steps 1 and 2 on all but the kth fold of the training data</li>
<li>Evaluate the mena squared prediction error in the left-out kth fold, as a function of <span class="math inline">\(\alpha\)</span>. Average the results for each value of alpha and pick alpha to minimuzze the average error.</li>
</ol></li>
<li>Return the subtree from step 2 that corresponds to the chosen value of <span class="math inline">\(\alpha\)</span>.
<ol style="list-style-type: lower-alpha">
<li>It may be advantagous to plot the pruning process to actually see how the removed branches improve the model and select a subtree of reason instead of being blinded by the absolute lowest prediction error, see an example hereof in figure <a href="the-basics-of-decision-trees.html#fig:TreePruningHitters">3.5</a></li>
</ol></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:T0Hitters"></span>
<img src="Images/T0Hitters.png" alt="T0 Hitters" width="592" />
<p class="caption">
Figure 3.4: T0 Hitters
</p>
</div>
<p><em>We see the big tree, that can be pruned to optimize prediction, the following illustration plots MSE using different tuning parameters</em></p>
<div class="figure" style="text-align: center"><span id="fig:TreePruningHitters"></span>
<img src="Images/TreePruningHitters.png" alt="Tree Pruning Hitters" width="420" />
<p class="caption">
Figure 3.5: Tree Pruning Hitters
</p>
</div>
<p><em>We see that the lowest train and CV MSE is at the tree size of 10, although between three terminal nodes and 10 terminal nodes appear to be equally good, hence we go for the most parsimonious model, see that tree in figure <a href="the-basics-of-decision-trees.html#fig:SubtreeHitters">3.1</a></em></p>
<p><br />
</p>
</div>
<div id="setting-contrains-of-the-tree-sise" class="section level5" number="3.1.1.1.3">
<h5><span class="header-section-number">3.1.1.1.3</span> Setting contrains of the tree sise</h5>
<p>An alternative to tree pruning is contraining the size of the tree. That can be done with four approaches:</p>
<ol style="list-style-type: decimal">
<li>Maximum depth of the tree (vertical depth) - tuned using CV</li>
<li>Minimum observations for a node split - tuned using CV</li>
<li>Minimum observations for a terminal node (lead) - lower values for inbalanced classes</li>
<li>Maximum number of terminal nodes - can be defined instead of depth.</li>
</ol>
</div>
</div>
</div>
<div id="classification-trees" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Classification Trees</h3>
<p>The procedure is very much the same as what we saw for regression trees, where the same pruning process etc. In the classification setting we predict qualitative outcomes, e.g., yes/no. We can either select based on frequency (i.e. most commonly occuring i.e. mode) or the proportions.</p>
<p>In classification we use error rate instead of RSS, as we can’t do RSS in a classification setting. I guess we can construct a confusion matrix as well.</p>
<p>We introduce a new term: <em>node purity</em>. This is about including internal nodes that leads to two terminal nodes which has the same conclusion. It will not reduce the classification error, it does improve the certainty of the classification. Therefore, to account for the certainty and thus the node purity, one should also make the <em>Gini Index</em> and the <em>Entropy</em>, equations can be found in <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>, p. 312.</p>
<p><strong><em>More on purity: as we classify based on the mode of the response variable, we intentionally misclassify some observations. Naturally, we want to avoid this. Thus we can increase purity, that is often seen by splitting the same variable consecutive times or just several times. Hence to circle in the actual classifications. Although, doing this too much leads to overfitting to the data. That is why the Gini and Entropy adds a penalty to the in-sample error and thus accounting for purity. But one should also make an out of sample validation</em></strong></p>
<p>Thus the splits are made to optimize the Gini, Entropy or perhaps Chi-Squared.</p>
<p>If we want to write it in math, we can do the following:</p>
<p><span class="math display">\[y = f(x_1,x_2)\]</span></p>
<p>Where we see that y is a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span></p>
</div>
<div id="tree-vs.-lienar-models" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Tree vs. Lienar Models</h3>
<p>One is never strictlu better than the other. But in general, one should start with a simple model, such as linear regression, to have a baseline, which we can compare more advanced mdoels with.</p>
<p>If the actual varaince of the response variable can be explained by linear regression, then it is likely to outperform a tree model, but if one experience more complex data, then it is likely that the tree model will outperform the linear model.</p>
</div>
<div id="advantages-and-disadvantages-of-trees" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Advantages and Disadvantages of Trees</h3>
<p>This is compared to classical approaches, linear regression and classification <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>, CH3 and CH 4.</p>
<p><strong><em>Advantges:</em></strong></p>
<ol style="list-style-type: decimal">
<li>Trees are very easy to explain to people. And also easy to visualize and understand.</li>
<li>Some argues that decision trees better mimiic human reasoning and decision making.</li>
<li>Can be shown graphically</li>
<li>One don’t have to construct dummy variables.</li>
<li>Simple and useful for interpretation</li>
<li>No need to transform features</li>
<li>Less cleaning required and not so sensible to outliers</li>
<li>No need to transform features</li>
<li>No assumptions, purely data driven</li>
<li>Capturing interactions between features</li>
<li>Ensemble trees compete with neural networks in terms of predictions</li>
<li>Useful in data exploration and feature selection</li>
</ol>
<p><strong><em>Disadvantages:</em></strong></p>
<ol style="list-style-type: decimal">
<li>Trees is typically outperformed by more advanced methods, included in <span class="citation"><a href="references.html#ref-hastie2013" role="doc-biblioref">Hastie et al.</a> (<a href="references.html#ref-hastie2013" role="doc-biblioref">2013</a>)</span>.</li>
<li>Trees are not robust and small changes in the data may lead to big changes in the model.</li>
<li>It is easy to overfit the data, hence high variance and low bias. This is dealt with by using the ensemble approach, hereunder, bagging, random forests and boosting.</li>
</ol>
<p>Although decision trees can be advanced with:</p>
<ol style="list-style-type: decimal">
<li>Bagging</li>
<li>Random Forests</li>
<li>Boosting</li>
</ol>
<p>To improve prediction power and therefore compete with other advanced models.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-based-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bagging-random-forests-boosting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
