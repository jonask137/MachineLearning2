<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Models Beyond Linearity | Machine Learning for Business Intelligence 2</title>
  <meta name="description" content="2.1 Models Beyond Linearity | Machine Learning for Business Intelligence 2" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Models Beyond Linearity | Machine Learning for Business Intelligence 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Models Beyond Linearity | Machine Learning for Business Intelligence 2" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moving-beyond-linearity.html"/>
<link rel="next" href="lecture-notes.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>setup</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>2</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html"><i class="fa fa-check"></i><b>2.1</b> Models Beyond Linearity</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#polynomial-regression"><i class="fa fa-check"></i><b>2.1.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="2.1.1.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#beta-coefficients-and-variance"><i class="fa fa-check"></i><b>2.1.1.1</b> Beta coefficients and variance</a></li>
<li class="chapter" data-level="2.1.1.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#application-procedure"><i class="fa fa-check"></i><b>2.1.1.2</b> Application procedure</a></li>
</ul></li>
<li class="chapter" data-level="2.1.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#step-functions"><i class="fa fa-check"></i><b>2.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="2.1.3" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#regression-splines"><i class="fa fa-check"></i><b>2.1.3</b> Regression Splines</a>
<ul>
<li class="chapter" data-level="2.1.3.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#piecewise-polynomials"><i class="fa fa-check"></i><b>2.1.3.1</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="2.1.3.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#constraints-and-splines"><i class="fa fa-check"></i><b>2.1.3.2</b> Constraints and Splines</a></li>
<li class="chapter" data-level="2.1.3.3" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#choosing-the-number-and-location-of-the-knots"><i class="fa fa-check"></i><b>2.1.3.3</b> Choosing the number and location of the Knots</a></li>
<li class="chapter" data-level="2.1.3.4" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#degrees-of-freedom"><i class="fa fa-check"></i><b>2.1.3.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="2.1.3.5" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#comparison-with-polynomial-regression"><i class="fa fa-check"></i><b>2.1.3.5</b> Comparison with Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.1.4" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#smoothing-splines"><i class="fa fa-check"></i><b>2.1.4</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="2.1.4.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#choosing-optimal-tuning-parameter"><i class="fa fa-check"></i><b>2.1.4.1</b> Choosing optimal tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.1.5" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#local-regression"><i class="fa fa-check"></i><b>2.1.5</b> Local Regression</a></li>
<li class="chapter" data-level="2.1.6" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#generalized-additive-models"><i class="fa fa-check"></i><b>2.1.6</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="2.1.6.1" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#gam-for-regression-problems"><i class="fa fa-check"></i><b>2.1.6.1</b> GAM for regression problems</a></li>
<li class="chapter" data-level="2.1.6.2" data-path="models-beyond-linearity.html"><a href="models-beyond-linearity.html#gam-for-classification-problems"><i class="fa fa-check"></i><b>2.1.6.2</b> GAM for classification problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>2.2</b> Lecture notes</a></li>
<li class="chapter" data-level="2.3" data-path="lab-section.html"><a href="lab-section.html"><i class="fa fa-check"></i><b>2.3</b> Lab section</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lab-section.html"><a href="lab-section.html#polynomial-regression-and-step-functions"><i class="fa fa-check"></i><b>2.3.1</b> Polynomial Regression and Step Functions</a>
<ul>
<li class="chapter" data-level="2.3.1.1" data-path="lab-section.html"><a href="lab-section.html#continous-model"><i class="fa fa-check"></i><b>2.3.1.1</b> Continous model</a></li>
<li class="chapter" data-level="2.3.1.2" data-path="lab-section.html"><a href="lab-section.html#logarithmic-model"><i class="fa fa-check"></i><b>2.3.1.2</b> Logarithmic model</a></li>
<li class="chapter" data-level="2.3.1.3" data-path="lab-section.html"><a href="lab-section.html#step-function"><i class="fa fa-check"></i><b>2.3.1.3</b> Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.3.2" data-path="lab-section.html"><a href="lab-section.html#splines"><i class="fa fa-check"></i><b>2.3.2</b> Splines</a>
<ul>
<li class="chapter" data-level="2.3.2.1" data-path="lab-section.html"><a href="lab-section.html#basis-function-splines"><i class="fa fa-check"></i><b>2.3.2.1</b> Basis Function Splines</a></li>
<li class="chapter" data-level="2.3.2.2" data-path="lab-section.html"><a href="lab-section.html#natural-splines"><i class="fa fa-check"></i><b>2.3.2.2</b> Natural Splines</a></li>
<li class="chapter" data-level="2.3.2.3" data-path="lab-section.html"><a href="lab-section.html#smooth-splines"><i class="fa fa-check"></i><b>2.3.2.3</b> Smooth Splines</a></li>
<li class="chapter" data-level="2.3.2.4" data-path="lab-section.html"><a href="lab-section.html#local-regression-1"><i class="fa fa-check"></i><b>2.3.2.4</b> Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3.3" data-path="lab-section.html"><a href="lab-section.html#gams"><i class="fa fa-check"></i><b>2.3.3</b> GAMs</a>
<ul>
<li class="chapter" data-level="2.3.3.1" data-path="lab-section.html"><a href="lab-section.html#with-only-natural-splines"><i class="fa fa-check"></i><b>2.3.3.1</b> With only natural splines</a></li>
<li class="chapter" data-level="2.3.3.2" data-path="lab-section.html"><a href="lab-section.html#with-different-splines"><i class="fa fa-check"></i><b>2.3.3.2</b> With different splines</a></li>
<li class="chapter" data-level="2.3.3.3" data-path="lab-section.html"><a href="lab-section.html#but-what-variables-to-include"><i class="fa fa-check"></i><b>2.3.3.3</b> But what variables to include?</a></li>
<li class="chapter" data-level="2.3.3.4" data-path="lab-section.html"><a href="lab-section.html#gam-with-local-regression"><i class="fa fa-check"></i><b>2.3.3.4</b> GAM with local regression</a></li>
<li class="chapter" data-level="2.3.3.5" data-path="lab-section.html"><a href="lab-section.html#logistic-regression"><i class="fa fa-check"></i><b>2.3.3.5</b> Logistic Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="exercises.html"><a href="exercises.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a>
<ul>
<li class="chapter" data-level="2.4.1.1" data-path="exercises.html"><a href="exercises.html#a-polynomial-regression"><i class="fa fa-check"></i><b>2.4.1.1</b> 6.a Polynomial Regression</a></li>
<li class="chapter" data-level="2.4.1.2" data-path="exercises.html"><a href="exercises.html#b-step-function"><i class="fa fa-check"></i><b>2.4.1.2</b> 6.b Step function</a></li>
</ul></li>
<li class="chapter" data-level="2.4.2" data-path="exercises.html"><a href="exercises.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises.html"><a href="exercises.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises.html"><a href="exercises.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a>
<ul>
<li class="chapter" data-level="2.4.4.1" data-path="exercises.html"><a href="exercises.html#a-using-poly-function-to-fit-cubic-polynomial-regression"><i class="fa fa-check"></i><b>2.4.4.1</b> (a) using poly function to fit cubic polynomial regression</a></li>
<li class="chapter" data-level="2.4.4.2" data-path="exercises.html"><a href="exercises.html#b-plotting-polynomial-fits-for-a-range-of-polynomials"><i class="fa fa-check"></i><b>2.4.4.2</b> (b) Plotting polynomial fits for a range of polynomials</a></li>
<li class="chapter" data-level="2.4.4.3" data-path="exercises.html"><a href="exercises.html#c-using-cv-to-select-best-degree-of-d"><i class="fa fa-check"></i><b>2.4.4.3</b> (c) Using CV to select best degree of d</a></li>
<li class="chapter" data-level="2.4.4.4" data-path="exercises.html"><a href="exercises.html#d-use-bs-to-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.4</b> (d) Use <code>bs()</code> to fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.5" data-path="exercises.html"><a href="exercises.html#e-now-fit-a-regression-spline"><i class="fa fa-check"></i><b>2.4.4.5</b> (e) Now fit a regression spline</a></li>
<li class="chapter" data-level="2.4.4.6" data-path="exercises.html"><a href="exercises.html#f-perform-cross-validation-to-select-degrees"><i class="fa fa-check"></i><b>2.4.4.6</b> (f) Perform cross-validation, to select degrees</a></li>
</ul></li>
<li class="chapter" data-level="2.4.5" data-path="exercises.html"><a href="exercises.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a>
<ul>
<li class="chapter" data-level="2.4.5.1" data-path="exercises.html"><a href="exercises.html#a-partitioning-the-data"><i class="fa fa-check"></i><b>2.4.5.1</b> (a) Partitioning the data</a></li>
<li class="chapter" data-level="2.4.5.2" data-path="exercises.html"><a href="exercises.html#b-fitting-a-gam"><i class="fa fa-check"></i><b>2.4.5.2</b> (b) Fitting a GAM</a></li>
<li class="chapter" data-level="2.4.5.3" data-path="exercises.html"><a href="exercises.html#c-evaluating-on-the-test-set"><i class="fa fa-check"></i><b>2.4.5.3</b> (c) Evaluating on the test set</a></li>
<li class="chapter" data-level="2.4.5.4" data-path="exercises.html"><a href="exercises.html#d-which-variables-appear-to-have-a-non-linear-relationship"><i class="fa fa-check"></i><b>2.4.5.4</b> (d) Which variables appear to have a non linear relationship?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html"><i class="fa fa-check"></i><b>2.5</b> Predicting the Return on Advertising Spent</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#background"><i class="fa fa-check"></i><b>2.5.1</b> 1. Background</a></li>
<li class="chapter" data-level="2.5.2" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#case-study-business-understanding-phase"><i class="fa fa-check"></i><b>2.5.2</b> 2. Case study (Business Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.3" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#the-data-data-understanding-phase"><i class="fa fa-check"></i><b>2.5.3</b> 3. The data (Data Understanding Phase)</a></li>
<li class="chapter" data-level="2.5.4" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#specific-requirements"><i class="fa fa-check"></i><b>2.5.4</b> 4. Specific requirements:</a>
<ul>
<li class="chapter" data-level="2.5.4.1" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#task-1---import-and-overview"><i class="fa fa-check"></i><b>2.5.4.1</b> 4.1 Task 1 - Import and overview</a></li>
<li class="chapter" data-level="2.5.4.2" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#task-2---data-inspection"><i class="fa fa-check"></i><b>2.5.4.2</b> 4.2 Task 2 - Data inspection</a></li>
<li class="chapter" data-level="2.5.4.3" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#task-3---building-different-models"><i class="fa fa-check"></i><b>2.5.4.3</b> 4.3 Task 3 - Building different models</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#a-generalized-additive-model-gam-to-predict-roas"><i class="fa fa-check"></i><b>2.5.4.3.1</b> A Generalized Additive Model (GAM) to predict ROAS</a>
<ul>
<li class="chapter" data-level="2.5.4.3.1.1" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#c1-feature-selection-using-regsubsets"><i class="fa fa-check"></i><b>2.5.4.3.1.1</b> c1) Feature selection using regsubsets()</a></li>
<li class="chapter" data-level="2.5.4.3.1.2" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#c2-feature-selection-using-step.gam"><i class="fa fa-check"></i><b>2.5.4.3.1.2</b> c2) Feature selection using step.GAM</a></li>
<li class="chapter" data-level="2.5.4.3.1.3" data-path="predicting-the-return-on-advertising-spent.html"><a href="predicting-the-return-on-advertising-spent.html#c3-fetaure-selection-using-random-forest"><i class="fa fa-check"></i><b>2.5.4.3.1.3</b> c3) Fetaure selection using random forest</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>3</b> Tree Based Methods</a></li>
<li class="chapter" data-level="4" data-path="subject-1.html"><a href="subject-1.html"><i class="fa fa-check"></i><b>4</b> Subject 1</a></li>
<li class="chapter" data-level="5" data-path="subject-1-1.html"><a href="subject-1-1.html"><i class="fa fa-check"></i><b>5</b> Subject 1</a></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Business Intelligence 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="models-beyond-linearity" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Models Beyond Linearity</h2>
<p>Notice that all approaches despite GAM are extensions of simple linear regression, as it only takes on one predictor variable.</p>
<div id="polynomial-regression" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Polynomial Regression</h3>
<p>Can be defined by the following</p>
<p><span class="math display" id="eq:PolynomialRegression">\[\begin{equation}
y_{i\ }=\ \beta_0+\beta_1x_i+\beta_2x_i^2+...+\ \beta_dx_i^d\ +\ \epsilon_i
\tag{2.1}
\end{equation}\]</span></p>
<p>Rules of thumb:</p>
<ul>
<li>We don’t take on more than 3 or 4 degrees of d, as that yields strange lines</li>
</ul>
<p>Note that we can still use standard errors for coefficient estimates.</p>
<div id="beta-coefficients-and-variance" class="section level4" number="2.1.1.1">
<h4><span class="header-section-number">2.1.1.1</span> Beta coefficients and variance</h4>
<p>Each beta coefficient has its own variance (just as in linear regression).</p>
<p>It can be defined by a matrix of j dimensions, e.g., if you have 5 betas (including beta 0) we can construct the correlation matrix.</p>
<p>Covariance matrix can be identified by <span class="math inline">\(\hat{C}\)</span>.</p>
<p>Generally we get point estimates, but it is also interesting to show the confidence intervals (using 2 standard errors).</p>
<p>Notice, that we cant really interprete beta coefficients as we do with linear regression, hence we dont have the same ability to do inference as the coefficients are missleading.</p>
</div>
<div id="application-procedure" class="section level4" number="2.1.1.2">
<h4><span class="header-section-number">2.1.1.2</span> Application procedure</h4>
<ol style="list-style-type: decimal">
<li>Use <code>lm()</code> or <code>glm()</code></li>
<li>Use DV~poly(IV,degree)</li>
<li>Perform CV with <code>cv.glment()</code> / aonva F-test to select degree
<ul>
<li>This is basically either visually selecting the degrees that are the best using CV or using an ANOVA to assess if the MSE are significantly different from each other, hence an ANOVA test. Regarding ANOVA, if there stop being significance, then significant changes, e.g., in a poly 8, where the previous polynomials was not significant, then we can also disregard the 8’th polynomial.</li>
</ul></li>
<li>Fit the selected model</li>
<li>Look at <code>coef(summary(fit))</code></li>
<li>Plot data and predictions with <code>predict()</code></li>
<li>Check residuals</li>
<li>Interpret</li>
</ol>
<p>The lecture shows exercise number 6 in chapter 7.</p>
</div>
</div>
<div id="step-functions" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Step Functions</h3>
<p>This is literally just fitting a constant in different bins, see example on page 269. This is also called discretizing x. Thus it is not polynoomial, but it is non linear.</p>
<p>It is often applied when we see, e.g., five year age bins, e.g., a 20-25 year old is expected to earn so and so much etc. But we can only really use it, when there are natural cuts, hence one must be considerate using the model.</p>
<p><strong>Major disadvantage:</strong> If there are no natural breakpoints, then the model is likely to miss variance and also generalize too much.</p>
<p>Remember, that the steps reflect the average increase in Y for each step. Hence the first bin (range) is defined by <span class="math inline">\(\beta_0\)</span>, and can be regarded as the average of that range of x. Thus each coefficient of the ranges of x is to be understood as the average increase of response.</p>
<p>In other words, <span class="math inline">\(\beta_0\)</span> is the reference level, where the following cuts reflect the average increase or decrease hereon.</p>
<p>Note that we can still use standard errors for coefficient estimates.</p>
</div>
<div id="regression-splines" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Regression Splines</h3>
<div id="piecewise-polynomials" class="section level4" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Piecewise Polynomials</h4>
<p>This is basically polynomial regression, where the coefficients apply to specified ranges of X. The points where the coefficients changes are called <strong><em>knots</em></strong>. Hence a cubic function will look like the following.</p>
<p>Notice, that a piecewise polynomial function with no knots, is merely a standard polynomial function.</p>
<p><span class="math display" id="eq:PiecewisePolynomials">\[\begin{equation}
y_{i\ }=\ \beta_0+\beta_1x_i+\beta_2x_i^2+\ \beta_3x_i^3\ +\ \epsilon_i
\tag{2.2}
\end{equation}\]</span></p>
<p>Where it can be extended to be written for each range.</p>
<p>The coefficients can then be written with: <span class="math inline">\(\beta_{01},\beta_{11},\beta_{21}\)</span> etc. and for the second set: <span class="math inline">\(\beta_{02},\beta_{12},\beta_{22}\)</span></p>
<p>And for each of the betas in all of the cuts, you add one degree of freedom.</p>
<p>Rule of thumb:</p>
<ul>
<li>The more knots, the more complex = the more variance capture and noise trailed, hence low model bias but large model variance.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="Images/Piecewise%20regression.png" alt="Piecewise Polynomials" width="488" />
<p class="caption">
Figure 2.1: Piecewise Polynomials
</p>
</div>
</div>
<div id="constraints-and-splines" class="section level4" number="2.1.3.2">
<h4><span class="header-section-number">2.1.3.2</span> Constraints and Splines</h4>
<p><strong>It is a spline when the piecewise polynomials have been imposed with restrictions for continouity and derivatives, so the knot don’t break or so to say, hence the knot will not be visible.</strong></p>
<p>Figure 7.3 show how the splines look, as the top left window tell, the jump is rather odd. Hence, we can force the fit to be continuous, by imposing a constraint.</p>
<p>Notice, that it is a piecewise polynomial regression, when you merely fit polynomials onto bins of data. If you want to have the ends tied together, you impose contraints and thus created a spline.</p>
<p>We can further constrain the model, with adding derivatives of the functions, hence the first derivative and the second derivative (where it in this instance created linear splines, that is because we have added <span class="math inline">\(d-1\)</span> i.e. 2 derivatives, if the function was to the power of 4, then we should have imposed 3 derivatives to achieve linearity in the splines.)</p>
<p>Hence, the <em>linear spline</em> can be defined by: It is piecewise splines of degree-d polynomials, with continuity in derivatives up to degree <span class="math inline">\(d-1\)</span> at each knot.</p>
<p>Hence we have the following constraints:</p>
<ol style="list-style-type: decimal">
<li>Continuity</li>
<li>Derivatives</li>
</ol>
</div>
<div id="choosing-the-number-and-location-of-the-knots" class="section level4" number="2.1.3.3">
<h4><span class="header-section-number">2.1.3.3</span> Choosing the number and location of the Knots</h4>
<p>Choosing amount of knots? One may ask themself, how many degrees of freedom do you want to include in the model?</p>
<p>Amount of knots is therefore corresponding to amount of degrees of freedom.</p>
<p>We can let software estimate the best amount and the best locations. Here one can use:</p>
<ol style="list-style-type: decimal">
<li>In-sample performance</li>
<li>Out-of-sample performance, e.g. with CV, perhaps extent to K folds with K tests, to ensure, that each variable has been held out once.</li>
</ol>
<p>This can be followed by visualizing the MSE for the different simulations with different amount of knots.</p>
</div>
<div id="degrees-of-freedom" class="section level4" number="2.1.3.4">
<h4><span class="header-section-number">2.1.3.4</span> Degrees of freedom</h4>
<p>You count the amount of coefficients in the piecewise polynomial. Then you deduct df as you impose restrictions.</p>
<p>e.g. with one knot, you impose conituity, then you deduct one 1 df. for each derivative that we impose we can subtract one df.</p>
<p>Therefore the example above: 8 df in the beginning less 1 for the cut and two for the two derivatives. Hence, we end up with 5 df.</p>
</div>
<div id="comparison-with-polynomial-regression" class="section level4" number="2.1.3.5">
<h4><span class="header-section-number">2.1.3.5</span> Comparison with Polynomial Regression</h4>
<p>With regression splines we are able to introduce knots that account for variance as it slightly resets the model in each section, hence we can fit the model to the data without having to impose as much complexity as we would in normal polynomial regression.</p>
<p>Hence one often observes that regression splines have more stability than polynomial regression.</p>
</div>
</div>
<div id="smoothing-splines" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Smoothing Splines</h3>
<p>In prediction this is slightly better than basic functions and natural splines.</p>
<p>This is basically attempting to find a model that captures the variance by a smoothing line. Doing so, we fit a very flexible model and impose restrictions upon this, to achieve a shrunken model, just as with Lasso and Ridge Regression. Thus, the smoothing (imposing restrictions) deals with overfitting.</p>
<p>Also as we havee discovered previously, that degrees of freedom is equivilant with amount of knots, e.g., in polynomial splines, then three knots in a cubic function leads to 6 degrees of freedom, hence an smooth spline with df = 6.8 can be said to approximately have 3 knots, but we will never really know.</p>
<p>This can be defined as a cubic spline with knot at every unique value of <span class="math inline">\(x_i\)</span></p>
<p>Hence we have the following model:</p>
<p><span class="math display" id="eq:SmoothingSplines">\[\begin{equation}
RSS=\sum_{i=1}^n\left(y_i-g\left(x_i\right)\right)^{^2}+\lambda\int g&#39;&#39;\left(t\right)^{^2}dt
\tag{2.3}
\end{equation}\]</span></p>
<p><em>i.e. Loss + Penalty</em></p>
<p><em>Recall that the penalty controls the curvature of the function</em></p>
<p>Where:</p>
<ul>
<li>We define model g(x)</li>
<li><span class="math inline">\((y_i-g(x_i))^{^2}\)</span> = the loss, meaning the difference between the fitted model and the actual y’s</li>
<li><span class="math inline">\(\lambda\)</span> = the tuning parameter, hence the restriction that we want to impose. If lambda is low, then much flexibility, if lambda is high, then low flexibility. Hence, controls the bias variance tradeoff.</li>
<li><span class="math inline">\(\int g&#39;&#39;\left(t\right)^{^2}dt\)</span> = a measure of how much <span class="math inline">\(g&#39;(x)\)</span> changes oer time. Hence, the higher we set <span class="math inline">\(\lambda\)</span> the more imposed restrictions, meaning the smoother the model, as lambda gets closer to infinity, the model becomes linear.</li>
</ul>
<div id="choosing-optimal-tuning-parameter" class="section level4" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> Choosing optimal tuning parameter</h4>
<p>The analytic LOOCV can be calculated, the procedure appears to be the same as for lasso and ridge regression. The book (page 279) describes this a bit in details. However it says that software is able to do this.</p>
<p>Basically what is done, is LOOCV and simulating different tuning parameters to assess what model that performs the best.</p>
<p>Notice, that in R we are not working with <span class="math inline">\(\lambda\)</span>, but we can control the df associated with lambda. Or we can just let the model choose.</p>
<p>With this, degrees of freedom is not the same as we are used to. This creates sparsity as we know from regularization.</p>
<p><em>We are not expected to explain this, but we should be able to interpret the results</em></p>
</div>
</div>
<div id="local-regression" class="section level3" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Local Regression</h3>
<p>This i basically fitting a linear regression to each x, where s observations are included in the fitting procedure. Thus, one creates several fits, that are based on the observations weighted, where observations close to <span class="math inline">\(x_0\)</span> (the center of the regression) are given the highest weight and then the weight is gradually decreasing.</p>
<p>This is often really good when you have outliers, as you define how big a neighborhood you want to evaluate (also called the span, e.g. span of 0.5 = 50% of the observations).</p>
<p>This can be visualized with:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="Images/Local%20Regression.png" alt="Local regression" width="494" />
<p class="caption">
Figure 2.2: Local regression
</p>
</div>
<p>Doing local regression has the following procedure (algorithm)</p>
<ol style="list-style-type: decimal">
<li>Gather the fraction <span class="math inline">\(s = k/n\)</span> of training points whose <span class="math inline">\(x_i\)</span> are closest to <span class="math inline">\(x_0\)</span>.</li>
<li>Assign a weight <span class="math inline">\(K_{i0} = K(x_i, x_0)\)</span> to each point in this neighborhood, so that the point furthest from x0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.</li>
<li>Fit a weighted least squares regression of the <span class="math inline">\(y_i\)</span> on the <span class="math inline">\(x_i\)</span> using the aforementioned weights, by finding <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> that minimize</li>
</ol>
<p><span class="math display" id="eq:LocalRegression">\[\begin{equation}
\sum_{i=1}^nK_{i0}\left(y_i-\beta_0-\beta_1x_i\right)^{^2}
\tag{2.4}
\end{equation}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>The fitted value at <span class="math inline">\(x_0\)</span> is given by <span class="math inline">\(\hat{f}(x_0)=\hat\beta_0+\hat\beta_1x_0\)</span></li>
</ol>
<p>Where we see how the model is</p>
</div>
<div id="generalized-additive-models" class="section level3" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> Generalized Additive Models</h3>
<p>This can naturally both be applied in regression and classification problems, futher elaborated in the following.</p>
<p>It is called generalized, as the dependent variable can be both continuous (e.g. Gaussian) and categorical (e.g., binomial, Poisson, or other distributions) distributed</p>
<p>Additive = the model is adding different polynomials of the IDV toghether. Notice, as the model is additive, it does not account for interactions, then you have to specify the interactions.</p>
<p>Thus GAM is merely an approach to make a model, where we include the posibility of having non linear components. Hence we include more complex model (with the ability to trail the observations more than linear models).</p>
<p>But the advantage of linear regressions, are that we are able to quickly deduct the effects the variables. Although we dont always have a linear relationship, hence you can be forced to choose a more complex model. (see the R file “GAMs with discussion R”).</p>
<p>We have previously worked with non parametric models (e.g., KNN regression). GAM is in between linear regression and non parametric models.</p>
<p>That is the beuty of GAMs, as we preserve the ability of having transparancy in the model, despite it coming at a cost of worse prediction power than neural networks, but at such complex models, you are not able to deduct how the variables are interrelated, you can only say which are important and which are not.</p>
<div id="gam-for-regression-problems" class="section level4" number="2.1.6.1">
<h4><span class="header-section-number">2.1.6.1</span> GAM for regression problems</h4>
<p>Now we move beyond being constrained to only one predictor variable, hence GAM can be seen more as an extension of multiple linear regression. Hence GAM is a combination of different functions, where they are each fitted while holding the other variables fixed. GAM can consist of any different non-linear model, e.g., we can just as well use local regression, polynomial regression, or any combination of the approaches seen above in this subsection.</p>
<p>See section @ref(fig:GAMPlotLab7.8.3) for explanation of interpretation of the plots.</p>
<p><strong>Disadvantages of GAM:</strong></p>
<ol style="list-style-type: decimal">
<li>The fitting procedure holds the other variables fixed, hence it does not count for interactions. Therefore, one may manually construct interaction variables to account for this, just like in mulitple linear regression.</li>
<li>Prediction wise it is not competitive with Neural Networks and Support Vector Machines.</li>
</ol>
<p><strong>Advantages of GAM:</strong></p>
<ol style="list-style-type: decimal">
<li>Allowing to fit non linear function for j variables (<span class="math inline">\(f_j\)</span>)</li>
<li>Has potential of making more accurate predictions</li>
<li>As the model is additive (meaning that each function is fitted holding the other variables fixed) we are still able to make inference, e.g., assessing how one variable affects the y variable.</li>
<li>Smoothness of function <span class="math inline">\(f_j\)</span> can be summarized with degrees of freedom.</li>
<li>Often applied when aiming for explanatory analysis (instead of prediction)</li>
</ol>
</div>
<div id="gam-for-classification-problems" class="section level4" number="2.1.6.2">
<h4><span class="header-section-number">2.1.6.2</span> GAM for classification problems</h4>
<p>When y is qualitative (categorical), GAM can also be applied in the logistical form.</p>
<p>As discovered in the classification section, we can apply logits (log of odds) and odds, see material from first semester.</p>
<p>The same advantages and disadvantages as in the prior section applies.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moving-beyond-linearity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture-notes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
