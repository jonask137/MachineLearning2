---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Deep Learning for Text and Sequences

The following chapters follow the structure of the book.

**Introduction**

This method is able to cover:

-   Text, understood as a sequence of words

-   Timeseries

-   Sequence data in general

To deal with this, we are going to work with recurrent neural networks and 1D convoluted networks.

## Working with Text Data

Notice that NN only works with numbers, hence we cannot use text as input. Hence we must create tensors that are text vectorized. To do this, we have several methods:

1.  Segment text into words and transforming each word into a vector.
2.  Segment text into characters and transform each character into a vector.
3.  Extract n-grams of words or characters, and transform each n-gram ino a vector. N-Grams are overlapping groups of multiple consecutive words or characters.

**N-Grams**

We see that the sentence, "The cat sat on the mat.", may be composed in 2-grams.

{"The", "The cat", "cat", "cat sat", "sat","sat on", "on", "on the", "the", "the mat", "mat"}, or 3-grams

{"The", "The cat", "cat", "cat sat", "The cat sat","sat", "sat on", "on", "cat sat on", "on the", "the","sat on the", "the mat", "mat", "on the mat"}.

*This is also cllaed bag-of-2-grams or equvilently bag-of-3-grams*

### One-hot encoding of words and characters

A visual example:

![](images/paste-DB1F1A56.png){width="440"}

*We see that the matrix will be sparse as we have a lot of 0's in there. Also it is hard coded, henced fixted dictionaries.*

This is the most common and basic way of tokenizing.

```{r}
# Listing 6.1. Word-level one-hot encoding (toy example)
samples <- c("The cat sat on the mat.", "The dog ate my homework.")
token_index <- list() #Creating an index

for (sample in samples){
  for (word in strsplit(sample, " ")[[1]]){ #Tokenizing
    if (!word %in% names(token_index)) #If not in the index
      token_index[[word]] <- length(token_index) + 2 #Ins
  }
}
max_length <- 10

#Saving the results in a 3D tensor
results <- array(0, dim = c(length(samples),
                            max_length,
                            max(as.integer(token_index))))

for (i in 1:length(samples)) {
  sample <- samples[[i]]
  words <- head(strsplit(sample, " ")[[1]], n = max_length)
  for (j in 1:length(words)) {
    index <- token_index[[words[[j]]]]
    results[[i, j, index]] <- 1
  }
}
```

Now we can also see an example on character level.

```{r}
# Listing 6.2. Character-level one-hot encoding (toy example)
samples <- c("The cat sat on the mat.", "The dog ate my homework.")
ascii_tokens <- c("", sapply(as.raw(c(32:126)), rawToChar))
token_index <- c(1:(length(ascii_tokens))) #Loading ascii tokens, predefined list
names(token_index) <- ascii_tokens
max_length <- 50
results <- array(0, dim = c(length(samples), max_length, length(token_index)))
for (i in 1:length(samples)) {
  sample <- samples[[i]]
  characters <- strsplit(sample, "")[[1]]
  for (j in 1:length(characters)) {
    character <- characters[[j]]
    results[i, j, token_index[[character]]] <- 1 #Inserting to dim i, j and
  }
}
```

Naturally we see that the dimensions of the tensor is greatly increased.

```{r}
#Listing 6.3. Using Keras for word-level one-hot encoding
library(keras)
samples <- c("The cat sat on the mat.", "The dog ate my homework.")
tokenizer <- text_tokenizer(num_words = 1000) %>%
  fit_text_tokenizer(samples)

sequences <- texts_to_sequences(tokenizer, samples)
one_hot_results <- texts_to_matrix(tokenizer, samples, mode = "binary")
word_index <- tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
```

We can also apply something called hashing trick, notice that the hashFunction cannot be isntalled. This is specifically useful if the vocabulary is very large.

In the following example we will be hashing words in 1000 characters long vectors. Notice that the more words you have, the longer must the vectors be.

```{r,eval=FALSE}
#Listing 6.4. Word-level one-hot encoding with hashing trick (toy example)
# library(hashFunction)
# samples <- c("The cat sat on the mat.", "The dog ate my homework.")
# dimensionality <- 1000                                                  1
# max_length <- 10
# results <- array(0, dim = c(length(samples), max_length, dimensionality))
# for (i in 1:length(samples)) {
#   sample <- samples[[i]]
#   words <- head(strsplit(sample, " ")[[1]], n = max_length)
#   for (j in 1:length(words)) {
#     index <- abs(spooky.32(words[[i]])) %% dimensionality               2
#     results[[i, j, index]] <- 1
#   }
# }
```

### Using word embeddings

One sees that one hot encoding leads to very big and sparse matrices, we can overcome this with word embeddings, which is the essence of the following.

To do this, there are two approaches:

1.  You start with some random word vectors and then learn the word vectors following the same principle as learning neural network weights.
2.  Load precomputed word embeddings, just like loading a pretrained conv. So called pretrained word embeddings.

The goal of the word embeddings is that one will map out similarity/connectedness of words. Meaning that we want to reflect the actual language. One must be aware that the advanced language nuances for what it is concerning, meaning that movie reviews and scientific papers may have different word embeddings.

It is hypothesized, that there is a true word embeddings map, although that is yet to me discovered. For example one can arrance wolf, tiger, dog and cat on two vectors, 1) wild to pet animal, and 2) canine to feline.

```{r,echo=FALSE,fig.cap="Figure 6.3. A toy example of a wordembedding space"}
knitr::include_graphics(rep("images/paste-795B4BE1.png"))
```

Naturally, this could be on many different scales. Thus, we see that we have vectorized the words and geometrically we can see if they are pointing in the same direction or if they are going away from each other, hence we can calculate the distance between the words, to measure how similar words are.

Compared to one-hot encoding, we see the following:

```{r,echo=FALSE,fig.cap="One-hot encoding vs. word embedding"}
knitr::include_graphics(rep("images/paste-AA3D9DB3.png"))
```

*We see that the left = the one-hot encoded and the right = words embedding.*

#### The first approach

Here we can use the power of backpropaganation.

Here is an example with IMDB reviews, with sentiment prediction.

```{r}
#Listing 6.6. Loading the IMDB data for use with an embedding layer
max_features <- 10000 #Number of words for consideration
maxlen <- 20 #Cutting the text after 20 words
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

#Creating 2D integer tensors, shape = (samples,maxlen)
x_train <- pad_sequences(x_train, maxlen = maxlen) #Adding 0, if the review is <20 words
x_test <- pad_sequences(x_test, maxlen = maxlen)
```

We see that the max length and max features are parameters where we need to control how much information to include, while having a competitive model and equally a model that is runable.

Now we can create a model with the word embedding.

```{r,results='hold'}
#Listing 6.7. Using an embedding layer and classifier on the IMDB data
dim_embeddings <- 8 #The amount of dimensions the words is to be measured on.
epochs <- 10
batch_size <- 32
val_split <- 0.2

model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features
                  ,output_dim = dim_embeddings
                  ,input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Note to the mode, We see that the embedding layer can be compared with the feature maps from the convnn, where we have a 3D tensor, which is going to be flattened.

```{r,results='hold'}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc") #We have balanced data, so acc. should do
)

summary(model)
history <- model %>% fit(
  x_train, y_train,
  epochs = epochs,
  batch_size = batch_size,
  validation_split = val_split
)
```

We see that there is an indication of overfitting. Although the validation accuracy tend towards 75%. And notice that we are only using 20 words from each review.

```{r}
plot(history)
```

What we are not doing and what we could do:

-   Can't tell the difference between *this movie is shit* and *this movie is the shit*
-   We could use RNN, to capture word relationships
-   We could take more than 20 words, or perhaps not the first 20 words, but rather 20 words in the middle or in the end, as for instance reviewee may tend to start with some kind of summary of the movie.

```{r}
rm(list = ls())
```

#### The second approach - using pretrained word embeddings

Like with other pretrained models, if you don't have much data, then you are better of just loading in a model that is trained on sufficient data and then use their weighs etc. The same applies here, where we can load in the word embeddings.

*Recall that features are constantly learned by the model, hence not enough data, means that the features that you end up with, will depend on the data, and if that is sparse, so will the features be*

Examples of the word embeddings is:

-   Word2vec

-   GloVe - Global Vectors for Word Representation

We see that a word embedding vectorize the words on n dimensions, the following example is King, Man and Woman on 50 dimension, where we see that particularly two dimensions light up respectively blue and red.

```{r,echo=FALSE,fig.cap="Word embeddings example"}
knitr::include_graphics(rep("images/paste-3D12324A.png"))
```

and another example with more words

```{r,echo=FALSE,fig.cap="Word embeddings example 2"}
knitr::include_graphics(rep("images/paste-46C140C0.png"))
```

It is worth mentioning that algorithm has found scores based on no labels, one merely specify how many dimensions they want.

The GloVe intuition can be represented in the following, where we see that `king - man + woman = queen` , where we see that you should be able to go from one word to other words.

```{r,echo=FALSE,fig.cap="GloVe intuition"}
knitr::include_graphics(rep("images/paste-5D6C3CBF.png"))
```

### Putting it all together: from raw text to word embeddings

```{r}
#Listing 6.8. Processing the labels of the raw IMDB data
imdb_dir <- "Data/3. Deep Learning/aclImdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

Now we can **tokenize** the data.

```{r}
#Listing 6.9. Tokenizing the text of the raw IMDB data
library(keras)
maxlen <- 100 #Cuttin reviews at 100 words
training_samples <- 200 #Amount of train samples
validation_samples <- 10000 #Amount of validation samples
max_words <- 10000 #We only want to consider the top 10.000 words.

tokenizer <- text_tokenizer(num_words = max_words) %>%
  fit_text_tokenizer(texts) #F

sequences <- texts_to_sequences(tokenizer, texts)
word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

#Splitting train and validation data
indices <- sample(1:nrow(data)) #We are shuffling the data
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1):
                              (training_samples + validation_samples)]
x_train <- data[training_indices,]
y_train <- labels[training_indices]
x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

#### Preprocessing the embeddings

First we need to create an index that maps the words.

```{r}
#Listing 6.10. Parsing the GloVe word-embeddings file
glove_dir = "Data/3. Deep Learning/glove"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))
embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}
cat("Found", length(embeddings_index), "word vectors.\n")
```

Then we want to build

embedding vector = EV

```{r}
#Listing 6.11. Preparing the GloVe word-embeddings matrix
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) { #Word_index is tokenized texts.
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      embedding_matrix[index+1,] <- embedding_vector #NOTICE, words that are not in the EV will be 0
  }
}
dim(embedding_matrix)
```

We see that the matrix consists of 100 columns, hence corresponding to the max length of the reviews. And the rows correspond with the amount of words we are assessing. We see that we are only interested in the top 10.000 words.

#### Defining a model

We create a network where we start with the word embedding, then we flatten, as an input for the densely connected layers, and in the end, we have the one unit layer, that assess the sentiment of the review.

```{r}
#Listing 6.12. Model definition
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words #10.000
                  ,output_dim = embedding_dim, #100
                  input_length = maxlen) %>% #100
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
summary(model)
```

We see that the embedded layer is corresponding with the amount of entries in the embedding matrix (hence 10.000 \* 100).

*Notice, that we don't want to train the embedded words, hence we freeze the layers*

#### Loading GloVe embeddings in the model

Now we are going to load the layer.

```{r}
#Listing 6.13. Loading pretrained word embeddings into the embedding layer
get_layer(object = model
          ,index = 1) %>% #What layer are we calling
  set_weights(list(embedding_matrix)) %>% #Setting the weights 
  freeze_weights() #We want to freeze these weights
```

#### Training and evaluating the model

```{r}
#Listing 6.14. Training and evaluation
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
save_model_weights_hdf5(model, "Saved Objects/pre_trained_glove_model.h5")
```

Now we can plot the results.

```{r}
plot(history)
```

We see that the model quickly starts overfitting and thus the validation data has really poor performance.

#### Training and evaluating without GloVe

```{r}
#Listing 6.16. Training the same model without pretrained word embeddings
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = embedding_dim,
                  input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
```

```{r}
plot(history)
```

Now we see that the overfitting looks even worse also the accuracy on the validation data is worse.

That is also kinda expected, as we have very little train data, so we may by chance be fitting to a very little foundation.

#### Using test data

First we must tokenize the test data. This is what we also did with the train and validation data.

```{r}
#Listing 6.17. Tokenizing the data of the test set
test_dir <- file.path(imdb_dir, "test")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(test_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)
```

Testing the model

```{r}
#Listing 6.18. Evaluating the model on the test set
model %>%
  load_model_weights_hdf5("Saved Objects/pre_trained_glove_model.h5") %>%
  evaluate(x_test, y_test)
```

We get an accuracy of 55%. Notice that is with only 200 reviews

## Understanding Recurrent Neural Networks (RNN)

So far we have seen densely connected feed forward networks along with convoluted networks. These lack ability of storing memory. Meaning that each input that the layers are given, they will regard them individually. That is a problem when we have data that is naturally followed in sequences, e.g., text or time series data.

To deal with this, we introduce recurrent neural networks. These enable the model the *'memorize'* what it has previously seen.

------------------------------------------------------------------------

*For example with IMDB reviews, each review is an input and all the tokens from each review are conveyed in their right order. Hence tokens are sequentially analyzed, while taking in account past words. When the review is over, the loop will reset.* \_\_\_

**A toy example**

```{r,results='hold'}
#Listing 6.21. R implementation of a simple RNN
timesteps <- 100 #No. of timestamps
input_features <- 32 #Dimensionality of input feature space
output_features <- 64 #Dimensionality of input feature space

random_array <- function(dim) {
  array(runif(prod(dim)), dim = dim)
}

inputs <- random_array(dim = c(timesteps, input_features)) #Random input noice
state_t <- rep_len(0, length = c(output_features)) #Set all states to 0
W <- random_array(dim = c(output_features, input_features)) #Random weight vector
U <- random_array(dim = c(output_features, output_features)) #Random weight vector
b <- random_array(dim = c(output_features, 1)) #Random weight vector

#One can show the dimensions of the wheights
dim(W) #[1] 64 32
dim(U) #[1] 64 64
dim(b) #[1] 64  1

output_sequence <- array(0, dim = c(timesteps, output_features))

for (i in 1:nrow(inputs)) {
  input_t <- inputs[i,] #The first column in the input data
  output_t <- tanh(as.numeric((W %*% input_t) + (U %*% state_t) + b)) #Combination of current state and the input, thus we memorize
  output_sequence[i,] <- as.numeric(output_t) #Updating the results matrix
  state_t <- output_t #Updating the state of the network
}
```

We see that we first set the state is set to 0, while we are iterating through each timestamp. This is basically what the RNN does. Thus, one can say that RNN is just a for loop, which reuse computations from previous steps.

We see that the output_sequence (being the result matrix), is a 2D tensor.

*Notice that the example above only simulates what would be one input.*

### A recurrent layer in Keras

The most simple layer is `layer_simple_rnn(units = 32)`. This function actually take batch sizes into account, hence we add another dimension, so we are now working with a 3D tensor input.

```{r}
library(keras)
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000
                  ,output_dim = 32) %>%
  layer_simple_rnn(units = 32)
summary(model)
```

We can also stack more layers on-top of each other.

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32) #We only want the last output
summary(model)
```

We can then use this model on the IMDB movie-review-classification problem. Before we can train the model, we must prepare the data.

```{r}
#Listing 6.22. Preparing the IMDB data
library(keras)
max_features <- 10000 #No. of words to consider as features
maxlen <- 500 #At what point are we cutting reviews
batch_size <- 32
cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb
cat(length(input_train), "train sequences\n")
cat(length(input_test), "test sequences")
cat("Pad sequences (samples x time)\n")
input_train <- pad_sequences(input_train #Pads sequences to the same length
                             ,maxlen = maxlen
                             ,value = 0) #Default, fills with 0's. 
input_test <- pad_sequences(input_test, maxlen = maxlen) #Pads sequences to the same length
cat("input_train shape:", dim(input_train), "\n")
cat("input_test shape:", dim(input_test), "\n")
```

Now we can train the model.

```{r}
#Listing 6.23. Training the model with embedding and simple RNN layers
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

```{r}
#Listing 6.24. Plotting results
plot(history)
```

In chapter three, we also dealt with this data and got an accuracy of 88%. There we used all data.

In this example we are only evaluating the first 500 words, and get an accuracy of a bit above 75% (in the book). In my example we are able to get validation accuracy of 85%, hence close to the baseline from chapter 3. So it is not better than what we have seen earlier.

Also it looks like we are overfitting in this approach.

Also `layer_simple_rnn` is not the best approach for dealing with long sequences, hence the result is kinda expected.

### Understanding the LSTM and GRU layers

Now we are going to introduce two new concepts. The reason is that the `layer_simple_rnn` is too simple to deal with actual sequences, as knowledge from previous steps vanish with the loops that it is iterating through, hence earlier data will overwritten/ruled out too quick. This is called ***the vanishing gradient problem***.

**LSTM = Long Short-Term Memory**, is an approach that saves information for later, hence preventing that older signals gets ruled out. In the book [@chollet2018, pg. 186 - 188] they describe it more into details, but the key take-away is that ***LSTM allows past information to be reinjected at a liter time, to fight the vanishing gradient problem***.

### A concrete LSTM example in Keras

```{r}
#Listing 6.27. Using the LSTM layer in Keras
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_lstm(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

```{r}
plot(history)
```

We see that the model is able to achieve up to 88% accuracy. Notice, that this is using a default model with no fine-tuning and also we are cutting the reviews at 500 words. Thus, it proves that when we attempt to get rid of the vanishing gradient problem (the previous information being forgotten), we are able to improve the model.

**What is LSTM good at?**

1.  Sentiment-analysis problems

## Advanced use of Recurrent neural networks

Now we will dig a bit deeper and the literature presents three advanced techniques for improving the performance and generalization power of recurrent neural networks. That being:

1.  Recurrent dropout, dropout like we have seen earlier to fight overfitting.
2.  Stacking recurrent layers, to increase the representational power of the network.
3.  Bidirectional recurrent layers, here we are able to present the same information to a recurrent network in different ways. This attempts to overcome gradient vanishing problems and hence improve the accuracy.

```{r}
rm(list = ls())
```

### A temperature-forecasting problem

Now we are going to look at numeric data that comes in a natural time series. Hence we will look at weather data and predict temperature.

Notice that we have one observation for each 10 minutes.

First we must create a file directory and then download the data.

```{r}
dir.download <- file.path("Data/3. Deep Learning/jena_climate")
# dir.create(dir.download, recursive = TRUE)
# download.file(url = "https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"
#               ,destfile = file.path(dir.download,"jena_climate_2009_2016.csv.zip")
#               )
# 
# fname <- file.path(dir.download, "jena_climate_2009_2016.csv.zip")
# 
# unzip(zipfile = fname
#       ,exdir = dir.download
#       )
```

Now we can take a look at the data

```{r}
library(tibble)
library(readr)
data_dir <- dir.download
fname <- file.path(data_dir, "jena_climate_2009_2016.csv")
data <- read_csv(fname)
glimpse(data)
```

Now we can also do some exploration of the data.

```{r}
#Listing 6.29. Plotting the temperature timeseries
attach(data)
plot.new()
rect(par("usr")[1],par("usr")[3],par("usr")[2],par("usr")[4],col = "lightgray",border = "white")
par(new = TRUE)
plot(x = 1:nrow(data)
     ,y = `T (degC)`
     ,type = "l"
     ,main = "Celcius Degrees"
     ,axes = FALSE
     ,panel.first = grid(col = "white",lty = 1))
axis(side = 1,col = NA,tick = TRUE,col.ticks = "black")
axis(side = 2,col = NA,tick = TRUE,col.ticks = "black")
```

*I changed it to base graphics.*

We can also look at the temperature for the first 10 days.

```{r}
#Listing 6.30. Plotting the first 10 days of the temperature timeseries
plot.new()
rect(par("usr")[1],par("usr")[3],par("usr")[2],par("usr")[4],col = "lightgray",border = "white")
par(new = TRUE)
plot(x = c(1:length(`T (degC)`[1:1440]))
     ,y = `T (degC)`[1:1440]
     ,type = "l"
     ,main = "Celcius Degrees"
     ,axes = FALSE
     ,panel.first = grid(col = "white",lty = 1)
     ,xlab = "1:1440"
     ,ylab = "T (degC)")
axis(side = 1,col = NA,tick = TRUE,col.ticks = "black")
axis(side = 2,col = NA,tick = TRUE,col.ticks = "black")
```

*I changed it to base graphics.*

It looks like this is from some winter month, as we are having degrees below 0.

As weather often is kinda the same year to year, are we also able to forecast in the coming period.

### Preparing the data

First we are going to experiment with making predictions based on previous data. Hence we set up some prerequisites:

1.  We want to evaluate the last 10 days
2.  The amount of observations that will be sampled at on data point per hour.
3.  We want to predict this amount of periods into the future

```{r}
lookback <- 1440
step <- 6
delay <- 144
```

We also need to do some further preprocessing, namely:

1.  Normalize the data, so it is on the same scale
2.  Make a data generator

First we will make a matrix with the numeric values and the normalize

```{r}
#Listing 6.31. Converting the data into a floating-point matrix
data <- data.matrix(data[,-1])

#Listing 6.32. Normalizing the data
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean) #calc. the mean of each variable
std <- apply(train_data, 2, sd) #calc. st.dev. of each variable
data <- scale(data, center = mean, scale = std) #scales according to the mean and st.dev
```

Now we will make the data generator:

In its essence we want to shuffle the train data, while we want to preserve the chronoligical order for the validation and test data. The following code is able to handle different scenarios in respect of the predefined information. I have put some information in the code chunk.

***

_Notice that the if's does not have curly brackets, that is because they are only followed by a single expression. It can be tested with just running the if statement and the hereafter the console is ready to run another line. The following is an example just to show this._

_An example with the If statements_

```{r}
l <- 1
o <- 3

print("First if, which = TRUE")
if(l < o)
  print("l is smaller tan o")

print("Second if, which = FALSE")
if(l > o)
  print("l is smaller tan o")

rm(l)
rm(o)
```

_We see that the statement is only printed in the first scenario, as that is a TRUE statement_

***

```{r}
#Listing 6.33. Generator yielding timeseries samples and their targets
batch_size <- 128 #Originally 128
generator <- 
  
  #Starting the function and declaring the input
  function(data,lookback, delay, min_index, max_index,
                      shuffle = FALSE,#Notice that we set default to be FALSE
                      batch_size = batch_size, step = step) {
  
  ##For test##
  if (is.null(max_index)) #If max_index = NULL
    max_index <- nrow(data) - delay - 1 #Notice in the test partition we set max index to NULL
  
  i <- min_index + lookback
  
  # If we are not generating the test data, then the following will be done
  
  function() {
    
    ##For training##
    if (shuffle) { #If shuffle = TRUE
      
      rows <- sample(c((min_index+lookback):max_index) #Random selection
                     ,size = batch_size) #We want as many selections as our batch size
    
    ##For validation and test##
    } else { #If shuffle is not defined, it is by default FALSE
      if (i + batch_size >= max_index) #Evaluates if we are working within the scope
        i <<- min_index + lookback
      
      #We don't want to go over the limit we set on the data partition
      rows <- c(i:min((i+batch_size)-1, max_index))
    
      #Prepare i for the next batch
      i <<- i + length(rows)
    }
    
    #Create empty arrays for data ingestion
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    #Ingest data to the arrays
    for (j in 1:length(rows)) { #Looping up to the batch size
      
      #Subsetting the data we need
      indices <- seq(from = rows[[j]] - lookback
                     ,to = rows[[j]]
                     ,length.out = dim(samples)[[2]]) #The columns
      
      #Inserting the data into the samples
      samples[j,,] <- data[indices,] #Subsetting on rows
      targets[[j]] <- data[rows[[j]] + delay,2] #Notice that DV is column 2
    }
    
    #We compute a list of the object
    list(samples, targets)
  }
}
```

Now we want to use the function to generate data. First we can represent the dimensions of the data.

```{r}
dim(data)
```

We see that we have 420451 observations on 13 variables + the dependent variable. In the following we will in each data generation step select what observations we want. Thus, we use the data generator that was specified above to get the data.

```{r}
#Listing 6.34. Preparing the training, validation, and test generators
library(keras)

train_gen <- generator(
  data, #The normalized data
  lookback = lookback, #How far we look back
  delay = delay, #How far in the future we want to look
  min_index = 1, #First train observation
  max_index = 200000, #Last train observation
  shuffle = TRUE, #Do we want to shuffle the data or take it chronoligical?
  step = step, #The period (in timesteps)
  batch_size = batch_size #The amount of samples pr. bach
)

val_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001, #We see that we extend from the train set
  max_index = 300000,
  step = step,
  batch_size = batch_size
) #Notice that we don't shuffle

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001, #We extend from the validation data
  max_index = NULL,
  step = step,
  batch_size = batch_size
) #Notice that we don't shuffle

#Identifying amount of timesteps we can validate on
val_steps <- (300000 - 200001 - lookback) / batch_size
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

Notice that we subtract the lookback, as that is foundation for first predictions, and thus they


### A common-sense, non-machine-learning baseline

We want to get a simple baseline we can compare the model with. That is done in the following.

We want to predict that the temperature 24 hours from now is the same as now, hence the temperature is just assumed to be the same in each 10 minutes. Recall that we have observations for each 10 minutes and thus 128 observations make up for a 24 hour timespan.

```{r}
#Listing 6.35. Computing the common-sense baseline MAE
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_gen() #Val_gen both contains samples and targets
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}
evaluate_naive_method()
naive_mse <- evaluate_naive_method()
```

This yields an MSE of 0.28. Recall that we have normalized the data, hence this does not correspond to temperature, hence we can reform it to actual celcius degrees.

```{r}
#Listing 6.36. Converting the MAE back to a Celsius error
celsius_mae <- naive_mse * std[[2]] #Std. of the target variable
celsius_mae
```

We see that the mean absolute value = 2.46. That is now our reference.


### A basic machine-learning approach

Now we are going to make a simple model. In general it is good practice to start with a simple model and then we can gradually make it more complicated.

```{r}
#Listing 6.37. Training and evaluating a densely connected model
model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% #=c(240,14)
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
k_clear_session()
```

```{r}
plot(history)
```

Recall that the baseline is 2,77 and our model is at best 0.3 on the validation data, meaning that we don't get anything out of having a complex model (even just a simple densely connected network).

The explanation from the book is that the intuition is that complex models search for complex patterns, and this will not find the simple solution, hence it cant see the forest for the mere trees. And thus this variate of the model cannot compete with just the simple algorithm that was performed earlier, because often simple solutions require simple models.

Hence it proves the rule of thumb that one must start simple and then hereafter move on to more complex models.



### A first recurrent baseline

Now we will try to extend the example above to see if we can make a model that is more competitive with the baseline, and hopefully outperform it.

We are now going to introduce the GRU. It stands for ___Gated recurrent unit___. The overall principle is the same, while GRU is not as cumbersome to compute, although we may not get the same data representations as we do with LSTM.

```{r}
#Listing 6.39. Training and evaluating a model with layer_gru
model <- keras_model_sequential() %>%
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```

We see that already after a couple of epochs, we get a model that is more competitive with the baseline and even slightly outperform. Although it does look like we have an issue with overfitting to the train data.

So what is natural to do?

__We add dropout!__

### using recurrent dropout to fight overfitting

In this instance with RNN, we have two types of dropouts to add.

1. dropout, as we normally now, where activations are randomly set to 0
2. recurrent_dropout, same principle, just with the recurrent units. Hence we intentionally make it forget previous seen data.

```{r}
#Listing 6.40. Training and evaluating a dropout-regularized GRU-based model
model <- keras_model_sequential() %>%
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_dense(units = 1)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

What do we expect to see? we expect to see that the model is converging much more slowly, as we are making it more difficult for it to learn.

```{r}
plot(history)
```

We see that the model has improved and now we are not overfitting any more!


### Stacking recurrent layers

So since we are no more overfitting, we are also at a place where the model has learned what it can, given the prerequisities we gave it.

To enable it to learn more, we introduce more recurrent layers.

___

Fun fact, google is driving google translate on a model that has 7 recurrent layers stacked on-top of each other. That is pretty huge.

___

Notice, as when we stacked the simple_rnn's, we wanted it to output the 3D tensor, the same analogy applies here.

```{r}
#Listing 6.41. Training and evaluating a dropout-regularized, stacked GRU model 
model <- keras_model_sequential() %>%
  layer_gru(units = 32,
            dropout = 0.1,
            recurrent_dropout = 0.5,
            return_sequences = TRUE, #The 3D tensor the next layer is using
            input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_gru(units = 64, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```

We see that the model does in fact not get better than what we have previously seen. Although they suggest in the book, that one may add more layers, as overfitting is not yet tooooo big of a problem.


### Using bidirectional RNNs

In this example we want to see if we can improve the model by assessing newest information first, hence going in antichronoligical order. To do this, we merely have to iterate the datagenerator, that we have previously made.

After this we will run the same GRU-model to see how it performs.

__Creating the antichronoligical order data generator__

```{r}
#Based on listings 6.33
batch_size <- 128 #Originally 128
generator <- 
  
  #Starting the function and declaring the input
  function(data,lookback, delay, min_index, max_index,
                      shuffle = FALSE,#Notice that we set default to be FALSE
                      batch_size = batch_size, step = step) {
  
  ##For test##
  if (is.null(max_index)) #If max_index = NULL
    max_index <- nrow(data) - delay - 1 #Notice in the test partition we set max index to NULL
  
  i <- min_index + lookback
  
  # If we are not generating the test data, then the following will be done
  
  function() {
    
    ##For training##
    if (shuffle) { #If shuffle = TRUE
      
      rows <- sample(c((min_index+lookback):max_index) #Random selection
                     ,size = batch_size) #We want as many selections as our batch size
    
    ##For validation and test##
    } else { #If shuffle is not defined, it is by default FALSE
      if (i + batch_size >= max_index) #Evaluates if we are working within the scope
        i <<- min_index + lookback
      
      #We don't want to go over the limit we set on the data partition
      rows <- c(i:min((i+batch_size)-1, max_index))
    
      #Prepare i for the next batch
      i <<- i + length(rows)
    }
    
    #Create empty arrays for data ingestion
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    #Ingest data to the arrays
    for (j in 1:length(rows)) { #Looping up to the batch size
      
      #Subsetting the data we need
      indices <- seq(from = rows[[j]] - lookback
                     ,to = rows[[j]]
                     ,length.out = dim(samples)[[2]]) #The columns
      
      #Inserting the data into the samples
      samples[j,,] <- data[indices,] #Subsetting on rows
      targets[[j]] <- data[rows[[j]] + delay,2] #Notice that DV is column 2
    }
    
    #We compute a list of the object
    list(samples [,ncol(samples):1,], targets) #####NOTICE: THIS IS WHAT IS CHANGED####
  }
}
```

__Generating the data__

```{r}
library(keras)

train_gen <- generator(
  data, #The normalized data
  lookback = lookback, #How far we look back
  delay = delay, #How far in the future we want to look
  min_index = 1, #First train observation
  max_index = 200000, #Last train observation
  shuffle = TRUE, #Do we want to shuffle the data or take it chronoligical?
  step = step, #The period (in timesteps)
  batch_size = batch_size #The amount of samples pr. bach
)

val_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001, #We see that we extend from the train set
  max_index = 300000,
  step = step,
  batch_size = batch_size
) #Notice that we don't shuffle

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001, #We extend from the validation data
  max_index = NULL,
  step = step,
  batch_size = batch_size
) #Notice that we don't shuffle

#Identifying amount of timesteps we can validate on
val_steps <- (300000 - 200001 - lookback) / batch_size
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```



```{r}
model <- keras_model_sequential() %>%
  layer_gru(units = 32,
            dropout = 0.1,
            recurrent_dropout = 0.5,
            return_sequences = TRUE, #The 3D tensor the next layer is using
            input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_gru(units = 64, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 1,
  validation_data = val_gen,
  validation_steps = val_steps
)
```



### Going even further


## Sequence processing with convnets
