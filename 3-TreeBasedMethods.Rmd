---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Tree Based Methods

In its essence the tree based methods are decisions trees, which is a set of splitting rules, which can be drawn as a tree, hence the name - decision tree.

In its initial form, the decision tree is typically not competitive with non linear models and shrinkage methods prediction wise. Although the decision trees can be improve through:

1. Bagging
2. Random forests
3. Boosting

The methods above yields several decision trees, which yields a single consensus prediction. We see that combining a large number of trees improves prediction power greatly but also decrease the interpretability.

## The Basics of Decision Trees

The decision trees can be applied both for regression and classification. The following sections will firstly be about regression and then classification. A decision tree looks like the following:

```{r SubtreeHitters,echo=FALSE,fig.cap="Decision Tree"}
knitr::include_graphics(rep("Images/DecisionTree.png"))
```

We see that the illustration has __2 internal nodes__ - years < 4.5 and hits < 117.5 -- and __3 terminal nodes__ (i.e. leaf/leaves). The leaves consist of the mean of the response given the criterias in the decision tree. The lines connecting the nodes (internal and terminal) are called branches.

Each leaf can also be written as:

1. $R_1=${X|Years<4.5}
1. $R_2=${X|Years>=4.5,Hits<117.5}
1. $R_2=${X|Years>=4.5,HIts>=117.5}

### Regression Trees

We can take an example:

```{r DecisionTree,echo=FALSE,fig.cap="Decision Tree Hitters data"}
knitr::include_graphics(rep("Images/DecisionTreeHitters.png"))
```

_Note: based on the hitters data._

We see that the observations has been separated into regions which meet the criteria, one can then take the mean of the response variable of the observations in the regions to define the terminal nodes. This may be a simplification although, it is easily interpreted.

It should also be mentioned that the criteria, also shown in previous section, indicate that we are working with hard boundaries, hence the regions does not overlap with each other.

#### How to make the decision trees

Precedure:

1. Divide the predictor space into regions, as in the example above. We end up with J dostinct and non overlapping regions, we call these $R_1,R_2,...,R_J$
2. For all observations in region $R_J$, we take the mean of the response variable.

Notice, that the way of optimizing RSS is with an approach starting on one varaible and then spreading out. We call this the _recursive binary splitting_, that is a top-down approach which is said to be greedy. In the very initial phase of the splitting procedure, all observations are in the same region, and then we start splitting up the regions, figure \@ref(fig:DTDims) show examples of having a more complex model and also including the mean of the response variable in the illustration.

Notice the top left illustration, this is not from a recursive binary splitting process, hence it yields more strange regions.

##### The goal of regression

We want to minimize the RSS. That can be written as:

\begin{equation}
\text{}\sum_{j=1}^J\sum_{i\ \in R_j}^{ }\left(y_i-y_{R_j}\right)^{^2}
(\#eq:DTRSS)
\end{equation}

Where:

+ $J$ = total number of regions
+ $j$ = spefic number of regions in the range of 1 to J
+ $R_j$ = each region, R for region i guess.
+ $i \in R_J$ = the special e sign, means member ship off. Hence it can be seen as a filter on the specific regions.
+ $\hat{y}_{R_j}$ = The predicted values in the specific regions, i.e. the mean response

So what it says is that we take the sum of all squeard residuals. But then how do we split the regions? In general terms in can be written as the following:

\begin{equation}
R_1\left(j,s\right)=\left\{X|X_j<s\right\}\ AND\ R_2\left(j,s\right)=\left\{X|X_j\ge s\right\}
(\#InternalNodesSplit)
\end{equation}

Where:

+ $s$ = cutpoint
+ $j$ = reflectsion the regions, hence, 
+ $X_j$ = the x variable region.
+ $(X|X_j<s)$ = the region of predictor space in which $X_j$ takes o a value less than s (the cutpoint)

Thus in general terms we wish to select j and s that minimize the RSS, therefore we can also write the equation for RSS in general terms with:

\begin{equation}
\sum_{i:\ e_i\in R_1\left(j,s\right)}^{ }\left(y_i-y_{R_1}\right)^{^2}+\sum_{i:\ e_i\in R_2\left(j,s\right)}^{ }\left(y_i-y_{R_2}\right)^{^2}
(\#eq:InternalNodesSplit)
\end{equation}

Where:

+ $R_1(j,s)$ define the training observations, which are a basis for $\hat{y}_{R_1}$


___Adding third dimension - response variable:___

Basically software can quite quickly compute cutpoints of the x variables to optimize RSS. The example above in \@ref(fig:DecisionTree) express two dimensions, one could also have added the response variable as a dimension. Where the fitted y (mean of response in region $R_j$) will express the hight in the specifc region.

___Adding more cutpoints:___

Naturally we could also have hadded more cutpoints, to separate the regions even further. See an example in the following:

```{r DTDims,echo=FALSE,fig.cap="Decision Tree Hitters data"}
knitr::include_graphics(rep("Images/DTDims.png"))
```

_Note: the top left show regions that is not from the procedure listed above, namely the recusive binary splitting_

##### Tree Pruning & Algoritm

The more regions that you add, the more complexity and hence flexibilty. Thus, sometimes it is a good idea to have rather simple trees, to avoid fitting too much to specific observations. Also I recon, that in regions with few observations we are prone to overfitting in the specific regions.

___Considerations on making a stable tree___

One can perhaps add nodes until you don't lower RSS, like forward selection, although sometimes a significant improvement of the model may come after a certain cutpoint, hence that is not a good approach.

A better approach is the opposite, where we start with making a huge tree (this we call $T_0$) and cuts it down a subtree (merely cutting branches off). To do so, one can do _cost complexity pruning_, i.e. _weakest link pruning_. To do so we introduce $\alpha$, which is a nonnegative tuning parameter. This can be compared with backward selection. See examples:

+ T0: figure \@ref(fig:T0Hitters)
+ MSE with different alpha values in figure \@ref(fig:TreePruningHitters)
+ Selected subtree in figure \@ref(fig:SubtreeHitters)

$\alpha$ behaves in the following way:

+ If $\alpha$ = 0, then the subtree = $T_0$, hence the big tree.
+ As $\alpha$ increases we prune $T_0$ into subtree.

Thus the higher the tuning parameter, the smaller the tree. This can be compared with the lasso [@hastie2013]. Thus, for each $\alpha$ there corresponds a subtree of $T_0$, i.e. $T \subset T_0$. This can be written as:

\begin{equation}
\sum_{m=1}^{\left|T\right|}\sum_{i:\ x_i\in R_m}^{ }\left(y_{i\ }-y_{R_m}\right)^{^{2\ }}+\ \alpha\left|T\right|
(\#eq:Subtree)
\end{equation}

Where:

+ $|T|$ = the number of terminal nodes of the tree $T,R_m$ (the subset of predictor space)

Therefore we can write the procedure for building a regression tree with:

1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fwer than some minumum number of observations (to avoid overfitting). See an example of T0 in figure \@ref(fig:T0Hitters)
2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\alpha$
3. Use K-fold cross validation to choose $\alpha$. That is, divide the training observation into K-folds. For each $k=1,...,K$: Where we do the following:
    a. Repat steps 1 and 2 on all but the kth fold of the training data
    b. Evaluate the mena squared prediction error in the left-out kth fold, as a function of $\alpha$. Average the results for each value of alpha and pick alpha to minimuzze the average error.
4. Return the subtree from step 2 that corresponds to the chosen value of $\alpha$.
    a. It may be advantagous to plot the pruning process to actually see how the removed branches improve the model and select a subtree of reason instead of being blinded by the absolute lowest prediction error, see an example hereof in figure \@ref(fig:TreePruningHitters)

```{r T0Hitters,echo=FALSE,fig.cap="T0 Hitters"}
knitr::include_graphics(rep("Images/T0Hitters.png"))
```

_We see the big tree, that can be pruned to optimize prediction, the following illustration plots MSE using different tuning parameters_

```{r TreePruningHitters,echo=FALSE,fig.cap="Tree Pruning Hitters"}
knitr::include_graphics(rep("Images/TreePruningHitters.png"))
```

_We see that the lowest train and CV MSE is at the tree size of 10, although between three terminal nodes and 10 terminal nodes appear to be equally good, hence we go for the most parsimoneous model, see that tree in figure \@ref(fig:SubtreeHitters)_

\

### Classification Trees

The procedure is very much the same as what we saw for regression trees, where the same pruning process etc. In the classification setting we predict qualitative outcomes, e.g., yes/no. We can either select based on frequency (i.e. most commonly occuring) or the proportions.

In classification we use error rate instead of RSS, as we can't do RSS in a classification setting. I guess we can construct a confusion matrix as well.

We introduce a new term: _node purity_. This is about including internal nodes that leads to two terminal nodes which has the same conclusion. It will not reduce the classification error, it does improve the certainty of the classification. Therefore, to account for the certianty and thus the node purity, one should also make the _Gini Index_ and the _Entropy_, equations can be found in @hastie2013, p. 312.


### Tree vs. Lienar Models

One is never strictlu better than the other. But in general, one should start with a simple model, such as linear regression, to have a baseline, which we can compare more advanced mdoels with.

If the actual varaince of the response variable can be explained by linear regression, then it is likely to outperform a tree model, but if one experience more complex data, then it is likely that the tree model will outperform the linear model.


### Advantages and Disadvantages of Trees

This is compared to classical approaches, linear regression and classification @hastie2013, CH3 and CH 4.

___Advantges:___

1. Trees are very easy to explain to people. And also easy to visualize and understand.
2. Some argues that decision trees better mimiic human reasoning and decision making.
3. Can be shown graphically
4. One don't have to construct dummy variables.

___Disadvantages:___

1. Trees is typically outperformed by more advanced methods, included in @hastie2013.
2. Trees are not robust and small changes in the data may lead to big changes in the model.

Although decision trees can be advanced with:

1. Bagging
2. Random Forests
3. Boosing

To improce prediction power and therefore compete with other advanced models.


## Bagging, Random Forests, Boosting

These are merely building blocks to improve the prediction power.

### Bagging (Bootstrap Aggregation)

We want to reduce the variance, and thus the instability of the model, by training the decision tree on different training data. To solve this without requiring a huge amount of data, we introduce bootstrapping. We then have the following procedure:

1. Sample from the train data B times
2. Train the model, making B amount of models, can be written as $\hat{f}^{*b}(x)$
    _a. Notice that we didn't prune the tree, that is intentional._
3. Make predictions
4. Average the predictions - these are the final predictions. Hence we obtain:

\begin{equation}
\hat{y}_{bag}(x) = 1/B*\sum_{b=1}^{B}\hat{f}^{*b}(x)
(\#eq:BaggingTrees)
\end{equation}

We have therefore created B amount of models, that are each very flexible. But altogether we they create a consensus that acts like the principle estimation of the crowds. Therefore, by including B amount of models we avoid overfitting.

In a classification setting, it is very common to classify based on the majority vote.

Terms:

+ OOB = out-of-bag. These are observations that are not included in the bagging process.

### Random Forests

Illustrate the scenario of a clear ranking in the importance of the predictor variables, then the bagging method will almost everytime end up with the same trees, perhaps with different cutpoints, but in general, the trees are similar. This countereffects the reduction of variability that we are trying to impose.

With random forests we force the model only to select a subset of the predictors to avoid constructing trees that are very much alike, due to the independent selection. Thus, each time the algorithm considers a split, it is only able to choose among a random sample of m predictors amongst all of the p predictors. Notice, that we are still bagging the data.

By doings so, we are able to construct more different trees, that yields a far more stable model and also preserves the same prediction power through the bagging and hence averaging the consensus of each tree.

### Boosting

_Note, boosting is a general approach, that can be applied to many statistical learning methods_

The whole idea, is that we can construct one model and based on the residuals hereof, we can construct a new tree which is fitted to the residuals on the prior model. Therefore, the boosting method allows for learning. To control the learning we have three different tuning parameters:

1. B = the number of trees. As we constantly improve the model, by fitting to the residuals, we are now able to overfit the training data by iterating too much.
2. $\lambda$ = the shrinkage parameter. This is a small positive number. It controls at which rate the boosting works. Often the smaller lambda the higher does B need to be.
3. d = number of d splits (d for depth). This controls the complexity of the boosted ensemble. _Note: often $d = 1$ works very well, this is a stump with only one split_. The higher the d, the more interactions between the variables will be reflected. Although, the reason that d = 1 works well, is that each tree accounts for previous trees.

See the procedure on @hastie2013, p. 323.

__Notice, boosting in classification is a bit different and more complex, the details hereof are omitted in the book__



