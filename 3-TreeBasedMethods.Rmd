---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Tree Based Methods

In its essence the tree based methods are decisions trees, which is a set of splitting rules, which can be drawn as a tree, hence the name - decision tree.

We have three types of trees:

1. Regression trees
2. Classification trees
3. Ensemble methods of trees, e.g., bagging, random forests and boosting

In its initial form, the decision tree is typically not competitive with non linear models and shrinkage methods prediction wise. Although the decision trees can be improve through:

1. Bagging
2. Random forests
3. Boosting

*Those above are ensemble methods of trees, those above are controlled with hyper-parameters that counterfeits overfitting*

The methods above yields several decision trees, which yields a single consensus prediction. We see that combining a large number of trees improves prediction power greatly but also decrease the interpretability.

## The Basics of Decision Trees

The decision trees can be applied both for regression and classification. The following sections will firstly be about regression and then classification. A decision tree looks like the following:

```{r SubtreeHitters,echo=FALSE,fig.cap="Decision Tree"}
knitr::include_graphics(rep("Images/DecisionTree.png"))
```

We see that the illustration has __2 internal nodes__ - years < 4.5 and hits < 117.5 -- and __3 terminal nodes__ (i.e. leaf/leaves). The leaves consist of the mean of the response given the criterias in the decision tree. The lines connecting the nodes (internal and terminal) are called branches.

_Notice, that the same variables can occurs several times in the tree._

Each leaf can also be written as:

1. $R_1=${X|Years<4.5}
1. $R_2=${X|Years>=4.5,Hits<117.5}
1. $R_2=${X|Years>=4.5,HIts>=117.5}

___Terminology:___

+ Root: best predictor
+ Splitting: This is when we do the splitting on the internal nodes / decision node
+ Leaf: The leafs contain the mean or the mode of the respons from the observations which meet the circumstances that are set in the splits.


### Regression Trees

We can take an example:

```{r DecisionTree,echo=FALSE,fig.cap="Decision Tree Hitters data"}
knitr::include_graphics(rep("Images/DecisionTreeHitters.png"))
```

_Note: based on the hitters data._

We see that the observations has been separated into regions which meet the criteria, one can then take the mean of the response variable of the observations in the regions to define the terminal nodes. This may be a simplification although, it is easily interpreted.

It should also be mentioned that the criteria, also shown in previous section, indicate that we are working with hard boundaries, hence the regions does not overlap with each other.

#### How to make the decision trees

Precedure:

1. Divide the predictor space into regions, as in the example above. We end up with J dostinct and non overlapping regions, we call these $R_1,R_2,...,R_J$
2. For all observations in region $R_J$, we take the mean of the response variable.

Notice, that the way of optimizing RSS is with an approach starting on one varaible and then spreading out. We call this the _recursive binary splitting_, that is a top-down approach which is said to be greedy. In the very initial phase of the splitting procedure, all observations are in the same region, and then we start splitting up the regions, figure \@ref(fig:DTDims) show examples of having a more complex model and also including the mean of the response variable in the illustration.

Notice the top left illustration, this is not from a recursive binary splitting process, hence it yields more strange regions.

##### The goal of regression

We want to minimize the RSS. That can be written as:

\begin{equation}
\text{}\sum_{j=1}^J\sum_{i\ \in R_j}^{ }\left(y_i-\hat{y}_{R_j}\right)^{^2}
(\#eq:DTRSS)
\end{equation}

Where:

+ $J$ = total number of regions
+ $j$ = spefic number of regions in the range of 1 to J
+ $R_j$ = each region, R for region i guess.
+ $i \in R_J$ = the special e sign, means member ship off. Hence it can be seen as a filter on the specific regions.
+ $\hat{y}_{R_j}$ = The predicted values in the specific regions, i.e. the mean response

So what it says is that we take the sum of all squeard residuals. But then how do we split the regions? In general terms in can be written as the following:

\begin{equation}
R_1\left(j,s\right)=\left\{X|X_j<s\right\}\ AND\ R_2\left(j,s\right)=\left\{X|X_j\ge s\right\}
(\#eq:InternalNodesSplit)
\end{equation}

Where:

+ $s$ = cutpoint
+ $j$ = reflectsion the regions, hence, 
+ $X_j$ = the x variable region.
+ $(X|X_j<s)$ = the region of predictor space in which $X_j$ takes o a value less than s (the cutpoint)

Thus in general terms we wish to select j and s that minimize the RSS, therefore we can also write the equation for RSS in general terms with:

\begin{equation}
\sum_{i:\ e_i\in R_1\left(j,s\right)}^{ }\left(y_i-\hat{y}_{R_1}\right)^{^2}+\sum_{i:\ e_i\in R_2\left(j,s\right)}^{ }\left(y_i-\hat{y}_{R_2}\right)^{^2}
(\#eq:InternalNodesSplit)
\end{equation}

Where:

+ $R_1(j,s)$ define the training observations, which are a basis for $\hat{y}_{R_1}$


___Adding third dimension - response variable:___

Basically software can quite quickly compute cutpoints of the x variables to optimize RSS. The example above in \@ref(fig:DecisionTree) express two dimensions, one could also have added the response variable as a dimension. Where the fitted y (mean of response in region $R_j$) will express the hight in the specifc region.

___Adding more cutpoints:___

Naturally we could also have hadded more cutpoints, to separate the regions even further. See an example in the following:

```{r DTDims,echo=FALSE,fig.cap="Decision Tree Hitters data"}
knitr::include_graphics(rep("Images/DTDims.png"))
```

_Note: the top left show regions that is not from the procedure listed above, namely the recusive binary splitting_

##### Tree Pruning & Algoritm

The more regions that you add, the more complexity and hence flexibilty. Thus, sometimes it is a good idea to have rather simple trees, to avoid fitting too much to specific observations. Also I recon, that in regions with few observations we are prone to overfitting in the specific regions.

___Considerations on making a stable tree___

One can perhaps add nodes until you don't lower RSS, like forward selection, although sometimes a significant improvement of the model may come after a certain cutpoint, hence that is not a good approach.

A better approach is the opposite, where we start with making a huge tree (this we call $T_0$) and cuts it down a subtree (merely cutting branches off). To do so, one can do _cost complexity pruning_, i.e. _weakest link pruning_. To do so we introduce $\alpha$, which is a nonnegative tuning parameter. This can be compared with backward selection. See examples:

+ T0: figure \@ref(fig:T0Hitters)
+ MSE with different alpha values in figure \@ref(fig:TreePruningHitters)
+ Selected subtree in figure \@ref(fig:SubtreeHitters)

$\alpha$ behaves in the following way:

+ If $\alpha$ = 0, then the subtree = $T_0$, hence the big tree and therefore controls the complexity of the model
+ As $\alpha$ increases we prune $T_0$ into a subtree.

Thus the higher the tuning parameter, the smaller the tree. This can be compared with the lasso [@hastie2013]. Thus, for each $\alpha$ there corresponds a subtree of $T_0$, i.e. $T \subset T_0$. This can be written as:

\begin{equation}
\sum_{m=1}^{\left|T\right|}\sum_{i:\ x_i\in R_m}^{ }\left(y_{i\ }-\hat{y}_{R_m}\right)^{^{2\ }}+\ \alpha\left|T\right|
(\#eq:Subtree)
\end{equation}

Where:

+ $|T|$ = the number of terminal nodes of the tree $T,R_m$ (the subset of predictor space)

Therefore we can write the procedure for building a regression tree with:

1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fwer than some minumum number of observations (to avoid overfitting). See an example of T0 in figure \@ref(fig:T0Hitters)
2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\alpha$
3. Use K-fold cross validation to choose $\alpha$. That is, divide the training observation into K-folds. For each $k=1,...,K$: Where we do the following:
    a. Repat steps 1 and 2 on all but the kth fold of the training data
    b. Evaluate the mena squared prediction error in the left-out kth fold, as a function of $\alpha$. Average the results for each value of alpha and pick alpha to minimuzze the average error.
4. Return the subtree from step 2 that corresponds to the chosen value of $\alpha$.
    a. It may be advantagous to plot the pruning process to actually see how the removed branches improve the model and select a subtree of reason instead of being blinded by the absolute lowest prediction error, see an example hereof in figure \@ref(fig:TreePruningHitters)

```{r T0Hitters,echo=FALSE,fig.cap="T0 Hitters"}
knitr::include_graphics(rep("Images/T0Hitters.png"))
```

_We see the big tree, that can be pruned to optimize prediction, the following illustration plots MSE using different tuning parameters_

```{r TreePruningHitters,echo=FALSE,fig.cap="Tree Pruning Hitters"}
knitr::include_graphics(rep("Images/TreePruningHitters.png"))
```

_We see that the lowest train and CV MSE is at the tree size of 10, although between three terminal nodes and 10 terminal nodes appear to be equally good, hence we go for the most parsimonious model, see that tree in figure \@ref(fig:SubtreeHitters)_

\

##### Setting contrains of the tree sise

An alternative to tree pruning is contraining the size of the tree. That can be done with four approaches:

1. Maximum depth of the tree (vertical depth) - tuned using CV
1. Minimum observations for a node split - tuned using CV
1. Minimum observations for a terminal node (lead) - lower values for inbalanced classes
1. Maximum number of terminal nodes - can be defined instead of depth.


### Classification Trees

The procedure is very much the same as what we saw for regression trees, where the same pruning process etc. In the classification setting we predict qualitative outcomes, e.g., yes/no. We can either select based on frequency (i.e. most commonly occuring i.e. mode) or the proportions.

In classification we use error rate instead of RSS, as we can't do RSS in a classification setting. I guess we can construct a confusion matrix as well.

We introduce a new term: _node purity_. This is about including internal nodes that leads to two terminal nodes which has the same conclusion. It will not reduce the classification error, it does improve the certainty of the classification. Therefore, to account for the certainty and thus the node purity, one should also make the _Gini Index_ and the _Entropy_, equations can be found in @hastie2013, p. 312.

___More on purity: as we classify based on the mode of the response variable, we intentionally misclassify some observations. Naturally, we want to avoid this. Thus we can increase purity, that is often seen by splitting the same variable consecutive times or just several times. Hence to circle in the actual classifications. Although, doing this too much leads to overfitting to the data. That is why the Gini and Entropy adds a penalty to the in-sample error and thus accounting for purity. But one should also make an out of sample validation___

Thus the splits are made to optimize the Gini, Entropy or perhaps Chi-Squared.

If we want to write it in math, we can do the following:

$$y = f(x_1,x_2)$$

Where we see that y is a function of $x_1$ and $x_2$

### Tree vs. Lienar Models

One is never strictlu better than the other. But in general, one should start with a simple model, such as linear regression, to have a baseline, which we can compare more advanced mdoels with.

If the actual varaince of the response variable can be explained by linear regression, then it is likely to outperform a tree model, but if one experience more complex data, then it is likely that the tree model will outperform the linear model.


### Advantages and Disadvantages of Trees

This is compared to classical approaches, linear regression and classification @hastie2013, CH3 and CH 4.

___Advantges:___

1. Trees are very easy to explain to people. And also easy to visualize and understand.
2. Some argues that decision trees better mimiic human reasoning and decision making.
3. Can be shown graphically
4. One don't have to construct dummy variables.
5. Simple and useful for interpretation
1. No need to transform features
1. Less cleaning required and not so sensible to outliers
1. No need to transform features
1. No assumptions, purely data driven
1. Capturing interactions between features
1. Ensemble trees compete with neural networks in terms of predictions
1. Useful in data exploration and feature selection


___Disadvantages:___

1. Trees is typically outperformed by more advanced methods, included in @hastie2013.
2. Trees are not robust and small changes in the data may lead to big changes in the model.
1. It is easy to overfit the data, hence high variance and low bias. This is dealt with by using the ensemble approach, hereunder, bagging, random forests and boosting.

Although decision trees can be advanced with:

1. Bagging
2. Random Forests
3. Boosting

To improve prediction power and therefore compete with other advanced models.




## Bagging, Random Forests, Boosting

These are merely building blocks to improve the prediction power.

### Bagging (Bootstrap Aggregation)

We want to reduce the variance, and thus the instability of the model, by training the decision tree on different training data. To solve this without requiring a huge amount of data, we introduce bootstrapping. We then have the following procedure:

1. Sample from the train data B times
2. Train the model, making B amount of models, can be written as $\hat{f}^{*b}(x)$
    _a. Notice that we didn't prune the tree, that is intentional._
3. Make predictions
4. Average the predictions - these are the final predictions. Hence we obtain:

\begin{equation}
\hat{y}_{bag}(x) = 1/B*\sum_{b=1}^{B}\hat{f}^{*b}(x)
(\#eq:BaggingTrees)
\end{equation}

We have therefore created B amount of models, that are each very flexible. But altogether we they create a consensus that acts like the principle estimation of the crowds. Therefore, by including B amount of models we avoid overfitting.

In a classification setting, it is very common to classify based on the majority vote.

Terms:

+ OOB = out-of-bag. These are observations that are not included in the bagging process.
+ OOB Error Estimation: This is an estimation of the CV error on the test data.

### Random Forests

Illustrate the scenario of a clear ranking in the importance of the predictor variables, then the bagging method will almost everytime end up with the same trees, perhaps with different cutpoints, but in general, the trees are similar. This countereffects the reduction of variability that we are trying to impose.

With random forests we force the model only to select a subset of the predictors to avoid constructing trees that are very much alike, due to the independent selection. Thus, each time the algorithm considers a split, it is only able to choose among a random sample of m predictors among all of the p predictors. Notice, that we are still bagging the data.

The __rule of thumb__ is $p/3$ that is a part of the variables that are randomly considered and $\sqrt{p}$ for classification.

By doings so, we are able to construct more different trees, that yields a far more stable model and also preserves the same prediction power through the bagging and hence averaging the consensus of each tree.

### Boosting (i.e. Gradient Boosting)

_Note, boosting is a general approach, that can be applied to many statistical learning methods_

The whole idea, is that we can construct one model and based on the residuals hereof, we can construct a new tree which is fitted to the residuals on the prior model. Therefore, the boosting method allows for learning. To control the learning we have three different tuning parameters:

1. __B = the number of trees.__ As we constantly improve the model, by fitting to the residuals, we are now able to overfit the training data by iterating too much.
2. __$\lambda$ = the shrinkage parameter.__ This is a small positive number. It controls at which rate the boosting works. Often the smaller lambda the higher does B need to be.
3. __d = number of d splits (d for depth).__ This controls the complexity of the boosted ensemble. _Note: often $d = 1$ works very well, this is a stump with only one split_. The higher the d, the more interactions between the variables will be reflected. Although, the reason that d = 1 works well, is that each tree accounts for previous trees.

See the procedure on @hastie2013, p. 323.

__Notice, boosting in classification is a bit different and more complex, the details hereof are omitted in the book__


## XGBoost

See the XGBoost file.


## Application in R

### Decision trees

1. In library `tree`
    + Use `tree(DV~IV,data = ...)`
1. In library `rpart`
    + Use `rpart(DV~IV,daata = ...)`
    + _Note, this has better visualization_
1. Check summary using `summary()`
1. Plot the tree using `plot()`
1. Use `predict()` and evaluate the test error of the unpruned tree
1. Evaluate optimal level of tree complexity using `cv.tree()`
1. Prune the tree with `prune.misclass()` / `prune.tree()`
1. Use `predict()` and evaluate the test error of the pruned tree

__Interpreting summary()__

We want to look at:

+ Variables actually used in the tree construction, to see what variables that are in there
+ Number of terminal nodes: count of the leafs
+ Residual mean deviance: that is the error
+ We also have distribution of the errors.

__Interpreting the cv.tree() object__

We see following:
+ Size, amount of leaves
+ Deviance, that is the error. Where we want to find the model with the lowes deciance
+ We can get the alpha by calling `$k` on the `cv.tree()` model.


__Interpreting the prdict() object__

Here we find the predictions.

### Bagging

1. Un library `randomFores``
    + Use `randomForest(DV~IV,mtry = p,ntree = ...,importance = TRUE)
1. Use `predict()` evalutae the test error
1. Use `ìmportance()` to see the importance of each variable
1. Use `varImpPlot()` to plot the importance of each variable

__Interpreteing importance()__

We get two coloumns:

1. %IncMSE: each of the numbers is the mean decrease in accuracy of prediction (i.e. increase error). Hence if we exclude a specific variable from the model we can read how much the error will increase.
2. IncNodePurity: This is a measure of total increase of node purity by splitting over the specific variable.

Hence both coloumns return information about how the variables impact the performance of the model.

This can also be Plotted

### Random Forests

1. Un library `randomForest``
    + Use `randomForest(DV~IV,mtry = p,ntree = ...,importance = TRUE)
    + For RF of regression tree default _mtry = p/3_
    + FOr RF of classification tree fedault _mtry = $\sqrt{p}$_
1. Use `ìmportance()` to see the importance of each variable
1. Use `varImpPlot()` to plot the importance of each variable

We see that this is basically the same as bagging, merely where mtry is different, as we define how many variables that should be randomly considered.

Interpretation of the variable importance is the same as in bagging.


### Boosting

1. In library `gbm`
    + Use `gm(DF~IV,shrinkage = ,n.trees = ,interaction.depth = ,)`
    + We have to define the distribution, in exercise 8, she used Bernoulli
1. Use `summary()` to see the relative influence of the variables
1. Use `plot()` to produce partial dependence plot for each variable
1. Use `predict()` to evaluate the test error

__Interpreting summary()__

We get two columns:

1. var: that is the variable
2. Rel. inf: that is the relative influence: We see the highest values are the variables with the highest relationship with the 

We are also abe to plot the relationship between an X variable and the y, that is done with `plot(<boost.model>,i = "the variable")`

__Interpreting predict()__

we have to define:

+ Trained boosting model
+ The test data
+ The amount of trees, that is with n.trees
+ If classification, then type = "response". Thus, one get probabilities.
  + If type is not = "response", then we get the odds. We are interested in the probability, hence we must use type = "response"



## Lab section

### Fitting Classification Trees {#LabClassification}

```{r}
library(tree)
```

We want to use the carseats data, where sales = Unit sales (in thousands) at each location. We want to predict whether they sold more or less than 8.000, i.e. 8 as the variable is encoded in thousands.

```{r}
library(ISLR)
df <- Carseats
High <- ifelse(df$Sales <= 8,"No","Yes")
```

Merging the vector and the data set.

```{r}
df <- data.frame(Carseats,High)
df$High <- as.factor(df$High)
```

We now want to predict the High variable.

```{r}
tree.carseats <- tree(formula = High ~ . - Sales
                      ,data = df)
summary(tree.carseats)
```

+ We see the variables that are included
+ We see that there are 27 terminal nodes
+ Information about the error (deviance) and the missclassification rate = 9%

The deviance reported is from the calculation:

\begin{equation}
-2\sum_m\sum_kn_{mk}log\hat{p}_{mk}
(\#eq:deviance)
\end{equation}

Where:

+ $n_{mk}$ = the number of observations in the *m*th terminal node, that belong to the *k*th class.

A small deviance = low error, hence a good fit to the train data.

_Thus similar to the RSS calculation that we have seen earlier._

we find the residual mean deviance to be:

\begin{equation}
\frac{Deviance}{n-|T_0|}
(\#eq:ResidualMeanDeviance)
\end{equation}

Hence

$$\frac{170.7}{(400-27)}=373$$

Where $T_0$ is the unpruned tree, which we see is with 27 terminal nodes. And $n$ is merely the amount of observations.

__PLotting the tree:__

```{r}
plot(tree.carseats)
text(tree.carseats
     ,pretty = 0)
```

We see that the tree. Notice on the right side, we see that price occurs twice. This is probably due to purity and more certainty on the predictions.

We see that Shelve Location Bad and Medium appear to be the best predictor. As we start out with that variable.

```{r}
tree.carseats[1] #Added [1] to get it in a table
```

We are able to see the following from the print:

1. Variable
1. Criterion
1. Observations in the criterion, i.e. branch
1. The deviance
1. The overall prediction for the branch
1. The fraction of observations in that branch that take on values 1 and 0 (Y/N).

We also see that branches that leads to a terminal node is marked with an "*".

_Notice, that the ShelveLoc Says a and c for cut left. That is because the categories are not ordered. We can inspect the order with contrasts()_

```{r}
contrasts(df$ShelveLoc)
```

_Where it is ranked, bad, good, medium. Perhaps it could have been ordered correctly. But it does not matter when we are working with a decision tree, as it is purely data driven._

__Using test and train data__

```{r}
set.seed(2)
index <- sample(1:nrow(df),size = 200)
df.train <- df[index,]
df.test <- df[-index,]

tree.carseats <- tree(High ~ . - Sales,data = df.train)
tree.pred <- predict(tree.carseats,newdata = df.test,type = "class")

library(caret)
confusionMatrix(data = tree.pred,reference = df.test$High,positive = "Yes")
```

We see a sensitivity of 60% and specificity of 88%


__Pruning the tree__

It is likely that we are able to improve the model by pruning the tree. That is performed in the following.

```{r}
set.seed(1337)
cv.carseats <- cv.tree(object = tree.carseats
                       ,FUN = prune.misclass) # we are pruning to optimize against misclassifications

#Listing objects in the cv.carseats
names(cv.carseats)

#Retrieving informaiton in the object
cv.carseats
```


__Size:__
This is the size of the subtrees created

__dev:__
The cross-validation error rate in this instance. We want to select the tree that yields the lowest error rate, that appear to be tree no. 4 with 9 terminal nodes.

__k:__
Recall that we prune trees based on the tuning parameter $\alpha$, see equation \@ref(eq:Subtree). This is corresponding to k.

```{r}
par(mfrow = c(1,2))
plot(x = cv.carseats$size,y = cv.carseats$dev,type = "b")
plot(x = cv.carseats$k,y = cv.carseats$dev,type = "b")
```

We new apply the `prune.misclass()` function in order to prune the tree to obtain the nine-node tree.

```{r}
prune.carseats <- prune.misclass(tree.carseats
                                 ,best = 9) #We want to inspect a tree of size 9
par(mfrow = c(1,1))
{plot(prune.carseats)
text(prune.carseats,pretty = 0)}
```

```{r}
tree.pred <- predict(object = prune.carseats,newdata = df.test,type = "class")
confusionMatrix(tree.pred,reference = df.test$High,positive = "Yes")
```

We see that we have both improved the sensitivity to 69.9% and the specificity to 82.9% from respectively 60% and 88%.

The overall accuracy is 77.5%.

We can show the accuracy of other trees as well. But these will perform worse.

```{r}
prune.carseats <- prune.misclass(tree.carseats,best = 15)
{plot(prune.carseats)
text(prune.carseats,pretty = 0)}
tree.pred <- predict(object = prune.carseats,newdata = df.test,type = "class")
#Confusion matrix
confusionMatrix(data = tree.pred,reference = df.test$High,positive = "Yes")
```

We see that the overall accuracy is actually the same, but the sensitivity and specificity is worse.

\

```{r,include=F}
rm(list = ls())
```

### Fitting Regression Trees

We use the Boston data set.

```{r}
library(MASS)
set.seed(1)
df <- Boston
index <- sample(x = nrow(df),size = nrow(df)/2)
df.train <- df[index,]
df.test <- df[-index,]

tree.boston <- tree(medv ~ .,data = df.train)
summary(tree.boston)
```

For the interpretation hereof, i refer to section \@ref(LabClassification). But notice, that we are now wirking with a continous response variable, hence deviance = RSS, also found equation \@ref(eq:DTRSS).

We can now plot the tree.

```{r}
{plot(tree.boston)
  text(tree.boston,pretty = 0)}
```

Recall that the Y variable is median value of owner-occupied homes in \$1000s. We see that the predicted median house price is 45,380 when the amount of rooms are over 7.5.

Let us now use cross validation to see whether pruning the tree will improve the model.

```{r}
cv.boston <- cv.tree(tree.boston)
plot(x = cv.boston$size,y = cv.boston$dev,type = "b")
```

We see that as the tree size increases. This in this case it does not appear to improve the model with pruning the tree.

However, let us say, that we know that we want to prune the tree. Then we can do the following:

```{r}
prune.boston <- prune.tree(tree = tree.boston,best = 5) #5 terminal nodes
{plot(prune.boston)
  text(prune.boston,pretty = 0)}
```

```{r}
tree.pred <- predict(object = tree.boston,newdata = df.train)
{plot(tree.pred,df.test$medv)
abline(a = 0,b = 1)}
```

```{r}
mean((tree.pred - df.test$medv)^2)
```

We see an MSE of 153.5.


```{r,include=F}
rm(list = ls())
```


### Bagging and Random Forests

Notice that bagging and random forests are basically the same. Where random forests merely tweak the bagging trees with a random selection of IV, instead of the model always being able to select all models. Therefore, it can also be said that $m = p$, and thus the same function can be applied to both bagging and random forests.

#### Bagging

```{r}
library(MASS)
df <- Boston
index <- sample(x = nrow(df),size = nrow(df)/2)
df.train <- df[index,]
df.test <- df[-index,]
```


```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ .,data = df.train,mtry = 13,importance = TRUE)
bag.boston
```

We see that we have constructed 500 trees and tried 13 (all) variables at each split.

```{r}
bag.pred <- predict(object = bag.boston,newdata = df.test)
plot(x = bag.pred,y = df.test$medv)
abline(0,1)
mean((bag.pred-df.test$medv)^2) #MSE
```

We see that the MSE is 19.8 which is an immense improvement.

Notice, that we also have specified how many trees we want to grow:

```{r}
set.seed(1)
bag.boston <- randomForest(medv ~ .,data = df.train,mtry = 13,ntrees = 25,importance = TRUE)
bag.pred <- predict(object = bag.boston,newdata = df.test)
mean((bag.pred-df.test$medv)^2) #MSE
```

We see that the MSE is a bit higher than before. That is also intuitively correct, although with bagging we are actually also at risk of overfitting the data by growing too many trees, as it is likely that the trees are similar. To overcome this risk, we can grow a random forest instead, that is done in the following.

#### Random Forest

We use the same procedure except that we use fewer variables under evaluation. By default R follows a rule of thumb with $\frac{p}{3}$ for regression and $\sqrt{p}$ for classification. In this example, we use 6 variables.

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ .,data = df.train,mtry = 6,importance = TRUE)
rf.pred <- predict(object = rf.boston,newdata = df.test)
mean((rf.pred-df.test$medv)^2) #MSE
```

Now we see that the model is even better, with MSE of 16.4.

We can use `importance()` to interprete the variables.

```{r}
importance(rf.boston)
```

We see that rm (rooms pr. dwelling) has the highest influence on the model performance, and we see by removing this variable, the error increase by 32.3.

We also see that rm leads to the highest purity impact.

_Notice, that these metrics are based on the train data._

The data above can also be plotted with.

```{r}
varImpPlot(rf.boston)
```

Where we see that the varaibles are ranked according to the metrics.


### Boosting

Recall that this model initially fits a tree to the data and then afterwards fits consecutive models to the residuals of that model. This is why it is also called gradient boosting.

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ .
                    ,data = df.train
                    ,distribution = "gaussian" #Since it is regression
                    ,n.trees = 5000
                    ,interaction.depth = 4
                    ) #Lambda is by default = 0.001
summary(boost.boston)
```

We see that the model ranks relative influence, which is also plotted. Again we see that rm is the most important variable. lstat is also noticable.

We can plot _partial dependence plots_ for these variables.

```{r}
plot(boost.boston,i.var = "rm")
plot(boost.boston,i.var = "lstat")
```

We can now predict medv for the test data.

```{r}
boost.pred <- predict(object = boost.boston
                      ,newdata = df.test
                      ,n.trees = 5000 #Notice that we specify amount of trees
                      )
mean((boost.pred-df.test$medv)^2) 
```

We see that the MSE is lowered further by 14.6.

We can also fit a tree that has another tuning parameter, hence it is learning faster.

```{r}
boost.boston <- gbm(medv ~ .
                    ,data = df.train
                    ,distribution = "gaussian"
                    ,n.trees = 5000
                    ,interaction.depth = 4
                    ,shrinkage = 0.2 #Lambda, tuning parameter
                    ,verbose = FALSE #If = T, then it plots the progress
                    )
boost.pred <- predict(boost.boston
                      ,newdata = df.test
                      ,n.trees = 5000)
mean((boost.pred-df.test$medv)^2)
```

We see that the MSE is 17.22, with lambda = standard value of 0.001, hence it is not learning as fast.



## Exercies

### Exercise 7

```{r,results = 'hide'}
library(randomForest)

set.seed(1337)

library(MASS)
df <- Boston
index <- sample(x = nrow(df),size = nrow(df)/2)
df.train <- df[index,]
df.test <- df[-index,]


# Iterating number of trees

MSE.in <- rep(0,500)
MSE.out <- rep(0,500)
for (B in 1:500) {
  print(B)
  
  rf.boston <- randomForest(medv ~ .,data = df.train,ntree = B,mtry = 6,importance = TRUE)
  rf.pred <- predict(object = rf.boston,newdata = df.test)
  
  MSE.in[B] <- mean((rf.boston$predicted-df.train$medv)^2) #MSE
  MSE.out[B] <- mean((rf.pred-df.test$medv)^2) #MSE
}

plot(MSE.out,type = "l",col = 'darkred',xlab = "Number of trees",ylab = "MSE")
lines(rf.boston$mse,type = "l",col = 'limegreen')
grid()
legend("topright",legend = c("Out","In"),lty = c(1,1),col = c('darkred',"limegreen"))
```

```{r,results = 'hide'}
library(doParallel)
CoreCount  <- makePSOCKcluster(detectCores()-1)
registerDoParallel(CoreCount)

# Iterating number of trees and parameters under evaluation

MSE.1 <- rep(0,500)
MSE.2 <- rep(0,500)
MSE.3 <- rep(0,500)
p <- length(df)-1
p2 <- p/2
sqrtp <- sqrt(p) #Notice that randomForest will automatically round
for (B in 1:500) {
  print(B)
  
  rf.boston <- randomForest(medv ~ .,data = df.train,ntree = B,mtry = p,importance = TRUE)
  rf.pred <- predict(object = rf.boston,newdata = df.test)
  
  MSE.1[B] <- mean((rf.pred-df.test$medv)^2) #MSE
  
  rf.boston <- randomForest(medv ~ .,data = df.train,ntree = B,mtry = p2,importance = TRUE)
  rf.pred <- predict(object = rf.boston,newdata = df.test)
  
  MSE.2[B] <- mean((rf.pred-df.test$medv)^2) #MSE
  
  rf.boston <- randomForest(medv ~ .,data = df.train,ntree = B,mtry = sqrtp,importance = TRUE)
  rf.pred <- predict(object = rf.boston,newdata = df.test)
  
  MSE.3[B] <- mean((rf.pred-df.test$medv)^2) #MSE
  
}
stopCluster(CoreCount)
registerDoSEQ()
```


```{r}
plot(MSE.1,type = "l",col = 'darkred',ylim = c(8,12))
lines(MSE.2,type = "l",col = 'limegreen')
lines(MSE.3,type = "l",col = 'coral')
grid()
legend("topright",legend = c("m = p","m = p/2","m = sqrt(p)"),lty = c(1,1,1),col = c('darkred',"limegreen","coral"))
```


```{r}
rm(list = ls())
```

### Exercise 8

__(a) splitting the data__

```{r}
df <- Carseats
set.seed(123)
index <- sample(1:nrow(df),size = nrow(df)*0.75)
df.train <- df[index,]
df.test <- df[-index,]
rm(index)
```

__(b) fitting a regression tree and plotting__

```{r}
library(tree)
fit.tree <- tree(Sales ~ .,data = df.train)
plot(fit.tree) ; text(fit.tree,pretty = 0)
```

We see the fitted values above. The values a each terminal node represent the mean value of y given the different splits.

```{r}
library(dplyr)
mse.test.t0 <- mean((predict(object = fit.tree,newdata = df.test) - df.test$Sales)^2)
mse.test.t0
```

We see that the mean squared error = 4.06

__(c) Pruning the tree__

We must do the following:

1. Assess how big the tree should be. Done through cross validation
1. Fit the pruned tree.
1. Predict and calculate MSE

```{r}
set.seed(123)
fit.cv.tree <- cv.tree(object = fit.tree)
plot(x = fit.cv.tree$size,y = fit.cv.tree$dev,type = "l",col = "darkblue",xlab = "Tree size",ylab = "Deviance",main = "CV Tree")
abline(h = min(fit.cv.tree$dev),col = "darkred",lty = 2)
grid()
```

```{r}
fit.cv.tree$size[which.min(fit.cv.tree$dev)]
```

With a seed of 123, we see that the best tree size appear to be best 8.

```{r}
fit.tree.prune <- prune.tree(tree = fit.tree
                             ,best = fit.cv.tree$size[which.min(fit.cv.tree$dev)])

mse.test.t1 <- mean((predict(object = fit.tree.prune,newdata = df.test) - df.test$Sales)^2)
mse.test.t1
```

So we see that the MSE appear to be worse after pruning.


__(d) Using bagging__

```{r}
library(randomForest)
set.seed(123)
fit.bag <- randomForest(Sales ~ .
                        ,data = df.train
                        ,mtry = length(df.train)-1 #Merely identifies the amount of predictors
                        ,importance = TRUE
                        ,ntree = 500 #This is also the default value
                        )
mse.test.bag <- mean((predict(object = fit.bag,newdata = df.test) - df.test$Sales)^2)
mse.test.bag
```

We see that the MSE is now 2.25. Which is much better than previously.

```{r}
importance(fit.bag)
varImpPlot(fit.bag)
```

We see that the most important variables are ShelveLocation and Price, that was also what we saw in the trees earlier, where it started with the shelve location.

We notice that the Population is actually inverse, hence by removing it, it appears that the MSE will improve slightly.


__(e) Random Forest__

```{r}
library(randomForest)
set.seed(123)
fit.rf <- randomForest(Sales ~ .
                       ,data = df.train
                       ,mtry = length(df.train)/3 #Merely identifies the amount of predictors / 3
                       ,importance = TRUE
                       ,ntree = 500 #This is the default value
                       )
mse.test.rf <- mean((predict(object = fit.rf,newdata = df.test) - df.test$Sales)^2)
mse.test.rf
```

We see that the test MSE is in fact higher than the bagging model.

```{r}
importance(fit.rf)
varImpPlot(fit.rf)
```

Again we see that the shelve location and price are the mosts important variables just as when we were doing bagging.

__Summary__

```{r}
rbind(mse.test.t0
      ,mse.test.t1
      ,mse.test.bag
      ,mse.test.rf
      )
```


### Exercise 9

### Exercise 10

### Exercise 11

### Exercise 12
